{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyw/tg7wsn4LgzX0xfyFTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/new_test_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q629P-tpRbP",
        "outputId": "25332dd2-2855-4e47-d86c-0b1d160dfbd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_cluster-1.6.3+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124 torch_spline_conv-1.2.2+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch_geometric.datasets import Planetoid, Amazon\n",
        "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from sklearn.metrics import f1_score\n",
        "import json,time\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import networkx as nx # Import networkx"
      ],
      "metadata": {
        "id": "hyd8GQEIpj-5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        for i, (conv, adj) in enumerate(zip(self.convs[:-1], adjs)):\n",
        "            x = conv(x)\n",
        "            x = torch.sparse.mm(adj, x)\n",
        "            x = F.relu(x)\n",
        "        x = self.convs[-1](x)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "Zyk2tGcLq9rG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def build_subgraphs_by_feature_correlation(adj_matrix, features, num_neighbors=4, num_subgraphs=128):\n",
        "    num_nodes = features.shape[0]\n",
        "    G = nx.from_numpy_array(adj_matrix)\n",
        "    visited = set()\n",
        "    subgraphs = []\n",
        "    subgraph_node_lists = []\n",
        "\n",
        "    feature_sim = cosine_similarity(features)  # shape: [N, N]\n",
        "\n",
        "    node_indices = np.random.permutation(num_nodes)\n",
        "\n",
        "    for node in node_indices:\n",
        "        if node in visited:\n",
        "            continue\n",
        "\n",
        "        neighbors = list(G.neighbors(node))\n",
        "        if len(neighbors) == 0:\n",
        "            continue\n",
        "\n",
        "        # Keep neighbors that are unvisited\n",
        "        neighbors = [n for n in neighbors if n not in visited]\n",
        "        if not neighbors:\n",
        "            continue\n",
        "\n",
        "        # Rank neighbors by *lowest* feature correlation\n",
        "        correlations = feature_sim[node, neighbors]\n",
        "        sorted_neighbors = [n for _, n in sorted(zip(correlations, neighbors))][:num_neighbors]\n",
        "\n",
        "        sub_nodes = [node] + sorted_neighbors\n",
        "        sub_nodes = list(set(sub_nodes))  # remove any possible duplicates\n",
        "\n",
        "        # Mark as visited\n",
        "        visited.update(sub_nodes)\n",
        "        subgraph_node_lists.append(sub_nodes)\n",
        "\n",
        "        # Create subgraph adjacency\n",
        "        sub_adj = adj_matrix[np.ix_(sub_nodes, sub_nodes)]\n",
        "        subgraphs.append(sub_adj)\n",
        "\n",
        "        if len(subgraphs) >= num_subgraphs:\n",
        "            break\n",
        "\n",
        "    return subgraphs, subgraph_node_lists\n"
      ],
      "metadata": {
        "id": "x8gcrDk7q-5H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inter_subgraph_matrix(subgraph_node_lists, adj_matrix):\n",
        "    num_subgraphs = len(subgraph_node_lists)\n",
        "    inter_matrix = np.zeros((num_subgraphs, num_subgraphs))\n",
        "\n",
        "    for i in range(num_subgraphs):\n",
        "        for j in range(i+1, num_subgraphs):\n",
        "            nodes_i = subgraph_node_lists[i]\n",
        "            nodes_j = subgraph_node_lists[j]\n",
        "            # Count interconnections\n",
        "            inter_links = adj_matrix[np.ix_(nodes_i, nodes_j)]\n",
        "            if np.any(inter_links):\n",
        "                inter_matrix[i, j] = inter_matrix[j, i] = 1\n",
        "\n",
        "    return inter_matrix\n",
        "\n",
        "\n",
        "class CustomSubgraphDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subgraphs, node_lists, features, labels):\n",
        "        self.subgraphs = subgraphs\n",
        "        self.node_lists = node_lists\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subgraphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        nodes = self.node_lists[idx]\n",
        "        sub_x = self.features[nodes]\n",
        "        sub_y = self.labels[nodes]\n",
        "        sub_adj = self.subgraphs[idx]\n",
        "\n",
        "        sub_adj = torch.FloatTensor(sub_adj)\n",
        "        sub_x = torch.FloatTensor(sub_x)\n",
        "        sub_y = torch.LongTensor(sub_y)\n",
        "\n",
        "        return sub_x, sub_adj, sub_y\n"
      ],
      "metadata": {
        "id": "eyScwgOnrCCk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, criterion, subgraphs, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for sub_nodes in subgraphs:\n",
        "        # Mapping from global node index → local index in subgraph\n",
        "        node_map = {int(n): i for i, n in enumerate(sub_nodes)}\n",
        "        sub_nodes = torch.tensor(sub_nodes, dtype=torch.long)\n",
        "\n",
        "        # Build local edge_index: filter only edges inside the subgraph\n",
        "        sub_adj = data.edge_index.cpu().numpy()  # shape [2, num_edges]\n",
        "        sub_nodes_set = set(sub_nodes.tolist())\n",
        "        filtered_edges = []\n",
        "\n",
        "        for src, dst in sub_adj.T:\n",
        "            if src in sub_nodes_set and dst in sub_nodes_set:\n",
        "                filtered_edges.append([node_map[src], node_map[dst]])\n",
        "\n",
        "        if len(filtered_edges) == 0:\n",
        "            continue  # skip subgraph with no internal edges\n",
        "\n",
        "        # Build edge_index tensor [2, num_edges]\n",
        "        sub_edge_index = torch.tensor(filtered_edges, dtype=torch.long).T.to(device)\n",
        "\n",
        "        # Subgraph features and labels\n",
        "        x_sub = data.x[sub_nodes].to(device)\n",
        "        y_sub = data.y[sub_nodes].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x_sub, sub_edge_index)\n",
        "\n",
        "        # Loss: only on training nodes inside the subgraph\n",
        "        train_mask_sub = data.train_mask[sub_nodes].to(device)\n",
        "        if train_mask_sub.sum() == 0:\n",
        "            continue  # skip if no training nodes in this subgraph\n",
        "\n",
        "        loss = criterion(out[train_mask_sub], y_sub[train_mask_sub])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(subgraphs)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def test(model, data, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = data.x.to(device)\n",
        "        edge_index = data.edge_index.to(device)\n",
        "        values = torch.ones(edge_index.shape[1]).to(device)\n",
        "        adj = torch.sparse_coo_tensor(edge_index, values, size=(x.size(0), x.size(0))).to(device)\n",
        "\n",
        "        out = model(x, [adj] * (len(model.convs) - 1))\n",
        "        pred = out.argmax(dim=1)\n",
        "        true = data.y.to(device)\n",
        "\n",
        "        acc = accuracy_score(true.cpu(), pred.cpu())\n",
        "        f1 = f1_score(true.cpu(), pred.cpu(), average='micro')\n",
        "\n",
        "        return acc, f1\n"
      ],
      "metadata": {
        "id": "IViiRC7srRAA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_subgraphs(data, subgraph_size=5, num_neighbors=4):\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    features = data.x.cpu().numpy()\n",
        "    adj = nx.to_numpy_array(G)\n",
        "    visited = set()\n",
        "    subgraphs = []\n",
        "    subgraph_connections = []\n",
        "\n",
        "    # Node-to-node correlation matrix\n",
        "    similarity = cosine_similarity(features)\n",
        "\n",
        "    while len(visited) < data.num_nodes:\n",
        "        available_nodes = list(set(range(data.num_nodes)) - visited)\n",
        "        if not available_nodes:\n",
        "            break\n",
        "\n",
        "        start = np.random.choice(available_nodes)\n",
        "        current = [start]\n",
        "        visited.add(start)\n",
        "\n",
        "        neighbors = list(G.neighbors(start))\n",
        "        neighbors = sorted(\n",
        "            neighbors, key=lambda n: similarity[start, n]\n",
        "        )[:num_neighbors]\n",
        "        for n in neighbors:\n",
        "            if n not in visited:\n",
        "                current.append(n)\n",
        "                visited.add(n)\n",
        "\n",
        "        subgraphs.append(current)\n",
        "\n",
        "    # Build inter-subgraph connection matrix\n",
        "    num_subs = len(subgraphs)\n",
        "    connection_matrix = np.zeros((num_subs, num_subs))\n",
        "    node_to_subgraph = {}\n",
        "    for i, sub in enumerate(subgraphs):\n",
        "        for node in sub:\n",
        "            node_to_subgraph[node] = i\n",
        "\n",
        "    for edge in G.edges():\n",
        "        a, b = edge\n",
        "        if a in node_to_subgraph and b in node_to_subgraph:\n",
        "            i = node_to_subgraph[a]\n",
        "            j = node_to_subgraph[b]\n",
        "            if i != j:\n",
        "                connection_matrix[i][j] += 1\n",
        "\n",
        "    return subgraphs, connection_matrix"
      ],
      "metadata": {
        "id": "wKfL0wOJupoc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "data = dataset[0]\n"
      ],
      "metadata": {
        "id": "Y8_RyI5wrmtd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "adj_matrix = to_dense_adj(data.edge_index)[0].numpy()\n",
        "features = data.x.numpy()\n",
        "labels = data.y.numpy()\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(in_channels=data.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "subgraphs, conn_matrix = build_subgraphs(data, subgraph_size=5, num_neighbors=4)\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    loss = train(model, data, optimizer, criterion, subgraphs, device)\n",
        "    acc, f1 = test(model, data, device)\n",
        "    print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "QtVMy9w3rfYk",
        "outputId": "c5f987ba-b24c-40a3-8f01-bb58d697985f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2325698075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26-1141792106.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, subgraphs, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Loss: only on training nodes inside the subgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-911482615.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adjs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_networkx, to_dense_adj\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        # adjs is a list of sparse adjacency matrices (one per conv layer except last)\n",
        "        for conv, adj in zip(self.convs[:-1], adjs):\n",
        "            x = conv(x)\n",
        "            x = torch.sparse.mm(adj, x)\n",
        "            x = F.relu(x)\n",
        "        x = self.convs[-1](x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "def build_subgraphs(data, subgraph_size=5, num_neighbors=4):\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    features = data.x.cpu().numpy()\n",
        "    visited = set()\n",
        "    subgraphs = []\n",
        "\n",
        "    similarity = cosine_similarity(features)\n",
        "\n",
        "    while len(visited) < data.num_nodes:\n",
        "        available_nodes = list(set(range(data.num_nodes)) - visited)\n",
        "        if not available_nodes:\n",
        "            break\n",
        "\n",
        "        start = np.random.choice(available_nodes)\n",
        "        current = [start]\n",
        "        visited.add(start)\n",
        "\n",
        "        neighbors = list(G.neighbors(start))\n",
        "        neighbors = sorted(neighbors, key=lambda n: similarity[start, n])[:num_neighbors]\n",
        "        for n in neighbors:\n",
        "            if n not in visited:\n",
        "                current.append(n)\n",
        "                visited.add(n)\n",
        "\n",
        "        subgraphs.append(current)\n",
        "\n",
        "    # Build inter-subgraph connection matrix (optional)\n",
        "    num_subs = len(subgraphs)\n",
        "    connection_matrix = np.zeros((num_subs, num_subs))\n",
        "    node_to_subgraph = {}\n",
        "    for i, sub in enumerate(subgraphs):\n",
        "        for node in sub:\n",
        "            node_to_subgraph[node] = i\n",
        "\n",
        "    for a, b in G.edges():\n",
        "        if a in node_to_subgraph and b in node_to_subgraph:\n",
        "            i = node_to_subgraph[a]\n",
        "            j = node_to_subgraph[b]\n",
        "            if i != j:\n",
        "                connection_matrix[i, j] += 1\n",
        "\n",
        "    return subgraphs, connection_matrix\n",
        "\n",
        "def build_subgraph_adj(data, sub_nodes, device):\n",
        "    # Build adjacency for subgraph nodes with remapped indices\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    mask = np.isin(edge_index[0], sub_nodes) & np.isin(edge_index[1], sub_nodes)\n",
        "    filtered_edges = edge_index[:, mask]\n",
        "\n",
        "    # Remap global node indices to subgraph local indices\n",
        "    node_map = {n: i for i, n in enumerate(sub_nodes)}\n",
        "    remapped_edges = np.array([[node_map[n] for n in filtered_edges[0]],\n",
        "                              [node_map[n] for n in filtered_edges[1]]])\n",
        "\n",
        "    # Create sparse adjacency matrix (COO)\n",
        "    indices = torch.tensor(remapped_edges, dtype=torch.long).to(device)\n",
        "    values = torch.ones(indices.shape[1], device=device)\n",
        "    adj = torch.sparse_coo_tensor(indices, values, (len(sub_nodes), len(sub_nodes))).coalesce()\n",
        "    return adj\n",
        "\n",
        "def train(model, data, optimizer, criterion, subgraphs, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for sub_nodes in subgraphs:\n",
        "        x = data.x[sub_nodes].to(device)\n",
        "        y = data.y[sub_nodes].to(device)\n",
        "        adj = build_subgraph_adj(data, sub_nodes, device)\n",
        "        # Repeat adjacency matrix for each layer except the last\n",
        "        adjs = [adj] * (len(model.convs) - 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x, adjs)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(subgraphs)\n",
        "\n",
        "def test(model, data, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = data.x.to(device)\n",
        "        edge_index = data.edge_index.to(device)\n",
        "        values = torch.ones(edge_index.shape[1], device=device)\n",
        "        adj = torch.sparse_coo_tensor(edge_index, values, (x.size(0), x.size(0))).coalesce()\n",
        "        adjs = [adj] * (len(model.convs) - 1)\n",
        "\n",
        "        out = model(x, adjs)\n",
        "        pred = out.argmax(dim=1)\n",
        "        true = data.y.to(device)\n",
        "\n",
        "        acc = accuracy_score(true.cpu(), pred.cpu())\n",
        "        f1 = f1_score(true.cpu(), pred.cpu(), average='micro')\n",
        "        return acc, f1\n",
        "\n",
        "# Usage example (make sure you have dataset & data loaded properly):\n",
        "subgraphs, conn_matrix = build_subgraphs(data, subgraph_size=5, num_neighbors=4)\n",
        "\n",
        "res=[]\n",
        "for i in range(10):\n",
        "  for epoch in range(1, 101):\n",
        "      loss = train(model, data, optimizer, criterion, subgraphs, device)\n",
        "      acc, f1 = test(model, data, device)\n",
        "      print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "  acc, f1 = test(model, data, device)\n",
        "  res.append(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa-xEHkgw_4N",
        "outputId": "66815d1f-bd51-471f-96c6-333ee6086942"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Loss: 1.2516 | Acc: 0.8449 | F1: 0.8449\n",
            "Epoch 002 | Loss: 1.0687 | Acc: 0.8678 | F1: 0.8678\n",
            "Epoch 003 | Loss: 1.0615 | Acc: 0.8264 | F1: 0.8264\n",
            "Epoch 004 | Loss: 1.1070 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 005 | Loss: 1.0410 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 006 | Loss: 1.0471 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 007 | Loss: 1.0599 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 008 | Loss: 1.0213 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 009 | Loss: 1.0549 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 010 | Loss: 1.0442 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 011 | Loss: 1.0289 | Acc: 0.8194 | F1: 0.8194\n",
            "Epoch 012 | Loss: 1.0429 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 013 | Loss: 1.0287 | Acc: 0.8353 | F1: 0.8353\n",
            "Epoch 014 | Loss: 1.0337 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 015 | Loss: 1.0337 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 016 | Loss: 1.0300 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 017 | Loss: 1.0378 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 018 | Loss: 1.0267 | Acc: 0.8043 | F1: 0.8043\n",
            "Epoch 019 | Loss: 1.0760 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 020 | Loss: 1.0019 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 021 | Loss: 1.0284 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 022 | Loss: 1.0423 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 023 | Loss: 1.0259 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 024 | Loss: 1.0210 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 025 | Loss: 1.0381 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 026 | Loss: 1.0337 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 027 | Loss: 1.0280 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 028 | Loss: 1.0232 | Acc: 0.8419 | F1: 0.8419\n",
            "Epoch 029 | Loss: 1.0264 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 030 | Loss: 1.0245 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 031 | Loss: 1.0422 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 032 | Loss: 1.0298 | Acc: 0.8390 | F1: 0.8390\n",
            "Epoch 033 | Loss: 1.0407 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 034 | Loss: 1.0094 | Acc: 0.8408 | F1: 0.8408\n",
            "Epoch 035 | Loss: 1.0257 | Acc: 0.8468 | F1: 0.8468\n",
            "Epoch 036 | Loss: 1.0508 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 037 | Loss: 1.0236 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 038 | Loss: 1.0354 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 039 | Loss: 1.0312 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 040 | Loss: 1.0306 | Acc: 0.8301 | F1: 0.8301\n",
            "Epoch 041 | Loss: 1.0447 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 042 | Loss: 1.0320 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 043 | Loss: 1.0168 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 044 | Loss: 1.0273 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 045 | Loss: 1.0367 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 046 | Loss: 1.0174 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 047 | Loss: 1.0288 | Acc: 0.8357 | F1: 0.8357\n",
            "Epoch 048 | Loss: 1.0353 | Acc: 0.8431 | F1: 0.8431\n",
            "Epoch 049 | Loss: 1.0443 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 050 | Loss: 1.0111 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 051 | Loss: 1.0498 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 052 | Loss: 1.0340 | Acc: 0.8711 | F1: 0.8711\n",
            "Epoch 053 | Loss: 1.0055 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 054 | Loss: 1.0197 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 055 | Loss: 1.0422 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 056 | Loss: 1.0425 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 057 | Loss: 1.0471 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 058 | Loss: 1.0083 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 059 | Loss: 1.0268 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 060 | Loss: 1.0229 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 061 | Loss: 1.0252 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 062 | Loss: 1.0288 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 063 | Loss: 1.0193 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 064 | Loss: 1.0457 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 065 | Loss: 1.0188 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 066 | Loss: 1.0267 | Acc: 0.8301 | F1: 0.8301\n",
            "Epoch 067 | Loss: 1.0246 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 068 | Loss: 1.0334 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 069 | Loss: 1.0220 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 070 | Loss: 1.0402 | Acc: 0.8456 | F1: 0.8456\n",
            "Epoch 071 | Loss: 1.0002 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 072 | Loss: 1.0465 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 073 | Loss: 1.0544 | Acc: 0.8708 | F1: 0.8708\n",
            "Epoch 074 | Loss: 1.0000 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 075 | Loss: 1.0312 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 076 | Loss: 1.0367 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 077 | Loss: 1.0498 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 078 | Loss: 1.0073 | Acc: 0.8431 | F1: 0.8431\n",
            "Epoch 079 | Loss: 1.0373 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 080 | Loss: 1.0238 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 081 | Loss: 1.0132 | Acc: 0.8682 | F1: 0.8682\n",
            "Epoch 082 | Loss: 1.0293 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 083 | Loss: 1.0291 | Acc: 0.8342 | F1: 0.8342\n",
            "Epoch 084 | Loss: 1.0188 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 085 | Loss: 1.0398 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 086 | Loss: 1.0270 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 087 | Loss: 1.0139 | Acc: 0.8294 | F1: 0.8294\n",
            "Epoch 088 | Loss: 1.0502 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 089 | Loss: 1.0162 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 090 | Loss: 1.0230 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 091 | Loss: 1.0449 | Acc: 0.8390 | F1: 0.8390\n",
            "Epoch 092 | Loss: 1.0216 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 093 | Loss: 1.0182 | Acc: 0.8468 | F1: 0.8468\n",
            "Epoch 094 | Loss: 1.0339 | Acc: 0.8401 | F1: 0.8401\n",
            "Epoch 095 | Loss: 1.0280 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 096 | Loss: 1.0275 | Acc: 0.8737 | F1: 0.8737\n",
            "Epoch 097 | Loss: 1.0254 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 098 | Loss: 1.0090 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 099 | Loss: 1.0253 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 100 | Loss: 1.0320 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 001 | Loss: 1.0152 | Acc: 0.8305 | F1: 0.8305\n",
            "Epoch 002 | Loss: 1.0439 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 003 | Loss: 1.0039 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 004 | Loss: 1.0394 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 005 | Loss: 1.0462 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 006 | Loss: 1.0061 | Acc: 0.8390 | F1: 0.8390\n",
            "Epoch 007 | Loss: 1.0271 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 008 | Loss: 1.0179 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 009 | Loss: 1.0293 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 010 | Loss: 1.0477 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 011 | Loss: 1.0297 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 012 | Loss: 1.0014 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 013 | Loss: 1.0457 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 014 | Loss: 1.0066 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 015 | Loss: 1.0056 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 016 | Loss: 1.0410 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 017 | Loss: 1.0118 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 018 | Loss: 1.0090 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 019 | Loss: 1.0223 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 020 | Loss: 1.0193 | Acc: 0.8456 | F1: 0.8456\n",
            "Epoch 021 | Loss: 1.0147 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 022 | Loss: 1.0006 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 023 | Loss: 1.0417 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 024 | Loss: 1.0397 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 025 | Loss: 1.0118 | Acc: 0.8438 | F1: 0.8438\n",
            "Epoch 026 | Loss: 1.0291 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 027 | Loss: 1.0340 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 028 | Loss: 1.0125 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 029 | Loss: 1.0259 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 030 | Loss: 1.0178 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 031 | Loss: 1.0153 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 032 | Loss: 1.0287 | Acc: 0.8364 | F1: 0.8364\n",
            "Epoch 033 | Loss: 1.0237 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 034 | Loss: 1.0168 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 035 | Loss: 1.0235 | Acc: 0.8375 | F1: 0.8375\n",
            "Epoch 036 | Loss: 1.0297 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 037 | Loss: 1.0104 | Acc: 0.8157 | F1: 0.8157\n",
            "Epoch 038 | Loss: 1.0232 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 039 | Loss: 1.0213 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 040 | Loss: 1.0151 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 041 | Loss: 1.0343 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 042 | Loss: 1.0149 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 043 | Loss: 1.0096 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 044 | Loss: 1.0313 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 045 | Loss: 1.0259 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 046 | Loss: 1.0154 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 047 | Loss: 1.0267 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 048 | Loss: 1.0424 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 049 | Loss: 1.0027 | Acc: 0.8405 | F1: 0.8405\n",
            "Epoch 050 | Loss: 1.0117 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 051 | Loss: 1.0346 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 052 | Loss: 1.0067 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 053 | Loss: 1.0253 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 054 | Loss: 1.0153 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 055 | Loss: 1.0271 | Acc: 0.8445 | F1: 0.8445\n",
            "Epoch 056 | Loss: 1.0311 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 057 | Loss: 1.0319 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 058 | Loss: 0.9947 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 059 | Loss: 1.0480 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 060 | Loss: 1.0183 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 061 | Loss: 1.0496 | Acc: 0.8239 | F1: 0.8239\n",
            "Epoch 062 | Loss: 1.0109 | Acc: 0.8401 | F1: 0.8401\n",
            "Epoch 063 | Loss: 1.0090 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 064 | Loss: 1.0152 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 065 | Loss: 1.0183 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 066 | Loss: 1.0344 | Acc: 0.8386 | F1: 0.8386\n",
            "Epoch 067 | Loss: 1.0209 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 068 | Loss: 1.0171 | Acc: 0.8464 | F1: 0.8464\n",
            "Epoch 069 | Loss: 1.0414 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 070 | Loss: 1.0179 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 071 | Loss: 1.0207 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 072 | Loss: 1.0165 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 073 | Loss: 1.0186 | Acc: 0.8792 | F1: 0.8792\n",
            "Epoch 074 | Loss: 1.0099 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 075 | Loss: 1.0331 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 076 | Loss: 1.0470 | Acc: 0.8349 | F1: 0.8349\n",
            "Epoch 077 | Loss: 1.0184 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 078 | Loss: 1.0004 | Acc: 0.8408 | F1: 0.8408\n",
            "Epoch 079 | Loss: 1.0370 | Acc: 0.8323 | F1: 0.8323\n",
            "Epoch 080 | Loss: 1.0125 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 081 | Loss: 1.0111 | Acc: 0.8405 | F1: 0.8405\n",
            "Epoch 082 | Loss: 1.0394 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 083 | Loss: 1.0188 | Acc: 0.8431 | F1: 0.8431\n",
            "Epoch 084 | Loss: 1.0153 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 085 | Loss: 1.0080 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 086 | Loss: 1.0209 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 087 | Loss: 1.0109 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 088 | Loss: 1.0334 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 089 | Loss: 1.0116 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 090 | Loss: 1.0314 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 091 | Loss: 1.0074 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 092 | Loss: 1.0331 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 093 | Loss: 1.0219 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 094 | Loss: 1.0063 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 095 | Loss: 1.0391 | Acc: 0.8323 | F1: 0.8323\n",
            "Epoch 096 | Loss: 1.0151 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 097 | Loss: 1.0103 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 098 | Loss: 1.0077 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 099 | Loss: 1.0235 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 100 | Loss: 1.0132 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 001 | Loss: 1.0142 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 002 | Loss: 1.0332 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 003 | Loss: 1.0141 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 004 | Loss: 1.0144 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 005 | Loss: 1.0186 | Acc: 0.8405 | F1: 0.8405\n",
            "Epoch 006 | Loss: 1.0343 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 007 | Loss: 1.0160 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 008 | Loss: 1.0225 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 009 | Loss: 1.0334 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 010 | Loss: 1.0315 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 011 | Loss: 1.0011 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 012 | Loss: 1.0415 | Acc: 0.8423 | F1: 0.8423\n",
            "Epoch 013 | Loss: 1.0161 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 014 | Loss: 1.0154 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 015 | Loss: 1.0113 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 016 | Loss: 1.0507 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 017 | Loss: 1.0270 | Acc: 0.8449 | F1: 0.8449\n",
            "Epoch 018 | Loss: 1.0035 | Acc: 0.8442 | F1: 0.8442\n",
            "Epoch 019 | Loss: 1.0128 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 020 | Loss: 1.0438 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 021 | Loss: 1.0159 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 022 | Loss: 1.0089 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 023 | Loss: 1.0246 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 024 | Loss: 1.0146 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 025 | Loss: 1.0109 | Acc: 0.8338 | F1: 0.8338\n",
            "Epoch 026 | Loss: 1.0413 | Acc: 0.8327 | F1: 0.8327\n",
            "Epoch 027 | Loss: 1.0167 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 028 | Loss: 1.0186 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 029 | Loss: 1.0481 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 030 | Loss: 1.0225 | Acc: 0.8748 | F1: 0.8748\n",
            "Epoch 031 | Loss: 1.0121 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 032 | Loss: 1.0261 | Acc: 0.8390 | F1: 0.8390\n",
            "Epoch 033 | Loss: 1.0285 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 034 | Loss: 1.0009 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 035 | Loss: 1.0196 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 036 | Loss: 1.0181 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 037 | Loss: 1.0331 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 038 | Loss: 1.0198 | Acc: 0.8442 | F1: 0.8442\n",
            "Epoch 039 | Loss: 1.0064 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 040 | Loss: 1.0244 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 041 | Loss: 1.0297 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 042 | Loss: 1.0259 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 043 | Loss: 1.0155 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 044 | Loss: 1.0087 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 045 | Loss: 1.0209 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 046 | Loss: 1.0387 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 047 | Loss: 1.0086 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 048 | Loss: 1.0143 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 049 | Loss: 1.0327 | Acc: 0.8227 | F1: 0.8227\n",
            "Epoch 050 | Loss: 1.0129 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 051 | Loss: 1.0026 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 052 | Loss: 1.0277 | Acc: 0.8364 | F1: 0.8364\n",
            "Epoch 053 | Loss: 1.0346 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 054 | Loss: 1.0345 | Acc: 0.8456 | F1: 0.8456\n",
            "Epoch 055 | Loss: 0.9953 | Acc: 0.8194 | F1: 0.8194\n",
            "Epoch 056 | Loss: 1.0221 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 057 | Loss: 1.0016 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 058 | Loss: 1.0316 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 059 | Loss: 1.0204 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 060 | Loss: 1.0072 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 061 | Loss: 1.0142 | Acc: 0.8353 | F1: 0.8353\n",
            "Epoch 062 | Loss: 1.0465 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 063 | Loss: 1.0282 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 064 | Loss: 1.0369 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 065 | Loss: 1.0296 | Acc: 0.8708 | F1: 0.8708\n",
            "Epoch 066 | Loss: 1.0034 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 067 | Loss: 1.0177 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 068 | Loss: 1.0396 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 069 | Loss: 1.0053 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 070 | Loss: 1.0290 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 071 | Loss: 0.9993 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 072 | Loss: 1.0292 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 073 | Loss: 1.0408 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 074 | Loss: 1.0158 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 075 | Loss: 1.0365 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 076 | Loss: 1.0297 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 077 | Loss: 1.0223 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 078 | Loss: 1.0040 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 079 | Loss: 1.0522 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 080 | Loss: 1.0128 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 081 | Loss: 1.0246 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 082 | Loss: 1.0148 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 083 | Loss: 1.0431 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 084 | Loss: 1.0225 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 085 | Loss: 1.0108 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 086 | Loss: 1.0257 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 087 | Loss: 1.0354 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 088 | Loss: 1.0060 | Acc: 0.8438 | F1: 0.8438\n",
            "Epoch 089 | Loss: 1.0327 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 090 | Loss: 1.0243 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 091 | Loss: 1.0073 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 092 | Loss: 1.0414 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 093 | Loss: 1.0273 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 094 | Loss: 0.9999 | Acc: 0.8449 | F1: 0.8449\n",
            "Epoch 095 | Loss: 1.0273 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 096 | Loss: 1.0436 | Acc: 0.8250 | F1: 0.8250\n",
            "Epoch 097 | Loss: 1.0021 | Acc: 0.8730 | F1: 0.8730\n",
            "Epoch 098 | Loss: 1.0192 | Acc: 0.8408 | F1: 0.8408\n",
            "Epoch 099 | Loss: 1.0350 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 100 | Loss: 1.0051 | Acc: 0.8696 | F1: 0.8696\n",
            "Epoch 001 | Loss: 1.0311 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 002 | Loss: 1.0371 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 003 | Loss: 1.0072 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 004 | Loss: 1.0226 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 005 | Loss: 1.0213 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 006 | Loss: 1.0172 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 007 | Loss: 1.0129 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 008 | Loss: 1.0303 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 009 | Loss: 1.0103 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 010 | Loss: 1.0008 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 011 | Loss: 1.0275 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 012 | Loss: 1.0318 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 013 | Loss: 1.0187 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 014 | Loss: 1.0214 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 015 | Loss: 1.0213 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 016 | Loss: 1.0130 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 017 | Loss: 1.0230 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 018 | Loss: 1.0288 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 019 | Loss: 1.0116 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 020 | Loss: 1.0191 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 021 | Loss: 1.0420 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 022 | Loss: 1.0094 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 023 | Loss: 1.0325 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 024 | Loss: 1.0153 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 025 | Loss: 1.0268 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 026 | Loss: 1.0083 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 027 | Loss: 1.0022 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 028 | Loss: 1.0404 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 029 | Loss: 1.0144 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 030 | Loss: 1.0246 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 031 | Loss: 1.0366 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 032 | Loss: 1.0117 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 033 | Loss: 1.0357 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 034 | Loss: 1.0123 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 035 | Loss: 1.0237 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 036 | Loss: 0.9917 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 037 | Loss: 1.0153 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 038 | Loss: 1.0444 | Acc: 0.8445 | F1: 0.8445\n",
            "Epoch 039 | Loss: 1.0088 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 040 | Loss: 1.0220 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 041 | Loss: 1.0336 | Acc: 0.8272 | F1: 0.8272\n",
            "Epoch 042 | Loss: 1.0409 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 043 | Loss: 1.0232 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 044 | Loss: 1.0318 | Acc: 0.8682 | F1: 0.8682\n",
            "Epoch 045 | Loss: 1.0204 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 046 | Loss: 1.0104 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 047 | Loss: 1.0225 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 048 | Loss: 1.0342 | Acc: 0.8331 | F1: 0.8331\n",
            "Epoch 049 | Loss: 1.0065 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 050 | Loss: 1.0510 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 051 | Loss: 1.0075 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 052 | Loss: 1.0156 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 053 | Loss: 1.0116 | Acc: 0.8394 | F1: 0.8394\n",
            "Epoch 054 | Loss: 1.0317 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 055 | Loss: 1.0149 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 056 | Loss: 1.0124 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 057 | Loss: 1.0170 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 058 | Loss: 1.0289 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 059 | Loss: 1.0111 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 060 | Loss: 1.0216 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 061 | Loss: 1.0227 | Acc: 0.8711 | F1: 0.8711\n",
            "Epoch 062 | Loss: 1.0172 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 063 | Loss: 1.0273 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 064 | Loss: 1.0243 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 065 | Loss: 1.0340 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 066 | Loss: 1.0172 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 067 | Loss: 1.0341 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 068 | Loss: 1.0009 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 069 | Loss: 1.0304 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 070 | Loss: 1.0350 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 071 | Loss: 1.0032 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 072 | Loss: 1.0130 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 073 | Loss: 1.0454 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 074 | Loss: 1.0241 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 075 | Loss: 1.0176 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 076 | Loss: 1.0294 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 077 | Loss: 1.0347 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 078 | Loss: 1.0128 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 079 | Loss: 1.0021 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 080 | Loss: 1.0507 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 081 | Loss: 1.0271 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 082 | Loss: 1.0197 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 083 | Loss: 0.9981 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 084 | Loss: 1.0353 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 085 | Loss: 1.0320 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 086 | Loss: 1.0014 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 087 | Loss: 1.0252 | Acc: 0.8408 | F1: 0.8408\n",
            "Epoch 088 | Loss: 1.0222 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 089 | Loss: 1.0356 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 090 | Loss: 1.0401 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 091 | Loss: 1.0249 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 092 | Loss: 1.0103 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 093 | Loss: 1.0114 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 094 | Loss: 1.0290 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 095 | Loss: 1.0318 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 096 | Loss: 1.0146 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 097 | Loss: 1.0171 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 098 | Loss: 1.0196 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 099 | Loss: 1.0212 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 100 | Loss: 1.0351 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 001 | Loss: 1.0334 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 002 | Loss: 1.0175 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 003 | Loss: 1.0088 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 004 | Loss: 1.0270 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 005 | Loss: 1.0109 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 006 | Loss: 1.0043 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 007 | Loss: 1.0373 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 008 | Loss: 0.9980 | Acc: 0.8390 | F1: 0.8390\n",
            "Epoch 009 | Loss: 1.0361 | Acc: 0.8331 | F1: 0.8331\n",
            "Epoch 010 | Loss: 1.0016 | Acc: 0.8696 | F1: 0.8696\n",
            "Epoch 011 | Loss: 1.0248 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 012 | Loss: 1.0223 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 013 | Loss: 1.0234 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 014 | Loss: 1.0127 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 015 | Loss: 1.0320 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 016 | Loss: 1.0123 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 017 | Loss: 1.0228 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 018 | Loss: 0.9932 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 019 | Loss: 1.0444 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 020 | Loss: 1.0034 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 021 | Loss: 1.0227 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 022 | Loss: 1.0211 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 023 | Loss: 1.0480 | Acc: 0.8704 | F1: 0.8704\n",
            "Epoch 024 | Loss: 1.0086 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 025 | Loss: 1.0203 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 026 | Loss: 1.0341 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 027 | Loss: 1.0021 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 028 | Loss: 1.0174 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 029 | Loss: 1.0375 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 030 | Loss: 1.0117 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 031 | Loss: 0.9932 | Acc: 0.8678 | F1: 0.8678\n",
            "Epoch 032 | Loss: 1.0067 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 033 | Loss: 1.0284 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 034 | Loss: 1.0289 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 035 | Loss: 1.0103 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 036 | Loss: 1.0261 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 037 | Loss: 1.0181 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 038 | Loss: 1.0279 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 039 | Loss: 0.9959 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 040 | Loss: 1.0262 | Acc: 0.8711 | F1: 0.8711\n",
            "Epoch 041 | Loss: 1.0198 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 042 | Loss: 1.0220 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 043 | Loss: 1.0262 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 044 | Loss: 0.9983 | Acc: 0.8416 | F1: 0.8416\n",
            "Epoch 045 | Loss: 1.0348 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 046 | Loss: 1.0175 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 047 | Loss: 0.9996 | Acc: 0.8419 | F1: 0.8419\n",
            "Epoch 048 | Loss: 1.0370 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 049 | Loss: 1.0310 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 050 | Loss: 1.0094 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 051 | Loss: 1.0355 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 052 | Loss: 1.0115 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 053 | Loss: 1.0269 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 054 | Loss: 1.0278 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 055 | Loss: 1.0248 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 056 | Loss: 1.0066 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 057 | Loss: 1.0148 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 058 | Loss: 1.0162 | Acc: 0.8708 | F1: 0.8708\n",
            "Epoch 059 | Loss: 1.0364 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 060 | Loss: 1.0428 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 061 | Loss: 1.0046 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 062 | Loss: 1.0206 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 063 | Loss: 1.0180 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 064 | Loss: 1.0232 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 065 | Loss: 0.9974 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 066 | Loss: 1.0411 | Acc: 0.8368 | F1: 0.8368\n",
            "Epoch 067 | Loss: 1.0139 | Acc: 0.8804 | F1: 0.8804\n",
            "Epoch 068 | Loss: 1.0028 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 069 | Loss: 1.0120 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 070 | Loss: 1.0268 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 071 | Loss: 1.0278 | Acc: 0.8785 | F1: 0.8785\n",
            "Epoch 072 | Loss: 1.0014 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 073 | Loss: 1.0372 | Acc: 0.8763 | F1: 0.8763\n",
            "Epoch 074 | Loss: 1.0005 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 075 | Loss: 1.0262 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 076 | Loss: 1.0307 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 077 | Loss: 1.0202 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 078 | Loss: 1.0122 | Acc: 0.8323 | F1: 0.8323\n",
            "Epoch 079 | Loss: 1.0137 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 080 | Loss: 1.0144 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 081 | Loss: 1.0252 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 082 | Loss: 1.0249 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 083 | Loss: 0.9950 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 084 | Loss: 1.0258 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 085 | Loss: 1.0214 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 086 | Loss: 1.0206 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 087 | Loss: 1.0076 | Acc: 0.8722 | F1: 0.8722\n",
            "Epoch 088 | Loss: 1.0133 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 089 | Loss: 1.0345 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 090 | Loss: 1.0124 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 091 | Loss: 1.0271 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 092 | Loss: 1.0112 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 093 | Loss: 1.0049 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 094 | Loss: 1.0387 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 095 | Loss: 1.0105 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 096 | Loss: 1.0404 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 097 | Loss: 1.0255 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 098 | Loss: 1.0107 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 099 | Loss: 1.0242 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 100 | Loss: 0.9993 | Acc: 0.8715 | F1: 0.8715\n",
            "Epoch 001 | Loss: 1.0322 | Acc: 0.8298 | F1: 0.8298\n",
            "Epoch 002 | Loss: 1.0185 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 003 | Loss: 1.0122 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 004 | Loss: 1.0138 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 005 | Loss: 1.0061 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 006 | Loss: 1.0223 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 007 | Loss: 1.0135 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 008 | Loss: 1.0036 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 009 | Loss: 1.0338 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 010 | Loss: 1.0157 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 011 | Loss: 1.0301 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 012 | Loss: 1.0039 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 013 | Loss: 1.0100 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 014 | Loss: 1.0236 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 015 | Loss: 1.0151 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 016 | Loss: 1.0035 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 017 | Loss: 1.0189 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 018 | Loss: 1.0274 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 019 | Loss: 1.0140 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 020 | Loss: 1.0140 | Acc: 0.8682 | F1: 0.8682\n",
            "Epoch 021 | Loss: 1.0281 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 022 | Loss: 1.0280 | Acc: 0.8715 | F1: 0.8715\n",
            "Epoch 023 | Loss: 0.9940 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 024 | Loss: 1.0236 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 025 | Loss: 1.0325 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 026 | Loss: 1.0186 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 027 | Loss: 1.0148 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 028 | Loss: 1.0297 | Acc: 0.8383 | F1: 0.8383\n",
            "Epoch 029 | Loss: 1.0268 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 030 | Loss: 1.0138 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 031 | Loss: 1.0129 | Acc: 0.8449 | F1: 0.8449\n",
            "Epoch 032 | Loss: 1.0308 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 033 | Loss: 1.0182 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 034 | Loss: 1.0085 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 035 | Loss: 1.0348 | Acc: 0.8283 | F1: 0.8283\n",
            "Epoch 036 | Loss: 1.0156 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 037 | Loss: 0.9989 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 038 | Loss: 1.0286 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 039 | Loss: 0.9991 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 040 | Loss: 1.0490 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 041 | Loss: 1.0215 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 042 | Loss: 1.0116 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 043 | Loss: 1.0212 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 044 | Loss: 1.0364 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 045 | Loss: 0.9952 | Acc: 0.8368 | F1: 0.8368\n",
            "Epoch 046 | Loss: 1.0433 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 047 | Loss: 1.0152 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 048 | Loss: 1.0100 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 049 | Loss: 1.0256 | Acc: 0.8682 | F1: 0.8682\n",
            "Epoch 050 | Loss: 1.0235 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 051 | Loss: 1.0050 | Acc: 0.8371 | F1: 0.8371\n",
            "Epoch 052 | Loss: 1.0414 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 053 | Loss: 1.0185 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 054 | Loss: 1.0081 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 055 | Loss: 1.0309 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 056 | Loss: 1.0186 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 057 | Loss: 1.0260 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 058 | Loss: 1.0065 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 059 | Loss: 1.0304 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 060 | Loss: 1.0365 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 061 | Loss: 1.0171 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 062 | Loss: 1.0116 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 063 | Loss: 1.0187 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 064 | Loss: 1.0187 | Acc: 0.8442 | F1: 0.8442\n",
            "Epoch 065 | Loss: 1.0357 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 066 | Loss: 1.0169 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 067 | Loss: 1.0225 | Acc: 0.8482 | F1: 0.8482\n",
            "Epoch 068 | Loss: 1.0210 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 069 | Loss: 1.0161 | Acc: 0.8711 | F1: 0.8711\n",
            "Epoch 070 | Loss: 1.0204 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 071 | Loss: 1.0153 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 072 | Loss: 1.0304 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 073 | Loss: 1.0196 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 074 | Loss: 1.0011 | Acc: 0.8431 | F1: 0.8431\n",
            "Epoch 075 | Loss: 1.0650 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 076 | Loss: 1.0138 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 077 | Loss: 1.0233 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 078 | Loss: 1.0331 | Acc: 0.8711 | F1: 0.8711\n",
            "Epoch 079 | Loss: 1.0256 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 080 | Loss: 1.0288 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 081 | Loss: 1.0078 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 082 | Loss: 1.0238 | Acc: 0.8335 | F1: 0.8335\n",
            "Epoch 083 | Loss: 1.0335 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 084 | Loss: 1.0011 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 085 | Loss: 1.0231 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 086 | Loss: 1.0208 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 087 | Loss: 1.0390 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 088 | Loss: 1.0076 | Acc: 0.8039 | F1: 0.8039\n",
            "Epoch 089 | Loss: 1.0276 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 090 | Loss: 1.0145 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 091 | Loss: 1.0237 | Acc: 0.8371 | F1: 0.8371\n",
            "Epoch 092 | Loss: 1.0511 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 093 | Loss: 1.0223 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 094 | Loss: 1.0110 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 095 | Loss: 1.0206 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 096 | Loss: 1.0404 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 097 | Loss: 1.0266 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 098 | Loss: 1.0139 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 099 | Loss: 1.0282 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 100 | Loss: 1.0085 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 001 | Loss: 1.0326 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 002 | Loss: 1.0230 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 003 | Loss: 1.0078 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 004 | Loss: 1.0387 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 005 | Loss: 1.0296 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 006 | Loss: 1.0318 | Acc: 0.8397 | F1: 0.8397\n",
            "Epoch 007 | Loss: 1.0046 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 008 | Loss: 1.0305 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 009 | Loss: 1.0206 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 010 | Loss: 1.0214 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 011 | Loss: 1.0155 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 012 | Loss: 1.0298 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 013 | Loss: 1.0132 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 014 | Loss: 1.0278 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 015 | Loss: 1.0397 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 016 | Loss: 0.9995 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 017 | Loss: 1.0260 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 018 | Loss: 1.0200 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 019 | Loss: 1.0199 | Acc: 0.8770 | F1: 0.8770\n",
            "Epoch 020 | Loss: 1.0108 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 021 | Loss: 1.0433 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 022 | Loss: 1.0109 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 023 | Loss: 1.0168 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 024 | Loss: 1.0351 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 025 | Loss: 1.0183 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 026 | Loss: 1.0387 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 027 | Loss: 1.0401 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 028 | Loss: 1.0169 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 029 | Loss: 1.0297 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 030 | Loss: 1.0086 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 031 | Loss: 1.0073 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 032 | Loss: 1.0648 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 033 | Loss: 1.0025 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 034 | Loss: 1.0064 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 035 | Loss: 1.0236 | Acc: 0.8464 | F1: 0.8464\n",
            "Epoch 036 | Loss: 1.0077 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 037 | Loss: 1.0132 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 038 | Loss: 1.0279 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 039 | Loss: 1.0266 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 040 | Loss: 1.0203 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 041 | Loss: 1.0262 | Acc: 0.8719 | F1: 0.8719\n",
            "Epoch 042 | Loss: 1.0384 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 043 | Loss: 1.0011 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 044 | Loss: 1.0566 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 045 | Loss: 1.0115 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 046 | Loss: 1.0312 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 047 | Loss: 1.0338 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 048 | Loss: 1.0022 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 049 | Loss: 1.0174 | Acc: 0.8412 | F1: 0.8412\n",
            "Epoch 050 | Loss: 1.0242 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 051 | Loss: 1.0281 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 052 | Loss: 1.0159 | Acc: 0.8449 | F1: 0.8449\n",
            "Epoch 053 | Loss: 1.0064 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 054 | Loss: 1.0293 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 055 | Loss: 1.0313 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 056 | Loss: 1.0144 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 057 | Loss: 1.0329 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 058 | Loss: 1.0126 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 059 | Loss: 1.0192 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 060 | Loss: 1.0303 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 061 | Loss: 1.0098 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 062 | Loss: 1.0224 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 063 | Loss: 1.0283 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 064 | Loss: 1.0006 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 065 | Loss: 1.0094 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 066 | Loss: 1.0331 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 067 | Loss: 1.0249 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 068 | Loss: 1.0228 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 069 | Loss: 1.0070 | Acc: 0.8423 | F1: 0.8423\n",
            "Epoch 070 | Loss: 1.0139 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 071 | Loss: 1.0398 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 072 | Loss: 1.0062 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 073 | Loss: 1.0384 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 074 | Loss: 0.9939 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 075 | Loss: 1.0261 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 076 | Loss: 1.0234 | Acc: 0.8704 | F1: 0.8704\n",
            "Epoch 077 | Loss: 1.0003 | Acc: 0.8733 | F1: 0.8733\n",
            "Epoch 078 | Loss: 1.0170 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 079 | Loss: 1.0261 | Acc: 0.8416 | F1: 0.8416\n",
            "Epoch 080 | Loss: 1.0104 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 081 | Loss: 1.0180 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 082 | Loss: 1.0252 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 083 | Loss: 1.0200 | Acc: 0.8419 | F1: 0.8419\n",
            "Epoch 084 | Loss: 1.0291 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 085 | Loss: 1.0039 | Acc: 0.8453 | F1: 0.8453\n",
            "Epoch 086 | Loss: 1.0253 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 087 | Loss: 0.9977 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 088 | Loss: 1.0429 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 089 | Loss: 1.0028 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 090 | Loss: 1.0026 | Acc: 0.8416 | F1: 0.8416\n",
            "Epoch 091 | Loss: 1.0276 | Acc: 0.8763 | F1: 0.8763\n",
            "Epoch 092 | Loss: 1.0345 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 093 | Loss: 1.0126 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 094 | Loss: 1.0270 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 095 | Loss: 1.0157 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 096 | Loss: 0.9914 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 097 | Loss: 1.0121 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 098 | Loss: 1.0353 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 099 | Loss: 1.0141 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 100 | Loss: 0.9983 | Acc: 0.8464 | F1: 0.8464\n",
            "Epoch 001 | Loss: 1.0257 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 002 | Loss: 1.0168 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 003 | Loss: 1.0233 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 004 | Loss: 1.0318 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 005 | Loss: 1.0126 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 006 | Loss: 1.0050 | Acc: 0.8364 | F1: 0.8364\n",
            "Epoch 007 | Loss: 1.0651 | Acc: 0.8397 | F1: 0.8397\n",
            "Epoch 008 | Loss: 1.0097 | Acc: 0.8789 | F1: 0.8789\n",
            "Epoch 009 | Loss: 1.0255 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 010 | Loss: 1.0082 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 011 | Loss: 1.0196 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 012 | Loss: 1.0034 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 013 | Loss: 1.0298 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 014 | Loss: 1.0195 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 015 | Loss: 1.0221 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 016 | Loss: 1.0375 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 017 | Loss: 1.0034 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 018 | Loss: 1.0299 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 019 | Loss: 1.0260 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 020 | Loss: 1.0214 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 021 | Loss: 1.0535 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 022 | Loss: 1.0249 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 023 | Loss: 1.0027 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 024 | Loss: 1.0162 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 025 | Loss: 1.0214 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 026 | Loss: 1.0223 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 027 | Loss: 1.0110 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 028 | Loss: 1.0239 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 029 | Loss: 1.0173 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 030 | Loss: 1.0270 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 031 | Loss: 1.0303 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 032 | Loss: 1.0087 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 033 | Loss: 1.0257 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 034 | Loss: 1.0325 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 035 | Loss: 0.9997 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 036 | Loss: 1.0175 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 037 | Loss: 1.0296 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 038 | Loss: 1.0188 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 039 | Loss: 1.0093 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 040 | Loss: 1.0269 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 041 | Loss: 1.0016 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 042 | Loss: 1.0216 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 043 | Loss: 1.0057 | Acc: 0.8748 | F1: 0.8748\n",
            "Epoch 044 | Loss: 1.0216 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 045 | Loss: 1.0154 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 046 | Loss: 1.0467 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 047 | Loss: 1.0064 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 048 | Loss: 0.9990 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 049 | Loss: 1.0253 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 050 | Loss: 1.0232 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 051 | Loss: 1.0180 | Acc: 0.8468 | F1: 0.8468\n",
            "Epoch 052 | Loss: 1.0109 | Acc: 0.8283 | F1: 0.8283\n",
            "Epoch 053 | Loss: 1.0288 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 054 | Loss: 1.0226 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 055 | Loss: 1.0188 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 056 | Loss: 1.0188 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 057 | Loss: 1.0272 | Acc: 0.8693 | F1: 0.8693\n",
            "Epoch 058 | Loss: 1.0186 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 059 | Loss: 1.0251 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 060 | Loss: 1.0179 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 061 | Loss: 1.0164 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 062 | Loss: 1.0143 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 063 | Loss: 1.0305 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 064 | Loss: 1.0145 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 065 | Loss: 1.0002 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 066 | Loss: 1.0111 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 067 | Loss: 1.0223 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 068 | Loss: 1.0133 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 069 | Loss: 1.0151 | Acc: 0.8696 | F1: 0.8696\n",
            "Epoch 070 | Loss: 0.9958 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 071 | Loss: 1.0247 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 072 | Loss: 1.0132 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 073 | Loss: 0.9934 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 074 | Loss: 1.0296 | Acc: 0.8331 | F1: 0.8331\n",
            "Epoch 075 | Loss: 1.0420 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 076 | Loss: 1.0068 | Acc: 0.8364 | F1: 0.8364\n",
            "Epoch 077 | Loss: 1.0155 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 078 | Loss: 1.0366 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 079 | Loss: 1.0013 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 080 | Loss: 1.0361 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 081 | Loss: 0.9875 | Acc: 0.8792 | F1: 0.8792\n",
            "Epoch 082 | Loss: 1.0239 | Acc: 0.8490 | F1: 0.8490\n",
            "Epoch 083 | Loss: 1.0235 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 084 | Loss: 1.0223 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 085 | Loss: 1.0230 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 086 | Loss: 1.0125 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 087 | Loss: 1.0308 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 088 | Loss: 1.0235 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 089 | Loss: 1.0171 | Acc: 0.8497 | F1: 0.8497\n",
            "Epoch 090 | Loss: 1.0383 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 091 | Loss: 0.9927 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 092 | Loss: 1.0010 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 093 | Loss: 1.0238 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 094 | Loss: 1.0330 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 095 | Loss: 1.0145 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 096 | Loss: 1.0100 | Acc: 0.8744 | F1: 0.8744\n",
            "Epoch 097 | Loss: 1.0244 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 098 | Loss: 1.0355 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 099 | Loss: 1.0063 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 100 | Loss: 1.0243 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 001 | Loss: 1.0085 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 002 | Loss: 0.9984 | Acc: 0.8637 | F1: 0.8637\n",
            "Epoch 003 | Loss: 1.0320 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 004 | Loss: 1.0158 | Acc: 0.8468 | F1: 0.8468\n",
            "Epoch 005 | Loss: 1.0332 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 006 | Loss: 1.0050 | Acc: 0.8752 | F1: 0.8752\n",
            "Epoch 007 | Loss: 1.0224 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 008 | Loss: 1.0201 | Acc: 0.8733 | F1: 0.8733\n",
            "Epoch 009 | Loss: 1.0058 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 010 | Loss: 1.0223 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 011 | Loss: 1.0084 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 012 | Loss: 1.0119 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 013 | Loss: 1.0276 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 014 | Loss: 1.0231 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 015 | Loss: 1.0283 | Acc: 0.8781 | F1: 0.8781\n",
            "Epoch 016 | Loss: 1.0008 | Acc: 0.8674 | F1: 0.8674\n",
            "Epoch 017 | Loss: 1.0325 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 018 | Loss: 1.0087 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 019 | Loss: 1.0107 | Acc: 0.8689 | F1: 0.8689\n",
            "Epoch 020 | Loss: 1.0154 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 021 | Loss: 1.0133 | Acc: 0.8626 | F1: 0.8626\n",
            "Epoch 022 | Loss: 1.0216 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 023 | Loss: 1.0112 | Acc: 0.8486 | F1: 0.8486\n",
            "Epoch 024 | Loss: 0.9982 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 025 | Loss: 1.0467 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 026 | Loss: 0.9873 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 027 | Loss: 1.0287 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 028 | Loss: 0.9856 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 029 | Loss: 1.0252 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 030 | Loss: 1.0103 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 031 | Loss: 1.0173 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 032 | Loss: 1.0084 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 033 | Loss: 1.0510 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 034 | Loss: 0.9966 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 035 | Loss: 1.0088 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 036 | Loss: 1.0521 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 037 | Loss: 1.0017 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 038 | Loss: 0.9858 | Acc: 0.8726 | F1: 0.8726\n",
            "Epoch 039 | Loss: 1.0197 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 040 | Loss: 1.0288 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 041 | Loss: 0.9933 | Acc: 0.8545 | F1: 0.8545\n",
            "Epoch 042 | Loss: 1.0274 | Acc: 0.8427 | F1: 0.8427\n",
            "Epoch 043 | Loss: 0.9941 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 044 | Loss: 1.0114 | Acc: 0.8778 | F1: 0.8778\n",
            "Epoch 045 | Loss: 1.0097 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 046 | Loss: 1.0272 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 047 | Loss: 1.0171 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 048 | Loss: 1.0065 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 049 | Loss: 1.0316 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 050 | Loss: 1.0176 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 051 | Loss: 1.0263 | Acc: 0.8696 | F1: 0.8696\n",
            "Epoch 052 | Loss: 1.0150 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 053 | Loss: 1.0162 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 054 | Loss: 1.0202 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 055 | Loss: 1.0051 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 056 | Loss: 1.0123 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 057 | Loss: 1.0313 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 058 | Loss: 1.0331 | Acc: 0.8630 | F1: 0.8630\n",
            "Epoch 059 | Loss: 1.0059 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 060 | Loss: 1.0299 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 061 | Loss: 1.0209 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 062 | Loss: 0.9821 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 063 | Loss: 1.0405 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 064 | Loss: 1.0174 | Acc: 0.8641 | F1: 0.8641\n",
            "Epoch 065 | Loss: 1.0153 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 066 | Loss: 1.0171 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 067 | Loss: 1.0007 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 068 | Loss: 1.0319 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 069 | Loss: 1.0113 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 070 | Loss: 1.0143 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 071 | Loss: 1.0189 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 072 | Loss: 0.9987 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 073 | Loss: 0.9998 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 074 | Loss: 1.0195 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 075 | Loss: 1.0272 | Acc: 0.8586 | F1: 0.8586\n",
            "Epoch 076 | Loss: 0.9891 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 077 | Loss: 1.0147 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 078 | Loss: 1.0251 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 079 | Loss: 0.9970 | Acc: 0.8471 | F1: 0.8471\n",
            "Epoch 080 | Loss: 1.0358 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 081 | Loss: 1.0142 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 082 | Loss: 1.0045 | Acc: 0.8549 | F1: 0.8549\n",
            "Epoch 083 | Loss: 1.0223 | Acc: 0.8704 | F1: 0.8704\n",
            "Epoch 084 | Loss: 1.0225 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 085 | Loss: 1.0026 | Acc: 0.8648 | F1: 0.8648\n",
            "Epoch 086 | Loss: 1.0277 | Acc: 0.8560 | F1: 0.8560\n",
            "Epoch 087 | Loss: 1.0035 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 088 | Loss: 1.0103 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 089 | Loss: 1.0163 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 090 | Loss: 1.0250 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 091 | Loss: 1.0112 | Acc: 0.8800 | F1: 0.8800\n",
            "Epoch 092 | Loss: 1.0036 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 093 | Loss: 1.0358 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 094 | Loss: 0.9947 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 095 | Loss: 1.0147 | Acc: 0.8456 | F1: 0.8456\n",
            "Epoch 096 | Loss: 1.0443 | Acc: 0.8342 | F1: 0.8342\n",
            "Epoch 097 | Loss: 1.0208 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 098 | Loss: 0.9950 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 099 | Loss: 1.0134 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 100 | Loss: 1.0473 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 001 | Loss: 1.0133 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 002 | Loss: 1.0068 | Acc: 0.8667 | F1: 0.8667\n",
            "Epoch 003 | Loss: 1.0306 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 004 | Loss: 1.0238 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 005 | Loss: 1.0017 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 006 | Loss: 1.0127 | Acc: 0.8696 | F1: 0.8696\n",
            "Epoch 007 | Loss: 1.0072 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 008 | Loss: 1.0446 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 009 | Loss: 1.0313 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 010 | Loss: 1.0037 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 011 | Loss: 1.0343 | Acc: 0.8338 | F1: 0.8338\n",
            "Epoch 012 | Loss: 1.0401 | Acc: 0.8538 | F1: 0.8538\n",
            "Epoch 013 | Loss: 1.0085 | Acc: 0.8733 | F1: 0.8733\n",
            "Epoch 014 | Loss: 1.0002 | Acc: 0.8564 | F1: 0.8564\n",
            "Epoch 015 | Loss: 1.0431 | Acc: 0.8682 | F1: 0.8682\n",
            "Epoch 016 | Loss: 1.0085 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 017 | Loss: 1.0293 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 018 | Loss: 1.0237 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 019 | Loss: 1.0048 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 020 | Loss: 1.0211 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 021 | Loss: 1.0305 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 022 | Loss: 1.0047 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 023 | Loss: 1.0296 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 024 | Loss: 1.0091 | Acc: 0.8656 | F1: 0.8656\n",
            "Epoch 025 | Loss: 1.0178 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 026 | Loss: 1.0183 | Acc: 0.8534 | F1: 0.8534\n",
            "Epoch 027 | Loss: 1.0132 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 028 | Loss: 1.0150 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 029 | Loss: 1.0211 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 030 | Loss: 1.0314 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 031 | Loss: 1.0010 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 032 | Loss: 1.0051 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 033 | Loss: 1.0296 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 034 | Loss: 1.0132 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 035 | Loss: 1.0013 | Acc: 0.8556 | F1: 0.8556\n",
            "Epoch 036 | Loss: 1.0277 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 037 | Loss: 1.0136 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 038 | Loss: 1.0242 | Acc: 0.8593 | F1: 0.8593\n",
            "Epoch 039 | Loss: 1.0057 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 040 | Loss: 0.9941 | Acc: 0.8608 | F1: 0.8608\n",
            "Epoch 041 | Loss: 1.0450 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 042 | Loss: 1.0238 | Acc: 0.8604 | F1: 0.8604\n",
            "Epoch 043 | Loss: 1.0119 | Acc: 0.8567 | F1: 0.8567\n",
            "Epoch 044 | Loss: 1.0018 | Acc: 0.8763 | F1: 0.8763\n",
            "Epoch 045 | Loss: 1.0182 | Acc: 0.8412 | F1: 0.8412\n",
            "Epoch 046 | Loss: 1.0328 | Acc: 0.8652 | F1: 0.8652\n",
            "Epoch 047 | Loss: 1.0163 | Acc: 0.8527 | F1: 0.8527\n",
            "Epoch 048 | Loss: 1.0202 | Acc: 0.8202 | F1: 0.8202\n",
            "Epoch 049 | Loss: 1.0107 | Acc: 0.8501 | F1: 0.8501\n",
            "Epoch 050 | Loss: 1.0445 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 051 | Loss: 1.0161 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 052 | Loss: 1.0193 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 053 | Loss: 1.0280 | Acc: 0.8541 | F1: 0.8541\n",
            "Epoch 054 | Loss: 1.0041 | Acc: 0.8508 | F1: 0.8508\n",
            "Epoch 055 | Loss: 1.0223 | Acc: 0.8660 | F1: 0.8660\n",
            "Epoch 056 | Loss: 1.0216 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 057 | Loss: 1.0082 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 058 | Loss: 1.0091 | Acc: 0.8708 | F1: 0.8708\n",
            "Epoch 059 | Loss: 1.0254 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 060 | Loss: 0.9974 | Acc: 0.8597 | F1: 0.8597\n",
            "Epoch 061 | Loss: 1.0160 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 062 | Loss: 1.0214 | Acc: 0.8504 | F1: 0.8504\n",
            "Epoch 063 | Loss: 1.0114 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 064 | Loss: 1.0177 | Acc: 0.8600 | F1: 0.8600\n",
            "Epoch 065 | Loss: 1.0112 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 066 | Loss: 1.0138 | Acc: 0.8479 | F1: 0.8479\n",
            "Epoch 067 | Loss: 1.0221 | Acc: 0.8578 | F1: 0.8578\n",
            "Epoch 068 | Loss: 1.0281 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 069 | Loss: 1.0068 | Acc: 0.8512 | F1: 0.8512\n",
            "Epoch 070 | Loss: 1.0226 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 071 | Loss: 1.0100 | Acc: 0.8434 | F1: 0.8434\n",
            "Epoch 072 | Loss: 1.0116 | Acc: 0.8460 | F1: 0.8460\n",
            "Epoch 073 | Loss: 1.0328 | Acc: 0.8523 | F1: 0.8523\n",
            "Epoch 074 | Loss: 1.0282 | Acc: 0.8781 | F1: 0.8781\n",
            "Epoch 075 | Loss: 0.9977 | Acc: 0.8619 | F1: 0.8619\n",
            "Epoch 076 | Loss: 1.0330 | Acc: 0.8552 | F1: 0.8552\n",
            "Epoch 077 | Loss: 1.0138 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 078 | Loss: 1.0075 | Acc: 0.8685 | F1: 0.8685\n",
            "Epoch 079 | Loss: 1.0003 | Acc: 0.8615 | F1: 0.8615\n",
            "Epoch 080 | Loss: 1.0542 | Acc: 0.8589 | F1: 0.8589\n",
            "Epoch 081 | Loss: 1.0251 | Acc: 0.8582 | F1: 0.8582\n",
            "Epoch 082 | Loss: 1.0036 | Acc: 0.8575 | F1: 0.8575\n",
            "Epoch 083 | Loss: 1.0319 | Acc: 0.8700 | F1: 0.8700\n",
            "Epoch 084 | Loss: 1.0005 | Acc: 0.8704 | F1: 0.8704\n",
            "Epoch 085 | Loss: 1.0213 | Acc: 0.8663 | F1: 0.8663\n",
            "Epoch 086 | Loss: 1.0011 | Acc: 0.8338 | F1: 0.8338\n",
            "Epoch 087 | Loss: 1.0198 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 088 | Loss: 1.0210 | Acc: 0.8530 | F1: 0.8530\n",
            "Epoch 089 | Loss: 1.0087 | Acc: 0.8645 | F1: 0.8645\n",
            "Epoch 090 | Loss: 1.0088 | Acc: 0.8493 | F1: 0.8493\n",
            "Epoch 091 | Loss: 1.0251 | Acc: 0.8623 | F1: 0.8623\n",
            "Epoch 092 | Loss: 0.9924 | Acc: 0.8519 | F1: 0.8519\n",
            "Epoch 093 | Loss: 1.0268 | Acc: 0.8634 | F1: 0.8634\n",
            "Epoch 094 | Loss: 1.0372 | Acc: 0.8516 | F1: 0.8516\n",
            "Epoch 095 | Loss: 1.0122 | Acc: 0.8571 | F1: 0.8571\n",
            "Epoch 096 | Loss: 0.9939 | Acc: 0.8475 | F1: 0.8475\n",
            "Epoch 097 | Loss: 1.0244 | Acc: 0.8671 | F1: 0.8671\n",
            "Epoch 098 | Loss: 1.0393 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 099 | Loss: 0.9882 | Acc: 0.8612 | F1: 0.8612\n",
            "Epoch 100 | Loss: 1.0058 | Acc: 0.8545 | F1: 0.8545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)\n",
        "print(sum(res)/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2-aX_blGYIj",
        "outputId": "16a017f1-888a-4589-d1c3-2b7dec92f114"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8578286558345642, 0.8574593796159528, 0.869645494830133, 0.8585672082717873, 0.8714918759231906, 0.8685376661742984, 0.8463810930576071, 0.8574593796159528, 0.8508124076809453, 0.8545051698670606]\n",
            "0.8592688330871493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0toSkjAGpnr",
        "outputId": "e6ebb12a-239d-4925-f6ba-fb7eb5e3a461"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 18.38 MB\n",
            "Reserved memory : 44.00 MB\n",
            "Peak allocated memory: 37.85 MB\n"
          ]
        }
      ]
    }
  ]
}