{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/spanGNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro8-WuoyNzH1",
        "outputId": "5bcf7eaa-812a-4a7b-9455-5ae38a8f9767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.11/dist-packages (0.4.0+pt26cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.utils import subgraph"
      ],
      "metadata": {
        "id": "wKuxDFSXahv6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset"
      ],
      "metadata": {
        "id": "lQ0fdkk0Yttb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9hkRXEMgkbV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used:  {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "dq3WEXlhapUL",
        "outputId": "11952700-f09b-4d5d-93d6-33a4282753f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU memory allocated: 95.69 MB\n",
            "Max GPU memory used:  591.23 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S31CjC7BauSq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_size, hid_size)\n",
        "        self.conv2 = GCNConv(hid_size, out_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "def drop_edge(edge_index, drop_prob):\n",
        "    num_edges = edge_index.size(1)\n",
        "    mask = torch.rand(num_edges, device=edge_index.device) > drop_prob\n",
        "    return edge_index[:, mask]\n",
        "\n",
        "\n",
        "def compute_gradient_prob(model, x, edge_index, labels, train_mask):\n",
        "    model.eval()\n",
        "    x = x.clone().detach().requires_grad_(True)\n",
        "\n",
        "    logits = model(x, edge_index)\n",
        "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    grad = x.grad  # [num_nodes, num_features]\n",
        "\n",
        "    src, dst = edge_index\n",
        "    grad_diff = (grad[src] - grad[dst]).norm(p=2, dim=1)  # edge importance score\n",
        "\n",
        "    prob = grad_diff / grad_diff.sum()\n",
        "    return prob.detach()\n",
        "\n",
        "\n",
        "def span_edge_sampling(edge_index, edge_sample_ratio, prob=None):\n",
        "    num_edges = edge_index.size(1)\n",
        "    sample_size = max(int(num_edges * edge_sample_ratio), 1)\n",
        "    if prob is not None:\n",
        "        sampled_indices = torch.multinomial(prob, sample_size, replacement=False)\n",
        "    else:\n",
        "        sampled_indices = torch.randperm(num_edges)[:sample_size]\n",
        "    return edge_index[:, sampled_indices]\n",
        "\n",
        "\n",
        "def train_one_epoch(data, model, optimizer, drop_prob, sample_ratio):\n",
        "    model.train()\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    labels = data.y\n",
        "    train_mask = data.train_mask\n",
        "\n",
        "    # 1) DropEdge\n",
        "    edge_index_dropped = drop_edge(edge_index, drop_prob)\n",
        "\n",
        "    # 2) Compute gradient-based sampling probabilities on dropped graph\n",
        "    prob = compute_gradient_prob(model, x, edge_index_dropped, labels, train_mask)\n",
        "\n",
        "    # 3) Sample edges according to probabilities\n",
        "    edge_index_sampled = span_edge_sampling(edge_index_dropped, sample_ratio, prob)\n",
        "\n",
        "    # 4) Train on sampled subgraph\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x, edge_index_sampled)\n",
        "    loss = F.cross_entropy(out[train_mask], labels[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(data, model):\n",
        "    model.eval()\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    labels = data.y\n",
        "    val_mask = data.val_mask\n",
        "\n",
        "    out = model(x, edge_index)\n",
        "    pred = out[val_mask].argmax(dim=1)\n",
        "    acc = (pred == labels[val_mask]).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = GCN(num_features, 16, num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 100\n",
        "drop_prob = 0.1\n",
        "sample_ratio = 0.5\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss = train_one_epoch(data, model, optimizer, drop_prob, sample_ratio)\n",
        "    val_acc = evaluate(data, model)\n",
        "    print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "SCzb9lIqiPeb",
        "outputId": "8ca49a60-cbf9-44cd-bf68-e75950e72189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Loss: 1.0991 | Val Acc: 0.5520\n",
            "Epoch 002 | Loss: 1.0926 | Val Acc: 0.5140\n",
            "Epoch 003 | Loss: 1.0860 | Val Acc: 0.5660\n",
            "Epoch 004 | Loss: 1.0794 | Val Acc: 0.6300\n",
            "Epoch 005 | Loss: 1.0709 | Val Acc: 0.6940\n",
            "Epoch 006 | Loss: 1.0616 | Val Acc: 0.7160\n",
            "Epoch 007 | Loss: 1.0524 | Val Acc: 0.6940\n",
            "Epoch 008 | Loss: 1.0440 | Val Acc: 0.7100\n",
            "Epoch 009 | Loss: 1.0324 | Val Acc: 0.7160\n",
            "Epoch 010 | Loss: 1.0227 | Val Acc: 0.7260\n",
            "Epoch 011 | Loss: 1.0101 | Val Acc: 0.7400\n",
            "Epoch 012 | Loss: 0.9931 | Val Acc: 0.7280\n",
            "Epoch 013 | Loss: 0.9839 | Val Acc: 0.7220\n",
            "Epoch 014 | Loss: 0.9705 | Val Acc: 0.7280\n",
            "Epoch 015 | Loss: 0.9502 | Val Acc: 0.7300\n",
            "Epoch 016 | Loss: 0.9355 | Val Acc: 0.7340\n",
            "Epoch 017 | Loss: 0.9219 | Val Acc: 0.7260\n",
            "Epoch 018 | Loss: 0.9038 | Val Acc: 0.7380\n",
            "Epoch 019 | Loss: 0.8808 | Val Acc: 0.7400\n",
            "Epoch 020 | Loss: 0.8686 | Val Acc: 0.7400\n",
            "Epoch 021 | Loss: 0.8582 | Val Acc: 0.7460\n",
            "Epoch 022 | Loss: 0.8197 | Val Acc: 0.7440\n",
            "Epoch 023 | Loss: 0.8100 | Val Acc: 0.7480\n",
            "Epoch 024 | Loss: 0.7936 | Val Acc: 0.7500\n",
            "Epoch 025 | Loss: 0.7784 | Val Acc: 0.7540\n",
            "Epoch 026 | Loss: 0.7523 | Val Acc: 0.7520\n",
            "Epoch 027 | Loss: 0.7380 | Val Acc: 0.7500\n",
            "Epoch 028 | Loss: 0.7176 | Val Acc: 0.7500\n",
            "Epoch 029 | Loss: 0.6975 | Val Acc: 0.7520\n",
            "Epoch 030 | Loss: 0.6722 | Val Acc: 0.7560\n",
            "Epoch 031 | Loss: 0.6579 | Val Acc: 0.7560\n",
            "Epoch 032 | Loss: 0.6264 | Val Acc: 0.7620\n",
            "Epoch 033 | Loss: 0.6030 | Val Acc: 0.7640\n",
            "Epoch 034 | Loss: 0.5837 | Val Acc: 0.7640\n",
            "Epoch 035 | Loss: 0.5789 | Val Acc: 0.7660\n",
            "Epoch 036 | Loss: 0.5557 | Val Acc: 0.7680\n",
            "Epoch 037 | Loss: 0.5329 | Val Acc: 0.7680\n",
            "Epoch 038 | Loss: 0.5006 | Val Acc: 0.7680\n",
            "Epoch 039 | Loss: 0.5010 | Val Acc: 0.7720\n",
            "Epoch 040 | Loss: 0.4744 | Val Acc: 0.7720\n",
            "Epoch 041 | Loss: 0.4488 | Val Acc: 0.7700\n",
            "Epoch 042 | Loss: 0.4372 | Val Acc: 0.7720\n",
            "Epoch 043 | Loss: 0.4183 | Val Acc: 0.7700\n",
            "Epoch 044 | Loss: 0.4103 | Val Acc: 0.7700\n",
            "Epoch 045 | Loss: 0.3937 | Val Acc: 0.7720\n",
            "Epoch 046 | Loss: 0.3787 | Val Acc: 0.7680\n",
            "Epoch 047 | Loss: 0.3639 | Val Acc: 0.7680\n",
            "Epoch 048 | Loss: 0.3512 | Val Acc: 0.7680\n",
            "Epoch 049 | Loss: 0.3380 | Val Acc: 0.7700\n",
            "Epoch 050 | Loss: 0.3215 | Val Acc: 0.7720\n",
            "Epoch 051 | Loss: 0.3064 | Val Acc: 0.7740\n",
            "Epoch 052 | Loss: 0.3022 | Val Acc: 0.7700\n",
            "Epoch 053 | Loss: 0.2814 | Val Acc: 0.7700\n",
            "Epoch 054 | Loss: 0.2500 | Val Acc: 0.7660\n",
            "Epoch 055 | Loss: 0.2647 | Val Acc: 0.7680\n",
            "Epoch 056 | Loss: 0.2514 | Val Acc: 0.7680\n",
            "Epoch 057 | Loss: 0.2435 | Val Acc: 0.7720\n",
            "Epoch 058 | Loss: 0.2393 | Val Acc: 0.7760\n",
            "Epoch 059 | Loss: 0.2300 | Val Acc: 0.7760\n",
            "Epoch 060 | Loss: 0.2062 | Val Acc: 0.7740\n",
            "Epoch 061 | Loss: 0.1947 | Val Acc: 0.7760\n",
            "Epoch 062 | Loss: 0.1995 | Val Acc: 0.7780\n",
            "Epoch 063 | Loss: 0.1864 | Val Acc: 0.7740\n",
            "Epoch 064 | Loss: 0.1812 | Val Acc: 0.7720\n",
            "Epoch 065 | Loss: 0.1838 | Val Acc: 0.7720\n",
            "Epoch 066 | Loss: 0.1759 | Val Acc: 0.7680\n",
            "Epoch 067 | Loss: 0.1596 | Val Acc: 0.7680\n",
            "Epoch 068 | Loss: 0.1497 | Val Acc: 0.7680\n",
            "Epoch 069 | Loss: 0.1556 | Val Acc: 0.7740\n",
            "Epoch 070 | Loss: 0.1553 | Val Acc: 0.7740\n",
            "Epoch 071 | Loss: 0.1338 | Val Acc: 0.7720\n",
            "Epoch 072 | Loss: 0.1350 | Val Acc: 0.7680\n",
            "Epoch 073 | Loss: 0.1316 | Val Acc: 0.7660\n",
            "Epoch 074 | Loss: 0.1339 | Val Acc: 0.7680\n",
            "Epoch 075 | Loss: 0.1270 | Val Acc: 0.7700\n",
            "Epoch 076 | Loss: 0.1155 | Val Acc: 0.7700\n",
            "Epoch 077 | Loss: 0.1186 | Val Acc: 0.7700\n",
            "Epoch 078 | Loss: 0.1172 | Val Acc: 0.7700\n",
            "Epoch 079 | Loss: 0.1146 | Val Acc: 0.7700\n",
            "Epoch 080 | Loss: 0.1086 | Val Acc: 0.7660\n",
            "Epoch 081 | Loss: 0.0995 | Val Acc: 0.7660\n",
            "Epoch 082 | Loss: 0.0946 | Val Acc: 0.7660\n",
            "Epoch 083 | Loss: 0.0933 | Val Acc: 0.7660\n",
            "Epoch 084 | Loss: 0.0976 | Val Acc: 0.7680\n",
            "Epoch 085 | Loss: 0.0937 | Val Acc: 0.7680\n",
            "Epoch 086 | Loss: 0.0838 | Val Acc: 0.7680\n",
            "Epoch 087 | Loss: 0.0812 | Val Acc: 0.7660\n",
            "Epoch 088 | Loss: 0.0855 | Val Acc: 0.7700\n",
            "Epoch 089 | Loss: 0.0770 | Val Acc: 0.7720\n",
            "Epoch 090 | Loss: 0.0726 | Val Acc: 0.7760\n",
            "Epoch 091 | Loss: 0.0775 | Val Acc: 0.7760\n",
            "Epoch 092 | Loss: 0.0749 | Val Acc: 0.7760\n",
            "Epoch 093 | Loss: 0.0704 | Val Acc: 0.7720\n",
            "Epoch 094 | Loss: 0.0717 | Val Acc: 0.7720\n",
            "Epoch 095 | Loss: 0.0670 | Val Acc: 0.7700\n",
            "Epoch 096 | Loss: 0.0623 | Val Acc: 0.7660\n",
            "Epoch 097 | Loss: 0.0656 | Val Acc: 0.7660\n",
            "Epoch 098 | Loss: 0.0595 | Val Acc: 0.7660\n",
            "Epoch 099 | Loss: 0.0591 | Val Acc: 0.7660\n",
            "Epoch 100 | Loss: 0.0593 | Val Acc: 0.7640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used:  {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "T4gL6uIQiTbf",
        "outputId": "90646bc7-3a2c-414b-bac2-f3ce573064d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 55.94 MB\n",
            "Max GPU memory used:  591.23 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_size, hid_size)\n",
        "        self.conv2 = GCNConv(hid_size, out_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class SpanGNN:\n",
        "    def __init__(self, model, edge_ratio=0.3, strategy='vm'):\n",
        "        self.model = model\n",
        "        self.edge_ratio = edge_ratio  # α_up\n",
        "        self.strategy = strategy  # 'vm' or 'gnr'\n",
        "        self.current_edges = None\n",
        "\n",
        "    def _compute_degrees(self, edge_index, num_nodes):\n",
        "        \"\"\"Compute node degrees with dtype matching edge_index.\"\"\"\n",
        "        return torch.zeros(num_nodes, dtype=edge_index.dtype, device=edge_index.device).scatter_add_(\n",
        "            0, edge_index[0], torch.ones_like(edge_index[0])\n",
        "        )\n",
        "\n",
        "    def _variance_minimized_sampling(self, edge_index, degrees):\n",
        "        \"\"\"Variance-minimized sampling (p_e ∝ 1/deg(u) + 1/deg(v)).\"\"\"\n",
        "        src, dst = edge_index\n",
        "        prob = 1.0 / (degrees[src] + degrees[dst] + 1e-10)  # Avoid division by zero\n",
        "        return prob / prob.sum()\n",
        "\n",
        "    def _gradient_noise_reduced_sampling(self, model, x, edge_index, labels, train_mask):\n",
        "        \"\"\"Gradient-noise-reduced sampling (p_e ∝ ‖∇W L‖).\"\"\"\n",
        "        x = x.clone().detach().requires_grad_(True)\n",
        "        model.eval()\n",
        "        logits = model(x, edge_index)\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "        loss.backward()\n",
        "        grad_norm = x.grad.norm(p=2, dim=1)[edge_index[0]]  # Use source node gradients\n",
        "        prob = grad_norm / (grad_norm.sum() + 1e-10)\n",
        "        return prob.detach()\n",
        "\n",
        "    def update_subgraph(self, data, edge_index_full):\n",
        "        \"\"\"Update subgraph with quality-aware edges.\"\"\"\n",
        "        num_edges_to_add = int(self.edge_ratio * edge_index_full.size(1))\n",
        "\n",
        "        # Step 1: Random candidate pool\n",
        "        candidate_idx = torch.randperm(edge_index_full.size(1))[:min(10_000, edge_index_full.size(1))]\n",
        "        edge_candidates = edge_index_full[:, candidate_idx]\n",
        "\n",
        "        # Step 2: Importance sampling\n",
        "        if self.strategy == 'vm':\n",
        "            degrees = self._compute_degrees(edge_index_full, data.num_nodes)\n",
        "            prob = self._variance_minimized_sampling(edge_candidates, degrees)\n",
        "        else:\n",
        "            prob = self._gradient_noise_reduced_sampling(\n",
        "                self.model, data.x, edge_candidates, data.y, data.train_mask\n",
        "            )\n",
        "\n",
        "        # Sample edges\n",
        "        selected = torch.multinomial(prob, min(num_edges_to_add, prob.size(0)), replacement=False)\n",
        "        new_edges = edge_candidates[:, selected]\n",
        "\n",
        "        # Merge with existing edges\n",
        "        if self.current_edges is None:\n",
        "            self.current_edges = new_edges\n",
        "        else:\n",
        "            self.current_edges = torch.unique(\n",
        "                torch.cat([self.current_edges, new_edges], dim=1),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "        # Enforce edge budget\n",
        "        if self.current_edges.size(1) > num_edges_to_add:\n",
        "            keep = torch.randperm(self.current_edges.size(1))[:num_edges_to_add]\n",
        "            self.current_edges = self.current_edges[:, keep]\n",
        "\n",
        "        return self.current_edges\n",
        "\n",
        "    def train(self, data, epochs=100):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Update subgraph\n",
        "            edge_index = self.update_subgraph(data, data.edge_index)\n",
        "\n",
        "            # Train step\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = self.model(data.x, edge_index)\n",
        "            loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            val_acc = self.evaluate(data)\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, data):\n",
        "        self.model.eval()\n",
        "        out = self.model(data.x, self.current_edges)\n",
        "        pred = out[data.val_mask].argmax(dim=1)\n",
        "        return (pred == data.y[data.val_mask]).float().mean().item()"
      ],
      "metadata": {
        "id": "KgyhPwngn089"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(data.num_features, 16, dataset.num_classes).to(device)\n",
        "span_gnn = SpanGNN(model, edge_ratio=0.3, strategy='vm')  # or 'gnr'\n",
        "\n",
        "# Train\n",
        "span_gnn.train(data, epochs=100)"
      ],
      "metadata": {
        "id": "It0P8WImn5uA",
        "outputId": "90b1de77-3726-4ee6-9a64-c97e0e44eed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000 | Loss: 1.0974 | Val Acc: 0.4020\n",
            "Epoch 001 | Loss: 1.0884 | Val Acc: 0.5580\n",
            "Epoch 002 | Loss: 1.0820 | Val Acc: 0.5780\n",
            "Epoch 003 | Loss: 1.0664 | Val Acc: 0.6020\n",
            "Epoch 004 | Loss: 1.0653 | Val Acc: 0.6080\n",
            "Epoch 005 | Loss: 1.0468 | Val Acc: 0.6240\n",
            "Epoch 006 | Loss: 1.0381 | Val Acc: 0.6420\n",
            "Epoch 007 | Loss: 1.0203 | Val Acc: 0.6720\n",
            "Epoch 008 | Loss: 1.0215 | Val Acc: 0.6840\n",
            "Epoch 009 | Loss: 1.0087 | Val Acc: 0.7000\n",
            "Epoch 010 | Loss: 0.9788 | Val Acc: 0.7240\n",
            "Epoch 011 | Loss: 0.9671 | Val Acc: 0.7120\n",
            "Epoch 012 | Loss: 0.9544 | Val Acc: 0.7160\n",
            "Epoch 013 | Loss: 0.9169 | Val Acc: 0.7020\n",
            "Epoch 014 | Loss: 0.9084 | Val Acc: 0.7120\n",
            "Epoch 015 | Loss: 0.9141 | Val Acc: 0.7120\n",
            "Epoch 016 | Loss: 0.8653 | Val Acc: 0.7280\n",
            "Epoch 017 | Loss: 0.8542 | Val Acc: 0.7420\n",
            "Epoch 018 | Loss: 0.8698 | Val Acc: 0.7380\n",
            "Epoch 019 | Loss: 0.8282 | Val Acc: 0.7360\n",
            "Epoch 020 | Loss: 0.8045 | Val Acc: 0.7380\n",
            "Epoch 021 | Loss: 0.7740 | Val Acc: 0.7240\n",
            "Epoch 022 | Loss: 0.7577 | Val Acc: 0.7260\n",
            "Epoch 023 | Loss: 0.7739 | Val Acc: 0.7220\n",
            "Epoch 024 | Loss: 0.7286 | Val Acc: 0.7260\n",
            "Epoch 025 | Loss: 0.6753 | Val Acc: 0.7260\n",
            "Epoch 026 | Loss: 0.6749 | Val Acc: 0.7260\n",
            "Epoch 027 | Loss: 0.6689 | Val Acc: 0.7260\n",
            "Epoch 028 | Loss: 0.6612 | Val Acc: 0.7260\n",
            "Epoch 029 | Loss: 0.6672 | Val Acc: 0.7280\n",
            "Epoch 030 | Loss: 0.6199 | Val Acc: 0.7420\n",
            "Epoch 031 | Loss: 0.6442 | Val Acc: 0.7300\n",
            "Epoch 032 | Loss: 0.5828 | Val Acc: 0.7220\n",
            "Epoch 033 | Loss: 0.5983 | Val Acc: 0.7400\n",
            "Epoch 034 | Loss: 0.5882 | Val Acc: 0.7400\n",
            "Epoch 035 | Loss: 0.5660 | Val Acc: 0.7280\n",
            "Epoch 036 | Loss: 0.5705 | Val Acc: 0.7440\n",
            "Epoch 037 | Loss: 0.5002 | Val Acc: 0.7420\n",
            "Epoch 038 | Loss: 0.5229 | Val Acc: 0.7340\n",
            "Epoch 039 | Loss: 0.5066 | Val Acc: 0.7280\n",
            "Epoch 040 | Loss: 0.4549 | Val Acc: 0.7300\n",
            "Epoch 041 | Loss: 0.5174 | Val Acc: 0.7380\n",
            "Epoch 042 | Loss: 0.4771 | Val Acc: 0.7140\n",
            "Epoch 043 | Loss: 0.4078 | Val Acc: 0.7220\n",
            "Epoch 044 | Loss: 0.4296 | Val Acc: 0.7280\n",
            "Epoch 045 | Loss: 0.4387 | Val Acc: 0.7360\n",
            "Epoch 046 | Loss: 0.4127 | Val Acc: 0.7420\n",
            "Epoch 047 | Loss: 0.4147 | Val Acc: 0.7340\n",
            "Epoch 048 | Loss: 0.3544 | Val Acc: 0.7240\n",
            "Epoch 049 | Loss: 0.3431 | Val Acc: 0.7200\n",
            "Epoch 050 | Loss: 0.3205 | Val Acc: 0.7520\n",
            "Epoch 051 | Loss: 0.3577 | Val Acc: 0.7480\n",
            "Epoch 052 | Loss: 0.3630 | Val Acc: 0.7260\n",
            "Epoch 053 | Loss: 0.3680 | Val Acc: 0.7460\n",
            "Epoch 054 | Loss: 0.3443 | Val Acc: 0.7460\n",
            "Epoch 055 | Loss: 0.3322 | Val Acc: 0.7420\n",
            "Epoch 056 | Loss: 0.3841 | Val Acc: 0.7420\n",
            "Epoch 057 | Loss: 0.3322 | Val Acc: 0.7380\n",
            "Epoch 058 | Loss: 0.3279 | Val Acc: 0.7420\n",
            "Epoch 059 | Loss: 0.2646 | Val Acc: 0.7280\n",
            "Epoch 060 | Loss: 0.2961 | Val Acc: 0.7400\n",
            "Epoch 061 | Loss: 0.3070 | Val Acc: 0.7300\n",
            "Epoch 062 | Loss: 0.2967 | Val Acc: 0.7220\n",
            "Epoch 063 | Loss: 0.2732 | Val Acc: 0.7160\n",
            "Epoch 064 | Loss: 0.2248 | Val Acc: 0.7080\n",
            "Epoch 065 | Loss: 0.2872 | Val Acc: 0.7140\n",
            "Epoch 066 | Loss: 0.2778 | Val Acc: 0.7100\n",
            "Epoch 067 | Loss: 0.2719 | Val Acc: 0.7340\n",
            "Epoch 068 | Loss: 0.2792 | Val Acc: 0.7400\n",
            "Epoch 069 | Loss: 0.2423 | Val Acc: 0.7420\n",
            "Epoch 070 | Loss: 0.2295 | Val Acc: 0.7260\n",
            "Epoch 071 | Loss: 0.2341 | Val Acc: 0.7380\n",
            "Epoch 072 | Loss: 0.2414 | Val Acc: 0.7400\n",
            "Epoch 073 | Loss: 0.2583 | Val Acc: 0.7420\n",
            "Epoch 074 | Loss: 0.2518 | Val Acc: 0.7260\n",
            "Epoch 075 | Loss: 0.2342 | Val Acc: 0.7400\n",
            "Epoch 076 | Loss: 0.1820 | Val Acc: 0.7340\n",
            "Epoch 077 | Loss: 0.2678 | Val Acc: 0.7420\n",
            "Epoch 078 | Loss: 0.2584 | Val Acc: 0.7460\n",
            "Epoch 079 | Loss: 0.2298 | Val Acc: 0.7500\n",
            "Epoch 080 | Loss: 0.1652 | Val Acc: 0.7460\n",
            "Epoch 081 | Loss: 0.1523 | Val Acc: 0.7480\n",
            "Epoch 082 | Loss: 0.2040 | Val Acc: 0.7620\n",
            "Epoch 083 | Loss: 0.2765 | Val Acc: 0.7500\n",
            "Epoch 084 | Loss: 0.2239 | Val Acc: 0.7540\n",
            "Epoch 085 | Loss: 0.2225 | Val Acc: 0.7480\n",
            "Epoch 086 | Loss: 0.1713 | Val Acc: 0.7420\n",
            "Epoch 087 | Loss: 0.2045 | Val Acc: 0.7360\n",
            "Epoch 088 | Loss: 0.2687 | Val Acc: 0.7420\n",
            "Epoch 089 | Loss: 0.1266 | Val Acc: 0.7420\n",
            "Epoch 090 | Loss: 0.1767 | Val Acc: 0.7420\n",
            "Epoch 091 | Loss: 0.1480 | Val Acc: 0.7420\n",
            "Epoch 092 | Loss: 0.1413 | Val Acc: 0.7400\n",
            "Epoch 093 | Loss: 0.1108 | Val Acc: 0.7320\n",
            "Epoch 094 | Loss: 0.1781 | Val Acc: 0.7300\n",
            "Epoch 095 | Loss: 0.1731 | Val Acc: 0.7140\n",
            "Epoch 096 | Loss: 0.1769 | Val Acc: 0.7180\n",
            "Epoch 097 | Loss: 0.1897 | Val Acc: 0.7300\n",
            "Epoch 098 | Loss: 0.1425 | Val Acc: 0.7260\n",
            "Epoch 099 | Loss: 0.1738 | Val Acc: 0.7180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used:  {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "QDZyzI5JgrT4",
        "outputId": "61b92459-83b9-4677-8ca8-f2ff5517b332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 96.34 MB\n",
            "Max GPU memory used:  591.23 MB\n"
          ]
        }
      ]
    }
  ]
}