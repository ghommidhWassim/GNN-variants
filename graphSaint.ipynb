{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/graphSaint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_MxXKQdoYf_",
        "outputId": "fc198e0f-9214-4405-fbb0-077a9f671c4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_cluster-1.6.3+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124 torch_spline_conv-1.2.2+pt26cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ruzytV_Nk9LG"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --quiet\n",
        "# !pip install scipy numpy --quiet\n",
        "# !git clone https://github.com/graphsaint/graphsaint.git  # if you want to use official repo, or upload your own files\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import time\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.datasets import Planetoid, Amazon\n",
        "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "from torch_geometric.loader import GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler\n",
        "from sklearn.metrics import f1_score       # add this import once, near the top\n",
        "import torch.nn as nn\n",
        "import json\n",
        "# (You will have to upload or place the GraphSAINT modules or install them if available)\n",
        "# For simplicity, assume graphsaint package is already in your environment or uploaded as files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ------------------- Load Dataset -------------------\n",
        "\n",
        "def clean_gpu_memory():\n",
        "    \"\"\"Cleans GPU memory without fully resetting the CUDA context\"\"\"\n",
        "    import gc\n",
        "    gc.collect()  # Python garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # PyTorch cache\n",
        "        torch.cuda.reset_peak_memory_stats()  # Reset tracking\n",
        "        print(f\"Memory after cleanup: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "aqMSSoVJ0rXT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJTVRr2c_kYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZhQYYlP5t8K",
        "outputId": "c06e8178-deff-4c06-830d-ed3088411617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compute GraphSAINT normalization: : 198778it [00:00, 927460.84it/s]                          \n",
            "Compute GraphSAINT normalization: : 198592it [03:03, 1085.04it/s]                          \n",
            "Compute GraphSAINT normalization: : 197445it [00:00, 2301321.51it/s]        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "q75oq00V834G"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csFgXNhR_A7-",
        "outputId": "ad7506c0-a0ce-41ea-e937-d61144441b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-596637002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-3487925365.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_channels)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index, batch.edge_norm)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# ------------------- Evaluation Function -------------------\n",
        "@torch.no_grad()\n",
        "#def test(model, full_data):\n",
        " #   model.eval()\n",
        "  #  out = model(full_data.x.to(device), full_data.edge_index.to(device))\n",
        "  #  pred = out.argmax(dim=1)\n",
        "  #  correct = (pred[full_data.test_mask] == full_data.y[full_data.test_mask].to(device)).sum()\n",
        "  #  acc = int(correct) / int(full_data.test_mask.sum())\n",
        "   # return acc\n",
        "\n",
        "def test(model, full_data):\n",
        "    model.eval()\n",
        "    out = model(full_data.x.to(device), full_data.edge_index.to(device))\n",
        "\n",
        "    # predictions for every node\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    # isolate test nodes (keep everything on the SAME device, then move to CPU once)\n",
        "    test_mask = full_data.test_mask\n",
        "    y_true = full_data.y[test_mask].cpu()\n",
        "    y_pred = pred[test_mask].cpu()\n",
        "\n",
        "    # accuracy\n",
        "    acc = (y_pred == y_true).float().mean().item()\n",
        "\n",
        "    # micro‑F1\n",
        "    f1_micro = f1_score(y_true.numpy(), y_pred.numpy(), average='micro')\n",
        "\n",
        "    return acc, f1_micro\n",
        "# ------------------- Run Training -------------------\n",
        "def run(loader, method_name):\n",
        "    # Use the global num_classes variable instead of trying to access it from the data object\n",
        "\n",
        "\n",
        "    for epoch in range(1, 101):\n",
        "        loss = train(model, loader, optimizer, criterion)\n",
        "        print(f'{method_name} | Epoch {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "    acc,test_f1 = test(model, data)\n",
        "    print(f'{method_name} | Final Test Accuracy: {acc:.4f} | Test F1 (micro): {test_f1:.4f}\")')\n",
        "    return acc,test_f1\n",
        "\n"
      ],
      "metadata": {
        "id": "rXSiTzZo9Aaf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "def dataset_load():\n",
        "    print(f\"Using device: {device}\")\n",
        "    dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "    data = dataset[0].to(device)\n",
        "    return dataset.num_features, data, dataset.num_classes\n",
        "\n",
        "num_features, data, num_classes = dataset_load()\n",
        "data = data.cpu()\n",
        "#loader_SAINT_256_node = GraphSAINTNodeSampler(data, batch_size=500, num_steps=4, sample_coverage=10)\n",
        "\n",
        "#loader_SAINT_256_edge = GraphSAINTEdgeSampler(data, batch_size=500, num_steps=4, sample_coverage=10)\n",
        "\n",
        "loader_SAINT_256_RW = GraphSAINTRandomWalkSampler(data, batch_size=128, walk_length=2, num_steps=4, sample_coverage=10)\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "model = GCN(\n",
        "        in_channels=num_features,\n",
        "        hidden_channels=64,\n",
        "        out_channels=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# ------------------- Execute Training for Each Loader -------------------\n",
        "\n",
        "#run(loader_SAINT_256_node, \"GraphSAINT-NodeSampler\")\n",
        "\n",
        "#run(loader_SAINT_256_edge, \"GraphSAINT-EdgeSampler\")\n",
        "start_time = time.time()\n",
        "\n",
        "test_acc,f1_micro =  run(loader_SAINT_256_RW, \"GraphSAINT-RandomWalkSampler\")\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "vaTz1kcJKacD",
        "outputId": "081d84c3-c81a-44e2-a680-497ba600b31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 32.35 MB\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compute GraphSAINT normalization: : 197171it [00:00, 700366.12it/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 32.35 MB\n",
            "Max GPU memory used: 71.91 MB\n",
            "GraphSAINT-RandomWalkSampler | Epoch 001, Loss: 1.0769\n",
            "GraphSAINT-RandomWalkSampler | Epoch 002, Loss: 1.0295\n",
            "GraphSAINT-RandomWalkSampler | Epoch 003, Loss: 1.0015\n",
            "GraphSAINT-RandomWalkSampler | Epoch 004, Loss: 0.9710\n",
            "GraphSAINT-RandomWalkSampler | Epoch 005, Loss: 0.9126\n",
            "GraphSAINT-RandomWalkSampler | Epoch 006, Loss: 0.8690\n",
            "GraphSAINT-RandomWalkSampler | Epoch 007, Loss: 0.8127\n",
            "GraphSAINT-RandomWalkSampler | Epoch 008, Loss: 0.8003\n",
            "GraphSAINT-RandomWalkSampler | Epoch 009, Loss: 0.7396\n",
            "GraphSAINT-RandomWalkSampler | Epoch 010, Loss: 0.7762\n",
            "GraphSAINT-RandomWalkSampler | Epoch 011, Loss: 0.6972\n",
            "GraphSAINT-RandomWalkSampler | Epoch 012, Loss: 0.6855\n",
            "GraphSAINT-RandomWalkSampler | Epoch 013, Loss: 0.6769\n",
            "GraphSAINT-RandomWalkSampler | Epoch 014, Loss: 0.6255\n",
            "GraphSAINT-RandomWalkSampler | Epoch 015, Loss: 0.6379\n",
            "GraphSAINT-RandomWalkSampler | Epoch 016, Loss: 0.6497\n",
            "GraphSAINT-RandomWalkSampler | Epoch 017, Loss: 0.6833\n",
            "GraphSAINT-RandomWalkSampler | Epoch 018, Loss: 0.5879\n",
            "GraphSAINT-RandomWalkSampler | Epoch 019, Loss: 0.6022\n",
            "GraphSAINT-RandomWalkSampler | Epoch 020, Loss: 0.5968\n",
            "GraphSAINT-RandomWalkSampler | Epoch 021, Loss: 0.5898\n",
            "GraphSAINT-RandomWalkSampler | Epoch 022, Loss: 0.6219\n",
            "GraphSAINT-RandomWalkSampler | Epoch 023, Loss: 0.5822\n",
            "GraphSAINT-RandomWalkSampler | Epoch 024, Loss: 0.5843\n",
            "GraphSAINT-RandomWalkSampler | Epoch 025, Loss: 0.5983\n",
            "GraphSAINT-RandomWalkSampler | Epoch 026, Loss: 0.5959\n",
            "GraphSAINT-RandomWalkSampler | Epoch 027, Loss: 0.5609\n",
            "GraphSAINT-RandomWalkSampler | Epoch 028, Loss: 0.5386\n",
            "GraphSAINT-RandomWalkSampler | Epoch 029, Loss: 0.5546\n",
            "GraphSAINT-RandomWalkSampler | Epoch 030, Loss: 0.5451\n",
            "GraphSAINT-RandomWalkSampler | Epoch 031, Loss: 0.5349\n",
            "GraphSAINT-RandomWalkSampler | Epoch 032, Loss: 0.6471\n",
            "GraphSAINT-RandomWalkSampler | Epoch 033, Loss: 0.5891\n",
            "GraphSAINT-RandomWalkSampler | Epoch 034, Loss: 0.6013\n",
            "GraphSAINT-RandomWalkSampler | Epoch 035, Loss: 0.5511\n",
            "GraphSAINT-RandomWalkSampler | Epoch 036, Loss: 0.5566\n",
            "GraphSAINT-RandomWalkSampler | Epoch 037, Loss: 0.6948\n",
            "GraphSAINT-RandomWalkSampler | Epoch 038, Loss: 0.5622\n",
            "GraphSAINT-RandomWalkSampler | Epoch 039, Loss: 0.5720\n",
            "GraphSAINT-RandomWalkSampler | Epoch 040, Loss: 0.5345\n",
            "GraphSAINT-RandomWalkSampler | Epoch 041, Loss: 0.5716\n",
            "GraphSAINT-RandomWalkSampler | Epoch 042, Loss: 0.5468\n",
            "GraphSAINT-RandomWalkSampler | Epoch 043, Loss: 0.5398\n",
            "GraphSAINT-RandomWalkSampler | Epoch 044, Loss: 0.5657\n",
            "GraphSAINT-RandomWalkSampler | Epoch 045, Loss: 0.5626\n",
            "GraphSAINT-RandomWalkSampler | Epoch 046, Loss: 0.5371\n",
            "GraphSAINT-RandomWalkSampler | Epoch 047, Loss: 0.5365\n",
            "GraphSAINT-RandomWalkSampler | Epoch 048, Loss: 0.5510\n",
            "GraphSAINT-RandomWalkSampler | Epoch 049, Loss: 0.5998\n",
            "GraphSAINT-RandomWalkSampler | Epoch 050, Loss: 0.5132\n",
            "GraphSAINT-RandomWalkSampler | Epoch 051, Loss: 0.5371\n",
            "GraphSAINT-RandomWalkSampler | Epoch 052, Loss: 0.5453\n",
            "GraphSAINT-RandomWalkSampler | Epoch 053, Loss: 0.5813\n",
            "GraphSAINT-RandomWalkSampler | Epoch 054, Loss: 0.5278\n",
            "GraphSAINT-RandomWalkSampler | Epoch 055, Loss: 0.4926\n",
            "GraphSAINT-RandomWalkSampler | Epoch 056, Loss: 0.5587\n",
            "GraphSAINT-RandomWalkSampler | Epoch 057, Loss: 0.5178\n",
            "GraphSAINT-RandomWalkSampler | Epoch 058, Loss: 0.5167\n",
            "GraphSAINT-RandomWalkSampler | Epoch 059, Loss: 0.5138\n",
            "GraphSAINT-RandomWalkSampler | Epoch 060, Loss: 0.5164\n",
            "GraphSAINT-RandomWalkSampler | Epoch 061, Loss: 0.5272\n",
            "GraphSAINT-RandomWalkSampler | Epoch 062, Loss: 0.5805\n",
            "GraphSAINT-RandomWalkSampler | Epoch 063, Loss: 0.5380\n",
            "GraphSAINT-RandomWalkSampler | Epoch 064, Loss: 0.5138\n",
            "GraphSAINT-RandomWalkSampler | Epoch 065, Loss: 0.5072\n",
            "GraphSAINT-RandomWalkSampler | Epoch 066, Loss: 0.5158\n",
            "GraphSAINT-RandomWalkSampler | Epoch 067, Loss: 0.4916\n",
            "GraphSAINT-RandomWalkSampler | Epoch 068, Loss: 0.5152\n",
            "GraphSAINT-RandomWalkSampler | Epoch 069, Loss: 0.5035\n",
            "GraphSAINT-RandomWalkSampler | Epoch 070, Loss: 0.5157\n",
            "GraphSAINT-RandomWalkSampler | Epoch 071, Loss: 0.5200\n",
            "GraphSAINT-RandomWalkSampler | Epoch 072, Loss: 0.5464\n",
            "GraphSAINT-RandomWalkSampler | Epoch 073, Loss: 0.5414\n",
            "GraphSAINT-RandomWalkSampler | Epoch 074, Loss: 0.5148\n",
            "GraphSAINT-RandomWalkSampler | Epoch 075, Loss: 0.5958\n",
            "GraphSAINT-RandomWalkSampler | Epoch 076, Loss: 0.5187\n",
            "GraphSAINT-RandomWalkSampler | Epoch 077, Loss: 0.5192\n",
            "GraphSAINT-RandomWalkSampler | Epoch 078, Loss: 0.5085\n",
            "GraphSAINT-RandomWalkSampler | Epoch 079, Loss: 0.5687\n",
            "GraphSAINT-RandomWalkSampler | Epoch 080, Loss: 0.4895\n",
            "GraphSAINT-RandomWalkSampler | Epoch 081, Loss: 0.6994\n",
            "GraphSAINT-RandomWalkSampler | Epoch 082, Loss: 0.5483\n",
            "GraphSAINT-RandomWalkSampler | Epoch 083, Loss: 0.5295\n",
            "GraphSAINT-RandomWalkSampler | Epoch 084, Loss: 0.5530\n",
            "GraphSAINT-RandomWalkSampler | Epoch 085, Loss: 0.5167\n",
            "GraphSAINT-RandomWalkSampler | Epoch 086, Loss: 0.5483\n",
            "GraphSAINT-RandomWalkSampler | Epoch 087, Loss: 0.5385\n",
            "GraphSAINT-RandomWalkSampler | Epoch 088, Loss: 0.5437\n",
            "GraphSAINT-RandomWalkSampler | Epoch 089, Loss: 0.5928\n",
            "GraphSAINT-RandomWalkSampler | Epoch 090, Loss: 0.6049\n",
            "GraphSAINT-RandomWalkSampler | Epoch 091, Loss: 0.5172\n",
            "GraphSAINT-RandomWalkSampler | Epoch 092, Loss: 0.5193\n",
            "GraphSAINT-RandomWalkSampler | Epoch 093, Loss: 0.5307\n",
            "GraphSAINT-RandomWalkSampler | Epoch 094, Loss: 0.5262\n",
            "GraphSAINT-RandomWalkSampler | Epoch 095, Loss: 0.4974\n",
            "GraphSAINT-RandomWalkSampler | Epoch 096, Loss: 0.5218\n",
            "GraphSAINT-RandomWalkSampler | Epoch 097, Loss: 0.5229\n",
            "GraphSAINT-RandomWalkSampler | Epoch 098, Loss: 0.5661\n",
            "GraphSAINT-RandomWalkSampler | Epoch 099, Loss: 0.5654\n",
            "GraphSAINT-RandomWalkSampler | Epoch 100, Loss: 0.5811\n",
            "GraphSAINT-RandomWalkSampler | Final Test Accuracy: 0.8630 | Test F1 (micro): 0.8630\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "QszHC7qEX3zc",
        "outputId": "3de38a82-5ae6-4abd-abdf-3b0981707b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 17.72 MB\n",
            "Max GPU memory used: 121.68 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAINT\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":71.91\n",
        "}\n",
        "\n",
        "with open(\"graphSAINT_rw_pubmed_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "buAZqY5HQgy2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cora**"
      ],
      "metadata": {
        "id": "bvNoGPbzZGJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "def dataset_load():\n",
        "    print(f\"Using device: {device}\")\n",
        "    dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "    data = dataset[0].to(device)\n",
        "    return dataset.num_features, data, dataset.num_classes\n",
        "\n",
        "num_features, data, num_classes = dataset_load()\n",
        "data = data.cpu()\n",
        "#loader_SAINT_256_node = GraphSAINTNodeSampler(data, batch_size=128, num_steps=4, sample_coverage=10)\n",
        "\n",
        "#loader_SAINT_256_edge = GraphSAINTEdgeSampler(data, batch_size=500, num_steps=4, sample_coverage=10)\n",
        "\n",
        "loader_SAINT_256_RW = GraphSAINTRandomWalkSampler(data, batch_size=128, walk_length=2, num_steps=4, sample_coverage=10)\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "model = GCN(\n",
        "        in_channels=num_features,\n",
        "        hidden_channels=64,\n",
        "        out_channels=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# ------------------- Execute Training for Each Loader -------------------\n",
        "\n",
        "#run(loader_SAINT_256_node, \"GraphSAINT-NodeSampler\")\n",
        "#run(loader_SAINT_256_edge, \"GraphSAINT-EdgeSampler\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "test_acc,f1_micro =  run(loader_SAINT_256_RW, \"GraphSAINT-RandomWalkSampler\")\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "Tj_VLh3yZIuL",
        "outputId": "b33b10e7-e336-431f-fd94-1b493d299c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 34.56 MB\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compute GraphSAINT normalization: : 27920it [00:00, 754882.79it/s]         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 34.56 MB\n",
            "Max GPU memory used: 49.56 MB\n",
            "GraphSAINT-RandomWalkSampler | Epoch 001, Loss: 1.9206\n",
            "GraphSAINT-RandomWalkSampler | Epoch 002, Loss: 1.8519\n",
            "GraphSAINT-RandomWalkSampler | Epoch 003, Loss: 1.7789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphSAINT-RandomWalkSampler | Epoch 004, Loss: 1.7180\n",
            "GraphSAINT-RandomWalkSampler | Epoch 005, Loss: 1.7123\n",
            "GraphSAINT-RandomWalkSampler | Epoch 006, Loss: 1.6454\n",
            "GraphSAINT-RandomWalkSampler | Epoch 007, Loss: 1.5716\n",
            "GraphSAINT-RandomWalkSampler | Epoch 008, Loss: 1.5109\n",
            "GraphSAINT-RandomWalkSampler | Epoch 009, Loss: 1.4647\n",
            "GraphSAINT-RandomWalkSampler | Epoch 010, Loss: 1.3780\n",
            "GraphSAINT-RandomWalkSampler | Epoch 011, Loss: 1.3111\n",
            "GraphSAINT-RandomWalkSampler | Epoch 012, Loss: 1.2841\n",
            "GraphSAINT-RandomWalkSampler | Epoch 013, Loss: 1.2303\n",
            "GraphSAINT-RandomWalkSampler | Epoch 014, Loss: 1.1116\n",
            "GraphSAINT-RandomWalkSampler | Epoch 015, Loss: 1.0837\n",
            "GraphSAINT-RandomWalkSampler | Epoch 016, Loss: 1.0886\n",
            "GraphSAINT-RandomWalkSampler | Epoch 017, Loss: 1.0257\n",
            "GraphSAINT-RandomWalkSampler | Epoch 018, Loss: 0.9950\n",
            "GraphSAINT-RandomWalkSampler | Epoch 019, Loss: 0.9533\n",
            "GraphSAINT-RandomWalkSampler | Epoch 020, Loss: 0.9064\n",
            "GraphSAINT-RandomWalkSampler | Epoch 021, Loss: 0.9116\n",
            "GraphSAINT-RandomWalkSampler | Epoch 022, Loss: 0.8535\n",
            "GraphSAINT-RandomWalkSampler | Epoch 023, Loss: 0.8440\n",
            "GraphSAINT-RandomWalkSampler | Epoch 024, Loss: 0.8477\n",
            "GraphSAINT-RandomWalkSampler | Epoch 025, Loss: 0.8776\n",
            "GraphSAINT-RandomWalkSampler | Epoch 026, Loss: 0.8327\n",
            "GraphSAINT-RandomWalkSampler | Epoch 027, Loss: 0.8418\n",
            "GraphSAINT-RandomWalkSampler | Epoch 028, Loss: 0.7248\n",
            "GraphSAINT-RandomWalkSampler | Epoch 029, Loss: 0.7799\n",
            "GraphSAINT-RandomWalkSampler | Epoch 030, Loss: 0.7627\n",
            "GraphSAINT-RandomWalkSampler | Epoch 031, Loss: 0.7190\n",
            "GraphSAINT-RandomWalkSampler | Epoch 032, Loss: 0.7234\n",
            "GraphSAINT-RandomWalkSampler | Epoch 033, Loss: 0.7114\n",
            "GraphSAINT-RandomWalkSampler | Epoch 034, Loss: 0.7033\n",
            "GraphSAINT-RandomWalkSampler | Epoch 035, Loss: 0.7195\n",
            "GraphSAINT-RandomWalkSampler | Epoch 036, Loss: 0.6895\n",
            "GraphSAINT-RandomWalkSampler | Epoch 037, Loss: 0.6839\n",
            "GraphSAINT-RandomWalkSampler | Epoch 038, Loss: 0.6707\n",
            "GraphSAINT-RandomWalkSampler | Epoch 039, Loss: 0.7014\n",
            "GraphSAINT-RandomWalkSampler | Epoch 040, Loss: 0.6482\n",
            "GraphSAINT-RandomWalkSampler | Epoch 041, Loss: 0.6964\n",
            "GraphSAINT-RandomWalkSampler | Epoch 042, Loss: 0.6647\n",
            "GraphSAINT-RandomWalkSampler | Epoch 043, Loss: 0.6501\n",
            "GraphSAINT-RandomWalkSampler | Epoch 044, Loss: 0.6574\n",
            "GraphSAINT-RandomWalkSampler | Epoch 045, Loss: 0.6515\n",
            "GraphSAINT-RandomWalkSampler | Epoch 046, Loss: 0.6274\n",
            "GraphSAINT-RandomWalkSampler | Epoch 047, Loss: 0.6166\n",
            "GraphSAINT-RandomWalkSampler | Epoch 048, Loss: 0.6610\n",
            "GraphSAINT-RandomWalkSampler | Epoch 049, Loss: 0.6073\n",
            "GraphSAINT-RandomWalkSampler | Epoch 050, Loss: 0.6501\n",
            "GraphSAINT-RandomWalkSampler | Epoch 051, Loss: 0.6075\n",
            "GraphSAINT-RandomWalkSampler | Epoch 052, Loss: 0.6283\n",
            "GraphSAINT-RandomWalkSampler | Epoch 053, Loss: 0.6257\n",
            "GraphSAINT-RandomWalkSampler | Epoch 054, Loss: 0.5979\n",
            "GraphSAINT-RandomWalkSampler | Epoch 055, Loss: 0.5866\n",
            "GraphSAINT-RandomWalkSampler | Epoch 056, Loss: 0.6342\n",
            "GraphSAINT-RandomWalkSampler | Epoch 057, Loss: 0.6338\n",
            "GraphSAINT-RandomWalkSampler | Epoch 058, Loss: 0.5661\n",
            "GraphSAINT-RandomWalkSampler | Epoch 059, Loss: 0.5968\n",
            "GraphSAINT-RandomWalkSampler | Epoch 060, Loss: 0.5420\n",
            "GraphSAINT-RandomWalkSampler | Epoch 061, Loss: 0.5832\n",
            "GraphSAINT-RandomWalkSampler | Epoch 062, Loss: 0.5866\n",
            "GraphSAINT-RandomWalkSampler | Epoch 063, Loss: 0.6087\n",
            "GraphSAINT-RandomWalkSampler | Epoch 064, Loss: 0.5936\n",
            "GraphSAINT-RandomWalkSampler | Epoch 065, Loss: 0.6051\n",
            "GraphSAINT-RandomWalkSampler | Epoch 066, Loss: 0.5910\n",
            "GraphSAINT-RandomWalkSampler | Epoch 067, Loss: 0.5782\n",
            "GraphSAINT-RandomWalkSampler | Epoch 068, Loss: 0.5995\n",
            "GraphSAINT-RandomWalkSampler | Epoch 069, Loss: 0.6125\n",
            "GraphSAINT-RandomWalkSampler | Epoch 070, Loss: 0.5781\n",
            "GraphSAINT-RandomWalkSampler | Epoch 071, Loss: 0.5875\n",
            "GraphSAINT-RandomWalkSampler | Epoch 072, Loss: 0.5957\n",
            "GraphSAINT-RandomWalkSampler | Epoch 073, Loss: 0.5603\n",
            "GraphSAINT-RandomWalkSampler | Epoch 074, Loss: 0.5816\n",
            "GraphSAINT-RandomWalkSampler | Epoch 075, Loss: 0.5635\n",
            "GraphSAINT-RandomWalkSampler | Epoch 076, Loss: 0.5582\n",
            "GraphSAINT-RandomWalkSampler | Epoch 077, Loss: 0.5813\n",
            "GraphSAINT-RandomWalkSampler | Epoch 078, Loss: 0.5384\n",
            "GraphSAINT-RandomWalkSampler | Epoch 079, Loss: 0.5849\n",
            "GraphSAINT-RandomWalkSampler | Epoch 080, Loss: 0.6156\n",
            "GraphSAINT-RandomWalkSampler | Epoch 081, Loss: 0.5710\n",
            "GraphSAINT-RandomWalkSampler | Epoch 082, Loss: 0.5458\n",
            "GraphSAINT-RandomWalkSampler | Epoch 083, Loss: 0.5817\n",
            "GraphSAINT-RandomWalkSampler | Epoch 084, Loss: 0.5368\n",
            "GraphSAINT-RandomWalkSampler | Epoch 085, Loss: 0.5926\n",
            "GraphSAINT-RandomWalkSampler | Epoch 086, Loss: 0.5626\n",
            "GraphSAINT-RandomWalkSampler | Epoch 087, Loss: 0.5531\n",
            "GraphSAINT-RandomWalkSampler | Epoch 088, Loss: 0.5751\n",
            "GraphSAINT-RandomWalkSampler | Epoch 089, Loss: 0.5752\n",
            "GraphSAINT-RandomWalkSampler | Epoch 090, Loss: 0.5702\n",
            "GraphSAINT-RandomWalkSampler | Epoch 091, Loss: 0.5275\n",
            "GraphSAINT-RandomWalkSampler | Epoch 092, Loss: 0.5999\n",
            "GraphSAINT-RandomWalkSampler | Epoch 093, Loss: 0.5470\n",
            "GraphSAINT-RandomWalkSampler | Epoch 094, Loss: 0.5345\n",
            "GraphSAINT-RandomWalkSampler | Epoch 095, Loss: 0.5594\n",
            "GraphSAINT-RandomWalkSampler | Epoch 096, Loss: 0.5568\n",
            "GraphSAINT-RandomWalkSampler | Epoch 097, Loss: 0.5159\n",
            "GraphSAINT-RandomWalkSampler | Epoch 098, Loss: 0.5051\n",
            "GraphSAINT-RandomWalkSampler | Epoch 099, Loss: 0.5692\n",
            "GraphSAINT-RandomWalkSampler | Epoch 100, Loss: 0.5536\n",
            "GraphSAINT-RandomWalkSampler | Final Test Accuracy: 0.8980 | Test F1 (micro): 0.8980\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAINT\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":49.56\n",
        "}\n",
        "\n",
        "with open(\"graphSAINT_rw_Cora_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Vlg9Z6_HZW_q"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cetiser**"
      ],
      "metadata": {
        "id": "MooX_bbnZJA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "def dataset_load():\n",
        "    print(f\"Using device: {device}\")\n",
        "    dataset = Planetoid(root='data/Planetoid', name='CiteSeer', transform=NormalizeFeatures())\n",
        "    data = dataset[0].to(device)\n",
        "    return dataset.num_features, data, dataset.num_classes\n",
        "\n",
        "num_features, data, num_classes = dataset_load()\n",
        "data = data.cpu()\n",
        "#loader_SAINT_256_node = GraphSAINTNodeSampler(data, batch_size=500, num_steps=4, sample_coverage=10)\n",
        "\n",
        "#loader_SAINT_256_edge = GraphSAINTEdgeSampler(data, batch_size=128, num_steps=4, sample_coverage=10)\n",
        "\n",
        "loader_SAINT_256_RW = GraphSAINTRandomWalkSampler(data, batch_size=128, walk_length=2, num_steps=4, sample_coverage=10)\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "model = GCN(\n",
        "        in_channels=num_features,\n",
        "        hidden_channels=64,\n",
        "        out_channels=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# ------------------- Execute Training for Each Loader -------------------\n",
        "\n",
        "#run(loader_SAINT_256_node, \"GraphSAINT-NodeSampler\")\n",
        "\n",
        "# run(loader_SAINT_256_edge, \"GraphSAINT-EdgeSampler\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "test_acc,f1_micro =  run(loader_SAINT_256_RW, \"GraphSAINT-RandomWalkSampler\")\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "UrOXhWjHZMcz",
        "outputId": "df62b978-d640-4e8d-cb7f-ca83ee2228b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCompute GraphSAINT normalization:   0%|          | 0/137520 [14:05<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 30.94 MB\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compute GraphSAINT normalization: : 34438it [00:00, 964641.18it/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 30.94 MB\n",
            "Max GPU memory used: 78.11 MB\n",
            "GraphSAINT-RandomWalkSampler | Epoch 001, Loss: 1.7738\n",
            "GraphSAINT-RandomWalkSampler | Epoch 002, Loss: 1.7283\n",
            "GraphSAINT-RandomWalkSampler | Epoch 003, Loss: 1.6956\n",
            "GraphSAINT-RandomWalkSampler | Epoch 004, Loss: 1.6590\n",
            "GraphSAINT-RandomWalkSampler | Epoch 005, Loss: 1.6234\n",
            "GraphSAINT-RandomWalkSampler | Epoch 006, Loss: 1.6000\n",
            "GraphSAINT-RandomWalkSampler | Epoch 007, Loss: 1.5372\n",
            "GraphSAINT-RandomWalkSampler | Epoch 008, Loss: 1.4884\n",
            "GraphSAINT-RandomWalkSampler | Epoch 009, Loss: 1.4561\n",
            "GraphSAINT-RandomWalkSampler | Epoch 010, Loss: 1.4074\n",
            "GraphSAINT-RandomWalkSampler | Epoch 011, Loss: 1.3485\n",
            "GraphSAINT-RandomWalkSampler | Epoch 012, Loss: 1.2784\n",
            "GraphSAINT-RandomWalkSampler | Epoch 013, Loss: 1.2542\n",
            "GraphSAINT-RandomWalkSampler | Epoch 014, Loss: 1.2034\n",
            "GraphSAINT-RandomWalkSampler | Epoch 015, Loss: 1.2021\n",
            "GraphSAINT-RandomWalkSampler | Epoch 016, Loss: 1.1122\n",
            "GraphSAINT-RandomWalkSampler | Epoch 017, Loss: 1.1206\n",
            "GraphSAINT-RandomWalkSampler | Epoch 018, Loss: 1.1106\n",
            "GraphSAINT-RandomWalkSampler | Epoch 019, Loss: 0.9980\n",
            "GraphSAINT-RandomWalkSampler | Epoch 020, Loss: 1.0224\n",
            "GraphSAINT-RandomWalkSampler | Epoch 021, Loss: 0.9916\n",
            "GraphSAINT-RandomWalkSampler | Epoch 022, Loss: 0.9748\n",
            "GraphSAINT-RandomWalkSampler | Epoch 023, Loss: 0.9389\n",
            "GraphSAINT-RandomWalkSampler | Epoch 024, Loss: 0.9493\n",
            "GraphSAINT-RandomWalkSampler | Epoch 025, Loss: 0.9627\n",
            "GraphSAINT-RandomWalkSampler | Epoch 026, Loss: 0.8919\n",
            "GraphSAINT-RandomWalkSampler | Epoch 027, Loss: 0.8579\n",
            "GraphSAINT-RandomWalkSampler | Epoch 028, Loss: 0.8880\n",
            "GraphSAINT-RandomWalkSampler | Epoch 029, Loss: 0.8989\n",
            "GraphSAINT-RandomWalkSampler | Epoch 030, Loss: 0.8930\n",
            "GraphSAINT-RandomWalkSampler | Epoch 031, Loss: 0.8320\n",
            "GraphSAINT-RandomWalkSampler | Epoch 032, Loss: 0.8815\n",
            "GraphSAINT-RandomWalkSampler | Epoch 033, Loss: 0.8330\n",
            "GraphSAINT-RandomWalkSampler | Epoch 034, Loss: 0.8584\n",
            "GraphSAINT-RandomWalkSampler | Epoch 035, Loss: 0.8232\n",
            "GraphSAINT-RandomWalkSampler | Epoch 036, Loss: 0.8242\n",
            "GraphSAINT-RandomWalkSampler | Epoch 037, Loss: 0.8267\n",
            "GraphSAINT-RandomWalkSampler | Epoch 038, Loss: 0.8210\n",
            "GraphSAINT-RandomWalkSampler | Epoch 039, Loss: 0.7855\n",
            "GraphSAINT-RandomWalkSampler | Epoch 040, Loss: 0.7833\n",
            "GraphSAINT-RandomWalkSampler | Epoch 041, Loss: 0.7663\n",
            "GraphSAINT-RandomWalkSampler | Epoch 042, Loss: 0.8286\n",
            "GraphSAINT-RandomWalkSampler | Epoch 043, Loss: 0.8401\n",
            "GraphSAINT-RandomWalkSampler | Epoch 044, Loss: 0.8144\n",
            "GraphSAINT-RandomWalkSampler | Epoch 045, Loss: 0.8018\n",
            "GraphSAINT-RandomWalkSampler | Epoch 046, Loss: 0.7921\n",
            "GraphSAINT-RandomWalkSampler | Epoch 047, Loss: 0.7843\n",
            "GraphSAINT-RandomWalkSampler | Epoch 048, Loss: 0.7913\n",
            "GraphSAINT-RandomWalkSampler | Epoch 049, Loss: 0.7835\n",
            "GraphSAINT-RandomWalkSampler | Epoch 050, Loss: 0.7767\n",
            "GraphSAINT-RandomWalkSampler | Epoch 051, Loss: 0.7784\n",
            "GraphSAINT-RandomWalkSampler | Epoch 052, Loss: 0.8086\n",
            "GraphSAINT-RandomWalkSampler | Epoch 053, Loss: 0.7589\n",
            "GraphSAINT-RandomWalkSampler | Epoch 054, Loss: 0.7195\n",
            "GraphSAINT-RandomWalkSampler | Epoch 055, Loss: 0.7479\n",
            "GraphSAINT-RandomWalkSampler | Epoch 056, Loss: 0.7960\n",
            "GraphSAINT-RandomWalkSampler | Epoch 057, Loss: 0.7697\n",
            "GraphSAINT-RandomWalkSampler | Epoch 058, Loss: 0.7818\n",
            "GraphSAINT-RandomWalkSampler | Epoch 059, Loss: 0.7898\n",
            "GraphSAINT-RandomWalkSampler | Epoch 060, Loss: 0.7424\n",
            "GraphSAINT-RandomWalkSampler | Epoch 061, Loss: 0.7806\n",
            "GraphSAINT-RandomWalkSampler | Epoch 062, Loss: 0.7785\n",
            "GraphSAINT-RandomWalkSampler | Epoch 063, Loss: 0.7206\n",
            "GraphSAINT-RandomWalkSampler | Epoch 064, Loss: 0.7539\n",
            "GraphSAINT-RandomWalkSampler | Epoch 065, Loss: 0.7582\n",
            "GraphSAINT-RandomWalkSampler | Epoch 066, Loss: 0.7445\n",
            "GraphSAINT-RandomWalkSampler | Epoch 067, Loss: 0.7775\n",
            "GraphSAINT-RandomWalkSampler | Epoch 068, Loss: 0.7433\n",
            "GraphSAINT-RandomWalkSampler | Epoch 069, Loss: 0.7331\n",
            "GraphSAINT-RandomWalkSampler | Epoch 070, Loss: 0.7547\n",
            "GraphSAINT-RandomWalkSampler | Epoch 071, Loss: 0.7398\n",
            "GraphSAINT-RandomWalkSampler | Epoch 072, Loss: 0.7346\n",
            "GraphSAINT-RandomWalkSampler | Epoch 073, Loss: 0.7399\n",
            "GraphSAINT-RandomWalkSampler | Epoch 074, Loss: 0.6913\n",
            "GraphSAINT-RandomWalkSampler | Epoch 075, Loss: 0.7403\n",
            "GraphSAINT-RandomWalkSampler | Epoch 076, Loss: 0.7424\n",
            "GraphSAINT-RandomWalkSampler | Epoch 077, Loss: 0.6995\n",
            "GraphSAINT-RandomWalkSampler | Epoch 078, Loss: 0.7400\n",
            "GraphSAINT-RandomWalkSampler | Epoch 079, Loss: 0.7099\n",
            "GraphSAINT-RandomWalkSampler | Epoch 080, Loss: 0.7048\n",
            "GraphSAINT-RandomWalkSampler | Epoch 081, Loss: 0.7036\n",
            "GraphSAINT-RandomWalkSampler | Epoch 082, Loss: 0.6968\n",
            "GraphSAINT-RandomWalkSampler | Epoch 083, Loss: 0.7188\n",
            "GraphSAINT-RandomWalkSampler | Epoch 084, Loss: 0.7489\n",
            "GraphSAINT-RandomWalkSampler | Epoch 085, Loss: 0.7515\n",
            "GraphSAINT-RandomWalkSampler | Epoch 086, Loss: 0.6850\n",
            "GraphSAINT-RandomWalkSampler | Epoch 087, Loss: 0.6761\n",
            "GraphSAINT-RandomWalkSampler | Epoch 088, Loss: 0.7333\n",
            "GraphSAINT-RandomWalkSampler | Epoch 089, Loss: 0.7222\n",
            "GraphSAINT-RandomWalkSampler | Epoch 090, Loss: 0.7321\n",
            "GraphSAINT-RandomWalkSampler | Epoch 091, Loss: 0.7423\n",
            "GraphSAINT-RandomWalkSampler | Epoch 092, Loss: 0.7160\n",
            "GraphSAINT-RandomWalkSampler | Epoch 093, Loss: 0.7071\n",
            "GraphSAINT-RandomWalkSampler | Epoch 094, Loss: 0.7058\n",
            "GraphSAINT-RandomWalkSampler | Epoch 095, Loss: 0.7195\n",
            "GraphSAINT-RandomWalkSampler | Epoch 096, Loss: 0.7513\n",
            "GraphSAINT-RandomWalkSampler | Epoch 097, Loss: 0.7242\n",
            "GraphSAINT-RandomWalkSampler | Epoch 098, Loss: 0.7376\n",
            "GraphSAINT-RandomWalkSampler | Epoch 099, Loss: 0.7118\n",
            "GraphSAINT-RandomWalkSampler | Epoch 100, Loss: 0.7239\n",
            "GraphSAINT-RandomWalkSampler | Final Test Accuracy: 0.8110 | Test F1 (micro): 0.8110\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAINT\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":78.11\n",
        "}\n",
        "\n",
        "with open(\"graphSAINT_rw_CiteSeer_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Cl0uK9z8ah_j"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amazon**"
      ],
      "metadata": {
        "id": "EmIjpiz4ZM_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Amazon(\n",
        "        root='data/Amazon',\n",
        "        name='Computers',\n",
        "        transform=T.Compose([\n",
        "        NormalizeFeatures(),          # feature‑wise ℓ₂ normalisation\n",
        "        RandomNodeSplit(              # ⇦ add a split transform\n",
        "                split='train_rest',       # 10% val, 10% test by default\n",
        "                num_val=0.1,\n",
        "                num_test=0.1,\n",
        "                num_splits=1,\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "data = data.cpu()\n",
        "#loader_SAINT_256_node = GraphSAINTNodeSampler(data, batch_size=500, num_steps=4, sample_coverage=10)\n",
        "\n",
        "#loader_SAINT_256_edge = GraphSAINTEdgeSampler(data, batch_size=128, num_steps=4, sample_coverage=10)\n",
        "\n",
        "loader_SAINT_256_RW = GraphSAINTRandomWalkSampler(data, batch_size=500, walk_length=2, num_steps=4, sample_coverage=10)\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "model = GCN(\n",
        "        in_channels=num_features,\n",
        "        hidden_channels=64,\n",
        "        out_channels=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# ------------------- Execute Training for Each Loader -------------------\n",
        "\n",
        "#run(loader_SAINT_256_node, \"GraphSAINT-NodeSampler\")\n",
        "\n",
        "# run(loader_SAINT_256_edge, \"GraphSAINT-EdgeSampler\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "test_acc,f1_micro = run(loader_SAINT_256_RW, \"GraphSAINT-RandomWalkSampler\")\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "leKvNLO4dlfn",
        "outputId": "2d7273a1-8c48-4164-a22b-6aade35e5f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 31.70 MB\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compute GraphSAINT normalization: : 139630it [00:00, 792482.18it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 31.70 MB\n",
            "Max GPU memory used: 79.58 MB\n",
            "GraphSAINT-RandomWalkSampler | Epoch 001, Loss: 2.2638\n",
            "GraphSAINT-RandomWalkSampler | Epoch 002, Loss: 2.1361\n",
            "GraphSAINT-RandomWalkSampler | Epoch 003, Loss: 2.0014\n",
            "GraphSAINT-RandomWalkSampler | Epoch 004, Loss: 1.9335\n",
            "GraphSAINT-RandomWalkSampler | Epoch 005, Loss: 1.9170\n",
            "GraphSAINT-RandomWalkSampler | Epoch 006, Loss: 1.9212\n",
            "GraphSAINT-RandomWalkSampler | Epoch 007, Loss: 1.9088\n",
            "GraphSAINT-RandomWalkSampler | Epoch 008, Loss: 1.8955\n",
            "GraphSAINT-RandomWalkSampler | Epoch 009, Loss: 1.8860\n",
            "GraphSAINT-RandomWalkSampler | Epoch 010, Loss: 1.9245\n",
            "GraphSAINT-RandomWalkSampler | Epoch 011, Loss: 1.8895\n",
            "GraphSAINT-RandomWalkSampler | Epoch 012, Loss: 1.9022\n",
            "GraphSAINT-RandomWalkSampler | Epoch 013, Loss: 1.8844\n",
            "GraphSAINT-RandomWalkSampler | Epoch 014, Loss: 1.8779\n",
            "GraphSAINT-RandomWalkSampler | Epoch 015, Loss: 1.8877\n",
            "GraphSAINT-RandomWalkSampler | Epoch 016, Loss: 1.8806\n",
            "GraphSAINT-RandomWalkSampler | Epoch 017, Loss: 1.8795\n",
            "GraphSAINT-RandomWalkSampler | Epoch 018, Loss: 1.8815\n",
            "GraphSAINT-RandomWalkSampler | Epoch 019, Loss: 1.8778\n",
            "GraphSAINT-RandomWalkSampler | Epoch 020, Loss: 1.8946\n",
            "GraphSAINT-RandomWalkSampler | Epoch 021, Loss: 1.8900\n",
            "GraphSAINT-RandomWalkSampler | Epoch 022, Loss: 1.8870\n",
            "GraphSAINT-RandomWalkSampler | Epoch 023, Loss: 1.9030\n",
            "GraphSAINT-RandomWalkSampler | Epoch 024, Loss: 1.8956\n",
            "GraphSAINT-RandomWalkSampler | Epoch 025, Loss: 1.9010\n",
            "GraphSAINT-RandomWalkSampler | Epoch 026, Loss: 1.8902\n",
            "GraphSAINT-RandomWalkSampler | Epoch 027, Loss: 1.8554\n",
            "GraphSAINT-RandomWalkSampler | Epoch 028, Loss: 1.9001\n",
            "GraphSAINT-RandomWalkSampler | Epoch 029, Loss: 1.8731\n",
            "GraphSAINT-RandomWalkSampler | Epoch 030, Loss: 1.8626\n",
            "GraphSAINT-RandomWalkSampler | Epoch 031, Loss: 1.8288\n",
            "GraphSAINT-RandomWalkSampler | Epoch 032, Loss: 1.8895\n",
            "GraphSAINT-RandomWalkSampler | Epoch 033, Loss: 1.8780\n",
            "GraphSAINT-RandomWalkSampler | Epoch 034, Loss: 1.8675\n",
            "GraphSAINT-RandomWalkSampler | Epoch 035, Loss: 1.8677\n",
            "GraphSAINT-RandomWalkSampler | Epoch 036, Loss: 1.8679\n",
            "GraphSAINT-RandomWalkSampler | Epoch 037, Loss: 1.8666\n",
            "GraphSAINT-RandomWalkSampler | Epoch 038, Loss: 1.8205\n",
            "GraphSAINT-RandomWalkSampler | Epoch 039, Loss: 1.8592\n",
            "GraphSAINT-RandomWalkSampler | Epoch 040, Loss: 1.8476\n",
            "GraphSAINT-RandomWalkSampler | Epoch 041, Loss: 1.8457\n",
            "GraphSAINT-RandomWalkSampler | Epoch 042, Loss: 1.8519\n",
            "GraphSAINT-RandomWalkSampler | Epoch 043, Loss: 1.8457\n",
            "GraphSAINT-RandomWalkSampler | Epoch 044, Loss: 1.8641\n",
            "GraphSAINT-RandomWalkSampler | Epoch 045, Loss: 1.8433\n",
            "GraphSAINT-RandomWalkSampler | Epoch 046, Loss: 1.8112\n",
            "GraphSAINT-RandomWalkSampler | Epoch 047, Loss: 1.8525\n",
            "GraphSAINT-RandomWalkSampler | Epoch 048, Loss: 1.8095\n",
            "GraphSAINT-RandomWalkSampler | Epoch 049, Loss: 1.8414\n",
            "GraphSAINT-RandomWalkSampler | Epoch 050, Loss: 1.8463\n",
            "GraphSAINT-RandomWalkSampler | Epoch 051, Loss: 1.8143\n",
            "GraphSAINT-RandomWalkSampler | Epoch 052, Loss: 1.8077\n",
            "GraphSAINT-RandomWalkSampler | Epoch 053, Loss: 1.7845\n",
            "GraphSAINT-RandomWalkSampler | Epoch 054, Loss: 1.7714\n",
            "GraphSAINT-RandomWalkSampler | Epoch 055, Loss: 1.8057\n",
            "GraphSAINT-RandomWalkSampler | Epoch 056, Loss: 1.7716\n",
            "GraphSAINT-RandomWalkSampler | Epoch 057, Loss: 1.7939\n",
            "GraphSAINT-RandomWalkSampler | Epoch 058, Loss: 1.7935\n",
            "GraphSAINT-RandomWalkSampler | Epoch 059, Loss: 1.7617\n",
            "GraphSAINT-RandomWalkSampler | Epoch 060, Loss: 1.8216\n",
            "GraphSAINT-RandomWalkSampler | Epoch 061, Loss: 1.7735\n",
            "GraphSAINT-RandomWalkSampler | Epoch 062, Loss: 1.8223\n",
            "GraphSAINT-RandomWalkSampler | Epoch 063, Loss: 1.7602\n",
            "GraphSAINT-RandomWalkSampler | Epoch 064, Loss: 1.7850\n",
            "GraphSAINT-RandomWalkSampler | Epoch 065, Loss: 1.8089\n",
            "GraphSAINT-RandomWalkSampler | Epoch 066, Loss: 1.7754\n",
            "GraphSAINT-RandomWalkSampler | Epoch 067, Loss: 1.7722\n",
            "GraphSAINT-RandomWalkSampler | Epoch 068, Loss: 1.7545\n",
            "GraphSAINT-RandomWalkSampler | Epoch 069, Loss: 1.7590\n",
            "GraphSAINT-RandomWalkSampler | Epoch 070, Loss: 1.7310\n",
            "GraphSAINT-RandomWalkSampler | Epoch 071, Loss: 1.7849\n",
            "GraphSAINT-RandomWalkSampler | Epoch 072, Loss: 1.7531\n",
            "GraphSAINT-RandomWalkSampler | Epoch 073, Loss: 1.7685\n",
            "GraphSAINT-RandomWalkSampler | Epoch 074, Loss: 1.7447\n",
            "GraphSAINT-RandomWalkSampler | Epoch 075, Loss: 1.7398\n",
            "GraphSAINT-RandomWalkSampler | Epoch 076, Loss: 1.7370\n",
            "GraphSAINT-RandomWalkSampler | Epoch 077, Loss: 1.7416\n",
            "GraphSAINT-RandomWalkSampler | Epoch 078, Loss: 1.7323\n",
            "GraphSAINT-RandomWalkSampler | Epoch 079, Loss: 1.7316\n",
            "GraphSAINT-RandomWalkSampler | Epoch 080, Loss: 1.6903\n",
            "GraphSAINT-RandomWalkSampler | Epoch 081, Loss: 1.7475\n",
            "GraphSAINT-RandomWalkSampler | Epoch 082, Loss: 1.7397\n",
            "GraphSAINT-RandomWalkSampler | Epoch 083, Loss: 1.7347\n",
            "GraphSAINT-RandomWalkSampler | Epoch 084, Loss: 1.7066\n",
            "GraphSAINT-RandomWalkSampler | Epoch 085, Loss: 1.7042\n",
            "GraphSAINT-RandomWalkSampler | Epoch 086, Loss: 1.7322\n",
            "GraphSAINT-RandomWalkSampler | Epoch 087, Loss: 1.6990\n",
            "GraphSAINT-RandomWalkSampler | Epoch 088, Loss: 1.7323\n",
            "GraphSAINT-RandomWalkSampler | Epoch 089, Loss: 1.7035\n",
            "GraphSAINT-RandomWalkSampler | Epoch 090, Loss: 1.6917\n",
            "GraphSAINT-RandomWalkSampler | Epoch 091, Loss: 1.7387\n",
            "GraphSAINT-RandomWalkSampler | Epoch 092, Loss: 1.7124\n",
            "GraphSAINT-RandomWalkSampler | Epoch 093, Loss: 1.7251\n",
            "GraphSAINT-RandomWalkSampler | Epoch 094, Loss: 1.6956\n",
            "GraphSAINT-RandomWalkSampler | Epoch 095, Loss: 1.7322\n",
            "GraphSAINT-RandomWalkSampler | Epoch 096, Loss: 1.6950\n",
            "GraphSAINT-RandomWalkSampler | Epoch 097, Loss: 1.6939\n",
            "GraphSAINT-RandomWalkSampler | Epoch 098, Loss: 1.6884\n",
            "GraphSAINT-RandomWalkSampler | Epoch 099, Loss: 1.6932\n",
            "GraphSAINT-RandomWalkSampler | Epoch 100, Loss: 1.6962\n",
            "GraphSAINT-RandomWalkSampler | Final Test Accuracy: 0.3745 | Test F1 (micro): 0.3745\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAINT\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":79.58\n",
        "}\n",
        "\n",
        "with open(\"graphSAINT_rw_amazon_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "LglZ26elp4kG"
      },
      "execution_count": 90,
      "outputs": []
    }
  ]
}