{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/graphSAGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX64VtCLY-Ur",
        "outputId": "ac46ffc8-1f9f-43e4-829c-3d4fa02cd87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_cluster-1.6.3+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124 torch_spline_conv-1.2.2+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib import cm\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "from torch_sparse import spmm\n",
        "# import pyg_lib\n",
        "import torch_sparse\n",
        "\n",
        "# PyTorch geometric\n",
        "from torch_geometric.nn import GCNConv,SAGEConv\n",
        "from torch_geometric.datasets import Planetoid,Amazon\n",
        "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric import seed_everything\n",
        "import torch\n",
        "import os.path as osp\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import NeighborLoader\n"
      ],
      "metadata": {
        "id": "z6sZeV9iZbZj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "def clean_gpu_memory():\n",
        "    \"\"\"Cleans GPU memory without fully resetting the CUDA context\"\"\"\n",
        "    import gc\n",
        "    gc.collect()  # Python garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # PyTorch cache\n",
        "        torch.cuda.reset_peak_memory_stats()  # Reset tracking\n",
        "        print(f\"Memory after cleanup: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YSfmdTwZfWu",
        "outputId": "1ec85f3b-ebca-448f-e3dc-156d533189bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes:          19717\n",
            "Number of edges:          88648\n",
            "Average node degree:      4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "id": "NCqZAWVS7yHD",
        "outputId": "c5512fc8-25ac-47ab-f80d-ca883551aea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 39.56 MB\n",
            "Reserved memory : 62.00 MB\n",
            "Peak allocated memory: 45.02 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()"
      ],
      "metadata": {
        "id": "4ENOBJw0728v",
        "outputId": "4e7b13a2-785a-4998-d0f5-13974fe3ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 167.96 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=data.train_mask,\n",
        "    num_neighbors=[10, 10],  # s = 10 per layer (2 layers)\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "1QWm3OO_Y8WK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")"
      ],
      "metadata": {
        "id": "EGLpR8TU8Bu1",
        "outputId": "24a1957d-51ae-414f-a091-0d8822eb0764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 39.56 MB\n",
            "Reserved memory : 62.00 MB\n",
            "Peak allocated memory: 42.99 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class testGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        # First layer: in_channels -> hidden_channels\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        # Intermediate layers: hidden_channels -> hidden_channels\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        # Last layer: hidden_channels -> out_channels (optional, if no linear layers)\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        # Optional MLP head (for further transformation)\n",
        "        self.lin1 = Linear(out_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Last layer (no ReLU/Dropout for classification)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "\n",
        "        # Optional MLP head\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "m-Y5deSUbE66"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels = 64\n",
        "\n",
        "model = testGraphSAGE(\n",
        "    in_channels=dataset.num_features,  # Input feature dimension\n",
        "    hidden_channels=64,               # Hidden layer size\n",
        "    num_layers=2,                     # Number of SAGEConv layers\n",
        "    out_channels=dataset.num_classes,  # Output dimension (number of classes)\n",
        "    dropout=0.5,                      # Dropout rate                         # Jumping Knowledge (optional: \"cat\", \"max\", \"lstm\")\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss()  # Negative Log Likelihood (used with log_softmax)\n"
      ],
      "metadata": {
        "id": "ydOqpr29bGWe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minibatch training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Full-batch evaluation (for simplicity)\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out[mask].argmax(dim=1)\n",
        "        acc = (pred == data.y[mask]).float().mean().item()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "f0XqokZ3bTJ3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "allocated_mem = []\n",
        "reserved_mem = []\n",
        "peak_mem = []\n",
        "for epoch in range(1, 101):\n",
        "    #torch.cuda.reset_peak_memory_stats()  # reset peak tracking\n",
        "    loss = train()\n",
        "    val_acc = evaluate(data.val_mask)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "    reserv = torch.cuda.memory_reserved() / 1024**2\n",
        "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "    allocated_mem.append(alloc)\n",
        "    reserved_mem.append(reserv)\n",
        "    peak_mem.append(peak)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuetwHrbbaQs",
        "outputId": "f384fb3f-6203-4991-cb01-eb1e4c18069b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1.1149, Val Acc: 0.1960\n",
            "Epoch: 002, Loss: 1.1030, Val Acc: 0.2040\n",
            "Epoch: 003, Loss: 1.0959, Val Acc: 0.5060\n",
            "Epoch: 004, Loss: 1.0900, Val Acc: 0.2760\n",
            "Epoch: 005, Loss: 1.1024, Val Acc: 0.4720\n",
            "Epoch: 006, Loss: 1.0964, Val Acc: 0.5400\n",
            "Epoch: 007, Loss: 1.0610, Val Acc: 0.5700\n",
            "Epoch: 008, Loss: 1.0622, Val Acc: 0.6480\n",
            "Epoch: 009, Loss: 1.0446, Val Acc: 0.7080\n",
            "Epoch: 010, Loss: 0.9879, Val Acc: 0.7320\n",
            "Epoch: 011, Loss: 0.9862, Val Acc: 0.7100\n",
            "Epoch: 012, Loss: 0.9494, Val Acc: 0.6940\n",
            "Epoch: 013, Loss: 0.8916, Val Acc: 0.6560\n",
            "Epoch: 014, Loss: 0.8399, Val Acc: 0.6340\n",
            "Epoch: 015, Loss: 0.7710, Val Acc: 0.6360\n",
            "Epoch: 016, Loss: 0.6743, Val Acc: 0.6440\n",
            "Epoch: 017, Loss: 0.6458, Val Acc: 0.6460\n",
            "Epoch: 018, Loss: 0.5323, Val Acc: 0.6660\n",
            "Epoch: 019, Loss: 0.4600, Val Acc: 0.6860\n",
            "Epoch: 020, Loss: 0.4175, Val Acc: 0.7020\n",
            "Epoch: 021, Loss: 0.4039, Val Acc: 0.7220\n",
            "Epoch: 022, Loss: 0.3220, Val Acc: 0.7280\n",
            "Epoch: 023, Loss: 0.2537, Val Acc: 0.7260\n",
            "Epoch: 024, Loss: 0.2062, Val Acc: 0.7320\n",
            "Epoch: 025, Loss: 0.1539, Val Acc: 0.7360\n",
            "Epoch: 026, Loss: 0.1847, Val Acc: 0.7240\n",
            "Epoch: 027, Loss: 0.1072, Val Acc: 0.7260\n",
            "Epoch: 028, Loss: 0.1187, Val Acc: 0.7300\n",
            "Epoch: 029, Loss: 0.0642, Val Acc: 0.7360\n",
            "Epoch: 030, Loss: 0.0611, Val Acc: 0.7460\n",
            "Epoch: 031, Loss: 0.1244, Val Acc: 0.7380\n",
            "Epoch: 032, Loss: 0.0581, Val Acc: 0.7580\n",
            "Epoch: 033, Loss: 0.0272, Val Acc: 0.7700\n",
            "Epoch: 034, Loss: 0.0428, Val Acc: 0.7680\n",
            "Epoch: 035, Loss: 0.0342, Val Acc: 0.7700\n",
            "Epoch: 036, Loss: 0.0165, Val Acc: 0.7660\n",
            "Epoch: 037, Loss: 0.0348, Val Acc: 0.7760\n",
            "Epoch: 038, Loss: 0.0211, Val Acc: 0.7760\n",
            "Epoch: 039, Loss: 0.0119, Val Acc: 0.7820\n",
            "Epoch: 040, Loss: 0.0210, Val Acc: 0.7700\n",
            "Epoch: 041, Loss: 0.0075, Val Acc: 0.7680\n",
            "Epoch: 042, Loss: 0.0078, Val Acc: 0.7620\n",
            "Epoch: 043, Loss: 0.0077, Val Acc: 0.7560\n",
            "Epoch: 044, Loss: 0.0195, Val Acc: 0.7620\n",
            "Epoch: 045, Loss: 0.0063, Val Acc: 0.7640\n",
            "Epoch: 046, Loss: 0.0207, Val Acc: 0.7660\n",
            "Epoch: 047, Loss: 0.0229, Val Acc: 0.7740\n",
            "Epoch: 048, Loss: 0.0037, Val Acc: 0.7760\n",
            "Epoch: 049, Loss: 0.0082, Val Acc: 0.7780\n",
            "Epoch: 050, Loss: 0.0064, Val Acc: 0.7840\n",
            "Epoch: 051, Loss: 0.0086, Val Acc: 0.7780\n",
            "Epoch: 052, Loss: 0.0125, Val Acc: 0.7700\n",
            "Epoch: 053, Loss: 0.0052, Val Acc: 0.7660\n",
            "Epoch: 054, Loss: 0.0276, Val Acc: 0.7660\n",
            "Epoch: 055, Loss: 0.0121, Val Acc: 0.7640\n",
            "Epoch: 056, Loss: 0.0096, Val Acc: 0.7700\n",
            "Epoch: 057, Loss: 0.0019, Val Acc: 0.7680\n",
            "Epoch: 058, Loss: 0.0064, Val Acc: 0.7660\n",
            "Epoch: 059, Loss: 0.0210, Val Acc: 0.7720\n",
            "Epoch: 060, Loss: 0.0214, Val Acc: 0.7680\n",
            "Epoch: 061, Loss: 0.0084, Val Acc: 0.7620\n",
            "Epoch: 062, Loss: 0.0094, Val Acc: 0.7640\n",
            "Epoch: 063, Loss: 0.0042, Val Acc: 0.7620\n",
            "Epoch: 064, Loss: 0.0331, Val Acc: 0.7580\n",
            "Epoch: 065, Loss: 0.0054, Val Acc: 0.7460\n",
            "Epoch: 066, Loss: 0.0087, Val Acc: 0.7360\n",
            "Epoch: 067, Loss: 0.0052, Val Acc: 0.7440\n",
            "Epoch: 068, Loss: 0.0069, Val Acc: 0.7600\n",
            "Epoch: 069, Loss: 0.0131, Val Acc: 0.7680\n",
            "Epoch: 070, Loss: 0.0049, Val Acc: 0.7700\n",
            "Epoch: 071, Loss: 0.0090, Val Acc: 0.7680\n",
            "Epoch: 072, Loss: 0.0279, Val Acc: 0.7660\n",
            "Epoch: 073, Loss: 0.0104, Val Acc: 0.7700\n",
            "Epoch: 074, Loss: 0.0147, Val Acc: 0.7760\n",
            "Epoch: 075, Loss: 0.0071, Val Acc: 0.7720\n",
            "Epoch: 076, Loss: 0.0146, Val Acc: 0.7780\n",
            "Epoch: 077, Loss: 0.0036, Val Acc: 0.7800\n",
            "Epoch: 078, Loss: 0.0158, Val Acc: 0.7780\n",
            "Epoch: 079, Loss: 0.0215, Val Acc: 0.7600\n",
            "Epoch: 080, Loss: 0.0071, Val Acc: 0.7380\n",
            "Epoch: 081, Loss: 0.0264, Val Acc: 0.7540\n",
            "Epoch: 082, Loss: 0.0061, Val Acc: 0.7600\n",
            "Epoch: 083, Loss: 0.0113, Val Acc: 0.7800\n",
            "Epoch: 084, Loss: 0.0126, Val Acc: 0.7740\n",
            "Epoch: 085, Loss: 0.0065, Val Acc: 0.7680\n",
            "Epoch: 086, Loss: 0.0468, Val Acc: 0.7760\n",
            "Epoch: 087, Loss: 0.0061, Val Acc: 0.7920\n",
            "Epoch: 088, Loss: 0.0114, Val Acc: 0.7860\n",
            "Epoch: 089, Loss: 0.0050, Val Acc: 0.7800\n",
            "Epoch: 090, Loss: 0.0082, Val Acc: 0.7800\n",
            "Epoch: 091, Loss: 0.0063, Val Acc: 0.7760\n",
            "Epoch: 092, Loss: 0.0044, Val Acc: 0.7780\n",
            "Epoch: 093, Loss: 0.0085, Val Acc: 0.7800\n",
            "Epoch: 094, Loss: 0.0057, Val Acc: 0.7860\n",
            "Epoch: 095, Loss: 0.0086, Val Acc: 0.7740\n",
            "Epoch: 096, Loss: 0.0033, Val Acc: 0.7760\n",
            "Epoch: 097, Loss: 0.0041, Val Acc: 0.7720\n",
            "Epoch: 098, Loss: 0.0174, Val Acc: 0.7860\n",
            "Epoch: 099, Loss: 0.0128, Val Acc: 0.7820\n",
            "Epoch: 100, Loss: 0.0034, Val Acc: 0.7780\n",
            "Training time: 1.84 seconds\n",
            "Test Accuracy: 0.7610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7NnuLDbfYp",
        "outputId": "2912eb67-4f1a-4516-8f9c-fab53b1e1b24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(allocated_mem, label='Allocated')\n",
        "plt.plot(reserved_mem, label='Reserved')\n",
        "plt.plot(peak_mem, label='Peak')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Memory (MB)')\n",
        "plt.title('CUDA Memory Usage Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iS8PF8xW5Nx4",
        "outputId": "e5c37b3a-39e1-4145-e83a-51673823f2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_graphsage_memory(\n",
        "    num_layers: int,\n",
        "    hidden_dim: int,\n",
        "    num_classes: int,\n",
        "    batch_size: int,\n",
        "    num_neighbors: list,\n",
        "    dtype_bytes: int = 4,  # float32=4, float16=2\n",
        "    verbose: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate memory usage for GraphSAGE based on:\n",
        "    - Weight matrices: O(LK²)\n",
        "    - Intermediate embeddings: Batch size × Neighbor fanout × Hidden dim\n",
        "\n",
        "    Args:\n",
        "        num_layers: Number of SAGEConv layers (L)\n",
        "        hidden_dim: Hidden dimension size (K)\n",
        "        num_classes: Output dimension size (C)\n",
        "        batch_size: Number of seed nodes (B)\n",
        "        num_neighbors: List of neighbors per layer (e.g., [25, 10] for 2 layers)\n",
        "        dtype_bytes: Bytes per parameter (4 for float32, 2 for float16)\n",
        "        verbose: Print detailed breakdown\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with memory components in MB\n",
        "    \"\"\"\n",
        "    # --- Weight Matrices (O(LK²)) ---\n",
        "    # Input-to-hidden: K×K\n",
        "    # Hidden-to-output: K×C\n",
        "    weight_memory = (num_layers * hidden_dim**2 + hidden_dim * num_classes) * dtype_bytes\n",
        "    weight_memory_mb = weight_memory / (1024 ** 2)\n",
        "\n",
        "    # --- Intermediate Embeddings ---\n",
        "    # Layer 0: seed nodes\n",
        "    current_nodes = batch_size\n",
        "    embedding_memory = 0\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        # Nodes in this layer = current_nodes × neighbors[l]\n",
        "        current_nodes *= num_neighbors[l]\n",
        "        # Memory = nodes × hidden_dim × bytes\n",
        "        layer_memory = current_nodes * hidden_dim * dtype_bytes\n",
        "        embedding_memory += layer_memory\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Layer {l+1}: {current_nodes:,} nodes → {layer_memory/(1024**2):.2f} MB\")\n",
        "\n",
        "    embedding_memory_mb = embedding_memory / (1024 ** 2)\n",
        "\n",
        "    # --- Total ---\n",
        "    total_memory_mb = weight_memory_mb + embedding_memory_mb\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nMemory Breakdown:\")\n",
        "        print(f\"- Weights:    {weight_memory_mb:.4f} MB\")\n",
        "        print(f\"- Embeddings: {embedding_memory_mb:.2f} MB\")\n",
        "        print(f\"Total:       {total_memory_mb:.2f} MB\")\n",
        "\n",
        "    return {\n",
        "        'weights_mb': weight_memory_mb,\n",
        "        'embeddings_mb': embedding_memory_mb,\n",
        "        'total_mb': total_memory_mb,\n",
        "        'peak_nodes': current_nodes\n",
        "    }\n",
        "\n",
        "# Example usage for PubMed dataset\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        'num_layers': 2,\n",
        "        'hidden_dim': 64,\n",
        "        'num_classes': num_classes,\n",
        "        'batch_size': 128,\n",
        "        'num_neighbors': [10, 10],\n",
        "        'dtype_bytes': 4  # float32\n",
        "    }\n",
        "\n",
        "    memory_stats = calculate_graphsage_memory(**config)"
      ],
      "metadata": {
        "id": "Erm71oq96Frg",
        "outputId": "27ae811c-c3f9-4d63-ba54-53448b9bf50e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: 1,280 nodes → 0.31 MB\n",
            "Layer 2: 12,800 nodes → 3.12 MB\n",
            "\n",
            "Memory Breakdown:\n",
            "- Weights:    0.0320 MB\n",
            "- Embeddings: 3.44 MB\n",
            "Total:       3.47 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "def profile_memory_usage(model, data_loader):\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CUDA],  # Track CUDA memory\n",
        "        profile_memory=True,\n",
        "        record_shapes=True\n",
        "    ) as prof:\n",
        "        for batch in data_loader:\n",
        "            with record_function(\"forward_pass\"):\n",
        "                out = model(batch.x, batch.edge_index)\n",
        "            with record_function(\"backward_pass\"):\n",
        "                loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "                loss.backward()\n",
        "\n",
        "    # Print memory summary\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "id": "ZpQ-sKpn61Ra"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pviuW1zbsaD",
        "outputId": "b30662ca-633a-4c96-d78f-2c14be1d3912"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 56.81 MB\n",
            "Reserved memory : 330.00 MB\n",
            "Peak allocated memory: 302.89 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summ=torch.cuda.memory_summary()"
      ],
      "metadata": {
        "id": "1l7_RQ52zvHh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summ"
      ],
      "metadata": {
        "id": "TGCQfCxv0TqI",
        "outputId": "24d0c43b-995e-4d72-a79a-2f26ec4b4588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  58174 KiB | 310156 KiB |  34238 MiB |  34181 MiB |\\n|       from large pool |  56937 KiB | 308841 KiB |  32384 MiB |  32328 MiB |\\n|       from small pool |   1237 KiB |   3987 KiB |   1853 MiB |   1852 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  58174 KiB | 310156 KiB |  34238 MiB |  34181 MiB |\\n|       from large pool |  56937 KiB | 308841 KiB |  32384 MiB |  32328 MiB |\\n|       from small pool |   1237 KiB |   3987 KiB |   1853 MiB |   1852 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  57760 KiB | 307997 KiB |  34060 MiB |  34004 MiB |\\n|       from large pool |  56534 KiB | 306695 KiB |  32209 MiB |  32154 MiB |\\n|       from small pool |   1225 KiB |   3976 KiB |   1850 MiB |   1849 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 337920 KiB | 337920 KiB | 339968 KiB |   2048 KiB |\\n|       from large pool | 331776 KiB | 331776 KiB | 331776 KiB |      0 KiB |\\n|       from small pool |   6144 KiB |   6144 KiB |   8192 KiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  23745 KiB |  37614 KiB |   8968 MiB |   8945 MiB |\\n|       from large pool |  22934 KiB |  34755 KiB |   6728 MiB |   6705 MiB |\\n|       from small pool |    811 KiB |   3073 KiB |   2240 MiB |   2239 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      48    |      66    |   14764    |   14716    |\\n|       from large pool |       4    |       8    |    1637    |    1633    |\\n|       from small pool |      44    |      61    |   13127    |   13083    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      48    |      66    |   14764    |   14716    |\\n|       from large pool |       4    |       8    |    1637    |    1633    |\\n|       from small pool |      44    |      61    |   13127    |   13083    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       9    |       9    |      10    |       1    |\\n|       from large pool |       6    |       6    |       6    |       0    |\\n|       from small pool |       3    |       3    |       4    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      11    |      17    |    6138    |    6127    |\\n|       from large pool |       2    |       4    |     512    |     510    |\\n|       from small pool |       9    |      14    |    5626    |    5617    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "import json\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": \"0.7200\",\n",
        "    \"memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time\n",
        "}\n",
        "\n",
        "with open(\"graphSAGE_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "b6WHTssHb1ic"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_graphsage_memory(\n",
        "    batch_size: int,\n",
        "    hidden_dim: int,\n",
        "    num_layers: int,\n",
        "    num_neighbors: int,\n",
        "    num_features: int = None,\n",
        "    num_classes: int = None,\n",
        "    dtype_bytes: int = 4,  # float32=4, float16=2\n",
        "    verbose: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate memory consumption for GraphSAGE according to LADIES paper's formulation:\n",
        "    - Embedding storage: O(b * K * s^{L-1})\n",
        "    - Weight matrices: O(L * K^2)\n",
        "\n",
        "    Where:\n",
        "        b = batch_size,\n",
        "        K = hidden_dim,\n",
        "        s = num_neighbors (per layer),\n",
        "        L = num_layers\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Number of seed nodes (b).\n",
        "        hidden_dim (int): Hidden dimension size (K).\n",
        "        num_layers (int): Number of layers (L).\n",
        "        num_neighbors (int): Neighbor sample size per layer (s).\n",
        "        num_features (int): Input feature dimension (optional, for first layer weights).\n",
        "        num_classes (int): Output dimension (optional, for last layer weights).\n",
        "        dtype_bytes (int): Bytes per parameter (4 for float32, 2 for float16).\n",
        "        verbose (bool): Print detailed breakdown.\n",
        "\n",
        "    Returns:\n",
        "        dict: Memory components in bytes and MB.\n",
        "    \"\"\"\n",
        "    # --- Embedding Storage (O(b * K * s^{L-1})) ---\n",
        "    embedding_memory_bytes = batch_size * hidden_dim * (num_neighbors ** (num_layers - 1)) * dtype_bytes\n",
        "\n",
        "    # --- Weight Matrices (O(L * K^2)) ---\n",
        "    # First layer: (num_features -> hidden_dim) if provided\n",
        "    if num_features is not None:\n",
        "        weight_memory_bytes = num_features * hidden_dim * dtype_bytes\n",
        "        remaining_layers = num_layers - 1\n",
        "    else:\n",
        "        weight_memory_bytes = 0\n",
        "        remaining_layers = num_layers\n",
        "\n",
        "    # Hidden layers: L-1 layers of (K x K)\n",
        "    weight_memory_bytes += remaining_layers * (hidden_dim * hidden_dim) * dtype_bytes\n",
        "\n",
        "    # Output layer: (K -> num_classes) if provided\n",
        "    if num_classes is not None:\n",
        "        weight_memory_bytes += hidden_dim * num_classes * dtype_bytes\n",
        "\n",
        "    total_memory_bytes = embedding_memory_bytes + weight_memory_bytes\n",
        "\n",
        "    # Convert to MB\n",
        "    embedding_memory_mb = embedding_memory_bytes / (1024 ** 2)\n",
        "    weight_memory_mb = weight_memory_bytes / (1024 ** 2)\n",
        "    total_memory_mb = total_memory_bytes / (1024 ** 2)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== GraphSAGE Memory Breakdown ===\")\n",
        "        print(f\"Batch size (b): {batch_size}\")\n",
        "        print(f\"Hidden dim (K): {hidden_dim}\")\n",
        "        print(f\"Layers (L): {num_layers}\")\n",
        "        print(f\"Neighbors (s): {num_neighbors}\")\n",
        "        print(f\"Data type: {'float32' if dtype_bytes == 4 else 'float16'}\")\n",
        "        print(\"\\nEmbedding Storage:\")\n",
        "        print(f\"- Formula: O(b * K * s^(L-1)) = {batch_size} * {hidden_dim} * {num_neighbors}^({num_layers}-1)\")\n",
        "        print(f\"- Memory: {embedding_memory_bytes:,} bytes ({embedding_memory_mb:.2f} MB)\")\n",
        "        print(\"\\nWeight Matrices:\")\n",
        "        print(f\"- Formula: O(L * K^2) = {num_layers} * {hidden_dim}^2\")\n",
        "        if num_features is not None:\n",
        "            print(f\"  (First layer: {num_features} -> {hidden_dim})\")\n",
        "        if num_classes is not None:\n",
        "            print(f\"  (Last layer: {hidden_dim} -> {num_classes})\")\n",
        "        print(f\"- Memory: {weight_memory_bytes:,} bytes ({weight_memory_mb:.2f} MB)\")\n",
        "        print(\"\\nTotal Memory:\")\n",
        "        print(f\"- Total: {total_memory_bytes:,} bytes ({total_memory_mb:.2f} MB)\")\n",
        "\n",
        "    return {\n",
        "        'embedding_bytes': embedding_memory_bytes,\n",
        "        'weight_bytes': weight_memory_bytes,\n",
        "        'total_bytes': total_memory_bytes,\n",
        "        'embedding_mb': embedding_memory_mb,\n",
        "        'weight_mb': weight_memory_mb,\n",
        "        'total_mb': total_memory_mb,\n",
        "        'formula': {\n",
        "            'embedding': f'O(b*K*s^(L-1)) = {batch_size}*{hidden_dim}*{num_neighbors}^({num_layers-1})',\n",
        "            'weights': f'O(L*K^2) = {num_layers}*{hidden_dim}^2'\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # PubMed dataset example\n",
        "    memory_stats = calculate_graphsage_memory(\n",
        "        batch_size=128,\n",
        "        hidden_dim=64,\n",
        "        num_layers=2,\n",
        "        num_neighbors=10,\n",
        "        num_features=500,  # PubMed node features\n",
        "        num_classes=3,     # PubMed classes\n",
        "        dtype_bytes=4      # float32\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJEFbt9ivMAX",
        "outputId": "c29811d7-66da-43fd-affe-abd0a2a0e7d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GraphSAGE Memory Breakdown ===\n",
            "Batch size (b): 128\n",
            "Hidden dim (K): 64\n",
            "Layers (L): 2\n",
            "Neighbors (s): 10\n",
            "Data type: float32\n",
            "\n",
            "Embedding Storage:\n",
            "- Formula: O(b * K * s^(L-1)) = 128 * 64 * 10^(2-1)\n",
            "- Memory: 327,680 bytes (0.31 MB)\n",
            "\n",
            "Weight Matrices:\n",
            "- Formula: O(L * K^2) = 2 * 64^2\n",
            "  (First layer: 500 -> 64)\n",
            "  (Last layer: 64 -> 3)\n",
            "- Memory: 145,152 bytes (0.14 MB)\n",
            "\n",
            "Total Memory:\n",
            "- Total: 472,832 bytes (0.45 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORA** **Dataset**"
      ],
      "metadata": {
        "id": "rCTO_tx7wLJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of features:       {num_features}')\n",
        "print(f'Number of classes:       {num_classes}')\n",
        "\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=data.train_mask,\n",
        "    num_neighbors=[10, 10],  # s = 10 per layer (2 layers)\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "hidden_channels = 64\n",
        "\n",
        "model = testGraphSAGE(\n",
        "    in_channels=dataset.num_features,  # Input feature dimension\n",
        "    hidden_channels=64,               # Hidden layer size\n",
        "    num_layers=2,                     # Number of SAGEConv layers\n",
        "    out_channels=dataset.num_classes,  # Output dimension (number of classes)\n",
        "    dropout=0.5,                      # Dropout rate                         # Jumping Knowledge (optional: \"cat\", \"max\", \"lstm\")\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss()  # Negative Log Likelihood (used with log_softmax)\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "allocated_mem = []\n",
        "reserved_mem = []\n",
        "peak_mem = []\n",
        "for epoch in range(1, 101):\n",
        "    #torch.cuda.reset_peak_memory_stats()  # reset peak tracking\n",
        "    loss = train()\n",
        "    val_acc = evaluate(data.val_mask)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "    reserv = torch.cuda.memory_reserved() / 1024**2\n",
        "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "    allocated_mem.append(alloc)\n",
        "    reserved_mem.append(reserv)\n",
        "    peak_mem.append(peak)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMedJpj01Gly",
        "outputId": "aa01a21a-f747-4aa6-d5c2-47cbfceb3110"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features:       1433\n",
            "Number of classes:       7\n",
            "Number of nodes:          2708\n",
            "Number of edges:          10556\n",
            "Average node degree:      3.90\n",
            "Number of training nodes: 140\n",
            "Training node label rate: 0.052\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 001, Loss: 1.9287, Val Acc: 0.0580\n",
            "Epoch: 002, Loss: 1.9194, Val Acc: 0.1000\n",
            "Epoch: 003, Loss: 1.9300, Val Acc: 0.0720\n",
            "Epoch: 004, Loss: 1.9351, Val Acc: 0.0720\n",
            "Epoch: 005, Loss: 1.9217, Val Acc: 0.1240\n",
            "Epoch: 006, Loss: 1.9167, Val Acc: 0.0900\n",
            "Epoch: 007, Loss: 1.8875, Val Acc: 0.1160\n",
            "Epoch: 008, Loss: 1.8894, Val Acc: 0.2960\n",
            "Epoch: 009, Loss: 1.7469, Val Acc: 0.3400\n",
            "Epoch: 010, Loss: 1.7367, Val Acc: 0.3100\n",
            "Epoch: 011, Loss: 1.6611, Val Acc: 0.3280\n",
            "Epoch: 012, Loss: 1.5023, Val Acc: 0.3580\n",
            "Epoch: 013, Loss: 1.3842, Val Acc: 0.3920\n",
            "Epoch: 014, Loss: 1.2098, Val Acc: 0.4620\n",
            "Epoch: 015, Loss: 1.1357, Val Acc: 0.4540\n",
            "Epoch: 016, Loss: 0.9955, Val Acc: 0.5100\n",
            "Epoch: 017, Loss: 0.8381, Val Acc: 0.5800\n",
            "Epoch: 018, Loss: 0.7089, Val Acc: 0.6000\n",
            "Epoch: 019, Loss: 0.5708, Val Acc: 0.6560\n",
            "Epoch: 020, Loss: 0.5630, Val Acc: 0.6200\n",
            "Epoch: 021, Loss: 0.3652, Val Acc: 0.6740\n",
            "Epoch: 022, Loss: 0.2840, Val Acc: 0.6580\n",
            "Epoch: 023, Loss: 0.2563, Val Acc: 0.6720\n",
            "Epoch: 024, Loss: 0.2343, Val Acc: 0.6480\n",
            "Epoch: 025, Loss: 0.2730, Val Acc: 0.6780\n",
            "Epoch: 026, Loss: 0.2239, Val Acc: 0.6860\n",
            "Epoch: 027, Loss: 0.2165, Val Acc: 0.7000\n",
            "Epoch: 028, Loss: 0.2077, Val Acc: 0.6680\n",
            "Epoch: 029, Loss: 0.1868, Val Acc: 0.6720\n",
            "Epoch: 030, Loss: 0.1834, Val Acc: 0.6780\n",
            "Epoch: 031, Loss: 0.1665, Val Acc: 0.7120\n",
            "Epoch: 032, Loss: 0.1535, Val Acc: 0.7180\n",
            "Epoch: 033, Loss: 0.0978, Val Acc: 0.7080\n",
            "Epoch: 034, Loss: 0.1538, Val Acc: 0.6960\n",
            "Epoch: 035, Loss: 0.1175, Val Acc: 0.6880\n",
            "Epoch: 036, Loss: 0.1600, Val Acc: 0.6840\n",
            "Epoch: 037, Loss: 0.1391, Val Acc: 0.6880\n",
            "Epoch: 038, Loss: 0.1156, Val Acc: 0.7040\n",
            "Epoch: 039, Loss: 0.1259, Val Acc: 0.7020\n",
            "Epoch: 040, Loss: 0.1397, Val Acc: 0.7120\n",
            "Epoch: 041, Loss: 0.2050, Val Acc: 0.7060\n",
            "Epoch: 042, Loss: 0.0935, Val Acc: 0.6880\n",
            "Epoch: 043, Loss: 0.0830, Val Acc: 0.6900\n",
            "Epoch: 044, Loss: 0.1084, Val Acc: 0.7260\n",
            "Epoch: 045, Loss: 0.0954, Val Acc: 0.7120\n",
            "Epoch: 046, Loss: 0.0589, Val Acc: 0.7040\n",
            "Epoch: 047, Loss: 0.0986, Val Acc: 0.7020\n",
            "Epoch: 048, Loss: 0.0728, Val Acc: 0.6860\n",
            "Epoch: 049, Loss: 0.0616, Val Acc: 0.7020\n",
            "Epoch: 050, Loss: 0.0880, Val Acc: 0.7140\n",
            "Epoch: 051, Loss: 0.0906, Val Acc: 0.6860\n",
            "Epoch: 052, Loss: 0.0881, Val Acc: 0.6880\n",
            "Epoch: 053, Loss: 0.0515, Val Acc: 0.7000\n",
            "Epoch: 054, Loss: 0.0614, Val Acc: 0.6720\n",
            "Epoch: 055, Loss: 0.0671, Val Acc: 0.6680\n",
            "Epoch: 056, Loss: 0.2205, Val Acc: 0.7160\n",
            "Epoch: 057, Loss: 0.0345, Val Acc: 0.7160\n",
            "Epoch: 058, Loss: 0.1666, Val Acc: 0.7120\n",
            "Epoch: 059, Loss: 0.1042, Val Acc: 0.7120\n",
            "Epoch: 060, Loss: 0.0874, Val Acc: 0.7120\n",
            "Epoch: 061, Loss: 0.1273, Val Acc: 0.6900\n",
            "Epoch: 062, Loss: 0.0541, Val Acc: 0.6680\n",
            "Epoch: 063, Loss: 0.1018, Val Acc: 0.7020\n",
            "Epoch: 064, Loss: 0.0558, Val Acc: 0.6960\n",
            "Epoch: 065, Loss: 0.0415, Val Acc: 0.6860\n",
            "Epoch: 066, Loss: 0.0641, Val Acc: 0.6680\n",
            "Epoch: 067, Loss: 0.1474, Val Acc: 0.6880\n",
            "Epoch: 068, Loss: 0.0620, Val Acc: 0.6760\n",
            "Epoch: 069, Loss: 0.0714, Val Acc: 0.6680\n",
            "Epoch: 070, Loss: 0.1539, Val Acc: 0.6740\n",
            "Epoch: 071, Loss: 0.0779, Val Acc: 0.6520\n",
            "Epoch: 072, Loss: 0.1078, Val Acc: 0.6600\n",
            "Epoch: 073, Loss: 0.0569, Val Acc: 0.6840\n",
            "Epoch: 074, Loss: 0.0672, Val Acc: 0.7060\n",
            "Epoch: 075, Loss: 0.1779, Val Acc: 0.6860\n",
            "Epoch: 076, Loss: 0.1474, Val Acc: 0.6440\n",
            "Epoch: 077, Loss: 0.0826, Val Acc: 0.6800\n",
            "Epoch: 078, Loss: 0.1046, Val Acc: 0.7160\n",
            "Epoch: 079, Loss: 0.0351, Val Acc: 0.6900\n",
            "Epoch: 080, Loss: 0.0691, Val Acc: 0.6880\n",
            "Epoch: 081, Loss: 0.1135, Val Acc: 0.6980\n",
            "Epoch: 082, Loss: 0.0453, Val Acc: 0.6340\n",
            "Epoch: 083, Loss: 0.1892, Val Acc: 0.6560\n",
            "Epoch: 084, Loss: 0.1383, Val Acc: 0.7020\n",
            "Epoch: 085, Loss: 0.0703, Val Acc: 0.7060\n",
            "Epoch: 086, Loss: 0.0979, Val Acc: 0.6900\n",
            "Epoch: 087, Loss: 0.1607, Val Acc: 0.6860\n",
            "Epoch: 088, Loss: 0.1312, Val Acc: 0.6940\n",
            "Epoch: 089, Loss: 0.0489, Val Acc: 0.6640\n",
            "Epoch: 090, Loss: 0.1324, Val Acc: 0.6640\n",
            "Epoch: 091, Loss: 0.0329, Val Acc: 0.6880\n",
            "Epoch: 092, Loss: 0.0459, Val Acc: 0.7080\n",
            "Epoch: 093, Loss: 0.1123, Val Acc: 0.7220\n",
            "Epoch: 094, Loss: 0.0707, Val Acc: 0.7260\n",
            "Epoch: 095, Loss: 0.0990, Val Acc: 0.7100\n",
            "Epoch: 096, Loss: 0.0272, Val Acc: 0.7060\n",
            "Epoch: 097, Loss: 0.0247, Val Acc: 0.7000\n",
            "Epoch: 098, Loss: 0.0779, Val Acc: 0.7120\n",
            "Epoch: 099, Loss: 0.0706, Val Acc: 0.7140\n",
            "Epoch: 100, Loss: 0.0657, Val Acc: 0.7220\n",
            "Training time: 1.12 seconds\n",
            "Test Accuracy: 0.7350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # PubMed dataset example\n",
        "    memory_stats = calculate_graphsage_memory(\n",
        "        batch_size=128,\n",
        "        hidden_dim=64,\n",
        "        num_layers=2,\n",
        "        num_neighbors=10,\n",
        "        num_features=1433,  # CORA node features\n",
        "        num_classes=7,     # CORA classes\n",
        "        dtype_bytes=4      # float32\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5p3sAj2JzD",
        "outputId": "e439ac0a-cd92-4b16-cd8b-20023df85005"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GraphSAGE Memory Breakdown ===\n",
            "Batch size (b): 128\n",
            "Hidden dim (K): 64\n",
            "Layers (L): 2\n",
            "Neighbors (s): 10\n",
            "Data type: float32\n",
            "\n",
            "Embedding Storage:\n",
            "- Formula: O(b * K * s^(L-1)) = 128 * 64 * 10^(2-1)\n",
            "- Memory: 327,680 bytes (0.31 MB)\n",
            "\n",
            "Weight Matrices:\n",
            "- Formula: O(L * K^2) = 2 * 64^2\n",
            "  (First layer: 1433 -> 64)\n",
            "  (Last layer: 64 -> 7)\n",
            "- Memory: 385,024 bytes (0.37 MB)\n",
            "\n",
            "Total Memory:\n",
            "- Total: 712,704 bytes (0.68 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CiteSeer DATASET**"
      ],
      "metadata": {
        "id": "CyaZaw76z2vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='CiteSeer', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of features:       {num_features}')\n",
        "print(f'Number of classes:       {num_classes}')\n",
        "\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=data.train_mask,\n",
        "    num_neighbors=[10, 10],  # s = 10 per layer (2 layers)\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "hidden_channels = 64\n",
        "\n",
        "model = testGraphSAGE(\n",
        "    in_channels=dataset.num_features,  # Input feature dimension\n",
        "    hidden_channels=64,               # Hidden layer size\n",
        "    num_layers=2,                     # Number of SAGEConv layers\n",
        "    out_channels=dataset.num_classes,  # Output dimension (number of classes)\n",
        "    dropout=0.5,                      # Dropout rate                         # Jumping Knowledge (optional: \"cat\", \"max\", \"lstm\")\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss()  # Negative Log Likelihood (used with log_softmax)\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "allocated_mem = []\n",
        "reserved_mem = []\n",
        "peak_mem = []\n",
        "for epoch in range(1, 101):\n",
        "    #torch.cuda.reset_peak_memory_stats()  # reset peak tracking\n",
        "    loss = train()\n",
        "    val_acc = evaluate(data.val_mask)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "    reserv = torch.cuda.memory_reserved() / 1024**2\n",
        "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "    allocated_mem.append(alloc)\n",
        "    reserved_mem.append(reserv)\n",
        "    peak_mem.append(peak)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20phiVO0zyPX",
        "outputId": "fdc92c11-4a9e-4a85-b11e-7241efcb919a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features:       3703\n",
            "Number of classes:       6\n",
            "Number of nodes:          3327\n",
            "Number of edges:          9104\n",
            "Average node degree:      2.74\n",
            "Number of training nodes: 120\n",
            "Training node label rate: 0.036\n",
            "Has isolated nodes:       True\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 001, Loss: 1.8096, Val Acc: 0.2120\n",
            "Epoch: 002, Loss: 1.7996, Val Acc: 0.2120\n",
            "Epoch: 003, Loss: 1.7890, Val Acc: 0.1720\n",
            "Epoch: 004, Loss: 1.7975, Val Acc: 0.1720\n",
            "Epoch: 005, Loss: 1.7924, Val Acc: 0.1740\n",
            "Epoch: 006, Loss: 1.7743, Val Acc: 0.1560\n",
            "Epoch: 007, Loss: 1.7653, Val Acc: 0.2360\n",
            "Epoch: 008, Loss: 1.7451, Val Acc: 0.2100\n",
            "Epoch: 009, Loss: 1.7322, Val Acc: 0.2760\n",
            "Epoch: 010, Loss: 1.7148, Val Acc: 0.2720\n",
            "Epoch: 011, Loss: 1.6664, Val Acc: 0.2820\n",
            "Epoch: 012, Loss: 1.6534, Val Acc: 0.2920\n",
            "Epoch: 013, Loss: 1.6130, Val Acc: 0.3940\n",
            "Epoch: 014, Loss: 1.5129, Val Acc: 0.5160\n",
            "Epoch: 015, Loss: 1.4756, Val Acc: 0.5200\n",
            "Epoch: 016, Loss: 1.4178, Val Acc: 0.4940\n",
            "Epoch: 017, Loss: 1.3352, Val Acc: 0.4500\n",
            "Epoch: 018, Loss: 1.2333, Val Acc: 0.4480\n",
            "Epoch: 019, Loss: 1.1945, Val Acc: 0.4460\n",
            "Epoch: 020, Loss: 1.0594, Val Acc: 0.4420\n",
            "Epoch: 021, Loss: 1.0076, Val Acc: 0.4820\n",
            "Epoch: 022, Loss: 0.9201, Val Acc: 0.4900\n",
            "Epoch: 023, Loss: 0.8501, Val Acc: 0.4820\n",
            "Epoch: 024, Loss: 0.8062, Val Acc: 0.4960\n",
            "Epoch: 025, Loss: 0.6998, Val Acc: 0.4840\n",
            "Epoch: 026, Loss: 0.6068, Val Acc: 0.4540\n",
            "Epoch: 027, Loss: 0.5701, Val Acc: 0.4620\n",
            "Epoch: 028, Loss: 0.4883, Val Acc: 0.5000\n",
            "Epoch: 029, Loss: 0.4190, Val Acc: 0.5260\n",
            "Epoch: 030, Loss: 0.3973, Val Acc: 0.5420\n",
            "Epoch: 031, Loss: 0.3492, Val Acc: 0.5260\n",
            "Epoch: 032, Loss: 0.3049, Val Acc: 0.4880\n",
            "Epoch: 033, Loss: 0.3598, Val Acc: 0.5380\n",
            "Epoch: 034, Loss: 0.2768, Val Acc: 0.5560\n",
            "Epoch: 035, Loss: 0.2679, Val Acc: 0.5560\n",
            "Epoch: 036, Loss: 0.1964, Val Acc: 0.5360\n",
            "Epoch: 037, Loss: 0.2025, Val Acc: 0.5340\n",
            "Epoch: 038, Loss: 0.1711, Val Acc: 0.5440\n",
            "Epoch: 039, Loss: 0.1542, Val Acc: 0.5400\n",
            "Epoch: 040, Loss: 0.1131, Val Acc: 0.5300\n",
            "Epoch: 041, Loss: 0.0974, Val Acc: 0.5300\n",
            "Epoch: 042, Loss: 0.1454, Val Acc: 0.5320\n",
            "Epoch: 043, Loss: 0.0982, Val Acc: 0.5400\n",
            "Epoch: 044, Loss: 0.1504, Val Acc: 0.5740\n",
            "Epoch: 045, Loss: 0.0993, Val Acc: 0.5600\n",
            "Epoch: 046, Loss: 0.0975, Val Acc: 0.5340\n",
            "Epoch: 047, Loss: 0.1229, Val Acc: 0.5420\n",
            "Epoch: 048, Loss: 0.1318, Val Acc: 0.5660\n",
            "Epoch: 049, Loss: 0.0547, Val Acc: 0.5580\n",
            "Epoch: 050, Loss: 0.0663, Val Acc: 0.5400\n",
            "Epoch: 051, Loss: 0.0860, Val Acc: 0.5560\n",
            "Epoch: 052, Loss: 0.0773, Val Acc: 0.5460\n",
            "Epoch: 053, Loss: 0.0989, Val Acc: 0.5420\n",
            "Epoch: 054, Loss: 0.1102, Val Acc: 0.5660\n",
            "Epoch: 055, Loss: 0.0766, Val Acc: 0.5860\n",
            "Epoch: 056, Loss: 0.0823, Val Acc: 0.5880\n",
            "Epoch: 057, Loss: 0.0512, Val Acc: 0.5820\n",
            "Epoch: 058, Loss: 0.0577, Val Acc: 0.5640\n",
            "Epoch: 059, Loss: 0.1118, Val Acc: 0.5620\n",
            "Epoch: 060, Loss: 0.0455, Val Acc: 0.5720\n",
            "Epoch: 061, Loss: 0.0888, Val Acc: 0.6000\n",
            "Epoch: 062, Loss: 0.0528, Val Acc: 0.6180\n",
            "Epoch: 063, Loss: 0.1005, Val Acc: 0.6100\n",
            "Epoch: 064, Loss: 0.0823, Val Acc: 0.6140\n",
            "Epoch: 065, Loss: 0.0923, Val Acc: 0.6120\n",
            "Epoch: 066, Loss: 0.0568, Val Acc: 0.6100\n",
            "Epoch: 067, Loss: 0.0458, Val Acc: 0.5840\n",
            "Epoch: 068, Loss: 0.0359, Val Acc: 0.5740\n",
            "Epoch: 069, Loss: 0.0437, Val Acc: 0.5800\n",
            "Epoch: 070, Loss: 0.0752, Val Acc: 0.6000\n",
            "Epoch: 071, Loss: 0.0357, Val Acc: 0.6040\n",
            "Epoch: 072, Loss: 0.0614, Val Acc: 0.6040\n",
            "Epoch: 073, Loss: 0.0425, Val Acc: 0.6000\n",
            "Epoch: 074, Loss: 0.0356, Val Acc: 0.5880\n",
            "Epoch: 075, Loss: 0.0320, Val Acc: 0.5820\n",
            "Epoch: 076, Loss: 0.0441, Val Acc: 0.5820\n",
            "Epoch: 077, Loss: 0.0524, Val Acc: 0.6000\n",
            "Epoch: 078, Loss: 0.0583, Val Acc: 0.6060\n",
            "Epoch: 079, Loss: 0.0509, Val Acc: 0.6020\n",
            "Epoch: 080, Loss: 0.0347, Val Acc: 0.5920\n",
            "Epoch: 081, Loss: 0.0334, Val Acc: 0.5860\n",
            "Epoch: 082, Loss: 0.0756, Val Acc: 0.6000\n",
            "Epoch: 083, Loss: 0.0397, Val Acc: 0.6040\n",
            "Epoch: 084, Loss: 0.0384, Val Acc: 0.6020\n",
            "Epoch: 085, Loss: 0.0393, Val Acc: 0.5960\n",
            "Epoch: 086, Loss: 0.0572, Val Acc: 0.6020\n",
            "Epoch: 087, Loss: 0.0492, Val Acc: 0.6100\n",
            "Epoch: 088, Loss: 0.0535, Val Acc: 0.6100\n",
            "Epoch: 089, Loss: 0.0437, Val Acc: 0.5980\n",
            "Epoch: 090, Loss: 0.0416, Val Acc: 0.5860\n",
            "Epoch: 091, Loss: 0.0550, Val Acc: 0.5840\n",
            "Epoch: 092, Loss: 0.0573, Val Acc: 0.5920\n",
            "Epoch: 093, Loss: 0.0429, Val Acc: 0.6020\n",
            "Epoch: 094, Loss: 0.0474, Val Acc: 0.6040\n",
            "Epoch: 095, Loss: 0.0595, Val Acc: 0.5980\n",
            "Epoch: 096, Loss: 0.0554, Val Acc: 0.6000\n",
            "Epoch: 097, Loss: 0.0521, Val Acc: 0.5940\n",
            "Epoch: 098, Loss: 0.0370, Val Acc: 0.5980\n",
            "Epoch: 099, Loss: 0.0522, Val Acc: 0.6020\n",
            "Epoch: 100, Loss: 0.0332, Val Acc: 0.6040\n",
            "Training time: 0.97 seconds\n",
            "Test Accuracy: 0.5980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # PubMed dataset example\n",
        "    memory_stats = calculate_graphsage_memory(\n",
        "        batch_size=128,\n",
        "        hidden_dim=64,\n",
        "        num_layers=2,\n",
        "        num_neighbors=10,\n",
        "        num_features=3703,  # CiteSeer node features\n",
        "        num_classes=6,     # CiteSeer classes\n",
        "        dtype_bytes=4      # float32\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edZ9SJy30LPp",
        "outputId": "e203cfcc-05cc-4b4f-e449-bb1acaf3db16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GraphSAGE Memory Breakdown ===\n",
            "Batch size (b): 128\n",
            "Hidden dim (K): 64\n",
            "Layers (L): 2\n",
            "Neighbors (s): 10\n",
            "Data type: float32\n",
            "\n",
            "Embedding Storage:\n",
            "- Formula: O(b * K * s^(L-1)) = 128 * 64 * 10^(2-1)\n",
            "- Memory: 327,680 bytes (0.31 MB)\n",
            "\n",
            "Weight Matrices:\n",
            "- Formula: O(L * K^2) = 2 * 64^2\n",
            "  (First layer: 3703 -> 64)\n",
            "  (Last layer: 64 -> 6)\n",
            "- Memory: 965,888 bytes (0.92 MB)\n",
            "\n",
            "Total Memory:\n",
            "- Total: 1,293,568 bytes (1.23 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Amazon DATASET**"
      ],
      "metadata": {
        "id": "NwWODhTc0zh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Amazon(\n",
        "        root='data/Amazon',\n",
        "        name='Computers',\n",
        "        transform=T.Compose([\n",
        "        NormalizeFeatures(),          # feature‑wise ℓ₂ normalisation\n",
        "        RandomNodeSplit(              # ⇦ add a split transform\n",
        "                split='train_rest',       # 10% val, 10% test by default\n",
        "                num_val=0.1,\n",
        "                num_test=0.1,\n",
        "                num_splits=1,\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of features:       {num_features}')\n",
        "print(f'Number of classes:       {num_classes}')\n",
        "\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f\"Training nodes: {data.train_mask.sum().item()}\")\n",
        "print(f\"Validation nodes: {data.val_mask.sum().item()}\")\n",
        "print(f\"Test nodes: {data.test_mask.sum().item()}\")\n",
        "#print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "#print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "#print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "#print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "#print(f'Is undirected:            {data.is_undirected()}')\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=data.train_mask,\n",
        "    num_neighbors=[10, 10],  # s = 10 per layer (2 layers)\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "hidden_channels = 64\n",
        "\n",
        "model = testGraphSAGE(\n",
        "    in_channels=dataset.num_features,  # Input feature dimension\n",
        "    hidden_channels=64,               # Hidden layer size\n",
        "    num_layers=2,                     # Number of SAGEConv layers\n",
        "    out_channels=dataset.num_classes,  # Output dimension (number of classes)\n",
        "    dropout=0.5,                      # Dropout rate                         # Jumping Knowledge (optional: \"cat\", \"max\", \"lstm\")\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss()  # Negative Log Likelihood (used with log_softmax)\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "allocated_mem = []\n",
        "reserved_mem = []\n",
        "peak_mem = []\n",
        "for epoch in range(1, 101):\n",
        "    #torch.cuda.reset_peak_memory_stats()  # reset peak tracking\n",
        "    loss = train()\n",
        "    val_acc = evaluate(data.val_mask)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "    reserv = torch.cuda.memory_reserved() / 1024**2\n",
        "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "    allocated_mem.append(alloc)\n",
        "    reserved_mem.append(reserv)\n",
        "    peak_mem.append(peak)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4fNMXq70zNP",
        "outputId": "c3ac7acd-4ee8-45c0-95cb-9a16b01d90b0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of features:       767\n",
            "Number of classes:       10\n",
            "Number of nodes:          13752\n",
            "Number of edges:          491722\n",
            "Average node degree:      35.76\n",
            "Training nodes: 11002\n",
            "Validation nodes: 1375\n",
            "Test nodes: 1375\n",
            "Epoch: 001, Loss: 1.7805, Val Acc: 0.5498\n",
            "Epoch: 002, Loss: 1.3485, Val Acc: 0.5782\n",
            "Epoch: 003, Loss: 1.1682, Val Acc: 0.6422\n",
            "Epoch: 004, Loss: 1.0022, Val Acc: 0.7105\n",
            "Epoch: 005, Loss: 0.8929, Val Acc: 0.7338\n",
            "Epoch: 006, Loss: 0.8165, Val Acc: 0.7433\n",
            "Epoch: 007, Loss: 0.7603, Val Acc: 0.7542\n",
            "Epoch: 008, Loss: 0.7749, Val Acc: 0.7135\n",
            "Epoch: 009, Loss: 0.7432, Val Acc: 0.7389\n",
            "Epoch: 010, Loss: 0.7273, Val Acc: 0.7295\n",
            "Epoch: 011, Loss: 0.7148, Val Acc: 0.7520\n",
            "Epoch: 012, Loss: 0.7158, Val Acc: 0.7549\n",
            "Epoch: 013, Loss: 0.7041, Val Acc: 0.7578\n",
            "Epoch: 014, Loss: 0.7164, Val Acc: 0.7520\n",
            "Epoch: 015, Loss: 0.6983, Val Acc: 0.7549\n",
            "Epoch: 016, Loss: 0.6902, Val Acc: 0.7542\n",
            "Epoch: 017, Loss: 0.6965, Val Acc: 0.7571\n",
            "Epoch: 018, Loss: 0.6906, Val Acc: 0.7556\n",
            "Epoch: 019, Loss: 0.7013, Val Acc: 0.7520\n",
            "Epoch: 020, Loss: 0.7078, Val Acc: 0.7549\n",
            "Epoch: 021, Loss: 0.7168, Val Acc: 0.7636\n",
            "Epoch: 022, Loss: 0.6943, Val Acc: 0.7593\n",
            "Epoch: 023, Loss: 0.6839, Val Acc: 0.7484\n",
            "Epoch: 024, Loss: 0.7018, Val Acc: 0.7593\n",
            "Epoch: 025, Loss: 0.6987, Val Acc: 0.7571\n",
            "Epoch: 026, Loss: 0.6930, Val Acc: 0.7636\n",
            "Epoch: 027, Loss: 0.7073, Val Acc: 0.7571\n",
            "Epoch: 028, Loss: 0.6950, Val Acc: 0.7636\n",
            "Epoch: 029, Loss: 0.6960, Val Acc: 0.7542\n",
            "Epoch: 030, Loss: 0.7023, Val Acc: 0.7578\n",
            "Epoch: 031, Loss: 0.6905, Val Acc: 0.7636\n",
            "Epoch: 032, Loss: 0.6848, Val Acc: 0.7440\n",
            "Epoch: 033, Loss: 0.6880, Val Acc: 0.7629\n",
            "Epoch: 034, Loss: 0.6882, Val Acc: 0.7549\n",
            "Epoch: 035, Loss: 0.6870, Val Acc: 0.7593\n",
            "Epoch: 036, Loss: 0.6967, Val Acc: 0.7702\n",
            "Epoch: 037, Loss: 0.6967, Val Acc: 0.7651\n",
            "Epoch: 038, Loss: 0.6855, Val Acc: 0.7556\n",
            "Epoch: 039, Loss: 0.6848, Val Acc: 0.7673\n",
            "Epoch: 040, Loss: 0.6955, Val Acc: 0.7644\n",
            "Epoch: 041, Loss: 0.6926, Val Acc: 0.7600\n",
            "Epoch: 042, Loss: 0.7065, Val Acc: 0.7658\n",
            "Epoch: 043, Loss: 0.6926, Val Acc: 0.7622\n",
            "Epoch: 044, Loss: 0.6842, Val Acc: 0.7447\n",
            "Epoch: 045, Loss: 0.6879, Val Acc: 0.7469\n",
            "Epoch: 046, Loss: 0.6867, Val Acc: 0.7607\n",
            "Epoch: 047, Loss: 0.6948, Val Acc: 0.7578\n",
            "Epoch: 048, Loss: 0.6848, Val Acc: 0.7636\n",
            "Epoch: 049, Loss: 0.6959, Val Acc: 0.7607\n",
            "Epoch: 050, Loss: 0.6946, Val Acc: 0.7556\n",
            "Epoch: 051, Loss: 0.6946, Val Acc: 0.7505\n",
            "Epoch: 052, Loss: 0.7025, Val Acc: 0.7585\n",
            "Epoch: 053, Loss: 0.6733, Val Acc: 0.7615\n",
            "Epoch: 054, Loss: 0.6978, Val Acc: 0.7600\n",
            "Epoch: 055, Loss: 0.6795, Val Acc: 0.7549\n",
            "Epoch: 056, Loss: 0.6881, Val Acc: 0.7564\n",
            "Epoch: 057, Loss: 0.6830, Val Acc: 0.7549\n",
            "Epoch: 058, Loss: 0.6922, Val Acc: 0.7542\n",
            "Epoch: 059, Loss: 0.6927, Val Acc: 0.7585\n",
            "Epoch: 060, Loss: 0.6966, Val Acc: 0.7600\n",
            "Epoch: 061, Loss: 0.7069, Val Acc: 0.7585\n",
            "Epoch: 062, Loss: 0.6991, Val Acc: 0.7527\n",
            "Epoch: 063, Loss: 0.6957, Val Acc: 0.7505\n",
            "Epoch: 064, Loss: 0.6994, Val Acc: 0.7542\n",
            "Epoch: 065, Loss: 0.7018, Val Acc: 0.7549\n",
            "Epoch: 066, Loss: 0.7005, Val Acc: 0.7629\n",
            "Epoch: 067, Loss: 0.6891, Val Acc: 0.7535\n",
            "Epoch: 068, Loss: 0.6893, Val Acc: 0.7520\n",
            "Epoch: 069, Loss: 0.6989, Val Acc: 0.7593\n",
            "Epoch: 070, Loss: 0.6998, Val Acc: 0.7651\n",
            "Epoch: 071, Loss: 0.6953, Val Acc: 0.7535\n",
            "Epoch: 072, Loss: 0.6984, Val Acc: 0.7607\n",
            "Epoch: 073, Loss: 0.6938, Val Acc: 0.7629\n",
            "Epoch: 074, Loss: 0.7061, Val Acc: 0.7600\n",
            "Epoch: 075, Loss: 0.6943, Val Acc: 0.7513\n",
            "Epoch: 076, Loss: 0.6942, Val Acc: 0.7520\n",
            "Epoch: 077, Loss: 0.7033, Val Acc: 0.7360\n",
            "Epoch: 078, Loss: 0.6988, Val Acc: 0.7527\n",
            "Epoch: 079, Loss: 0.6921, Val Acc: 0.7607\n",
            "Epoch: 080, Loss: 0.6897, Val Acc: 0.7658\n",
            "Epoch: 081, Loss: 0.6924, Val Acc: 0.7607\n",
            "Epoch: 082, Loss: 0.6897, Val Acc: 0.7607\n",
            "Epoch: 083, Loss: 0.6935, Val Acc: 0.7607\n",
            "Epoch: 084, Loss: 0.6869, Val Acc: 0.7498\n",
            "Epoch: 085, Loss: 0.6930, Val Acc: 0.7651\n",
            "Epoch: 086, Loss: 0.7004, Val Acc: 0.7702\n",
            "Epoch: 087, Loss: 0.6885, Val Acc: 0.7476\n",
            "Epoch: 088, Loss: 0.6934, Val Acc: 0.7600\n",
            "Epoch: 089, Loss: 0.6903, Val Acc: 0.7629\n",
            "Epoch: 090, Loss: 0.6903, Val Acc: 0.7593\n",
            "Epoch: 091, Loss: 0.7025, Val Acc: 0.7593\n",
            "Epoch: 092, Loss: 0.6946, Val Acc: 0.7629\n",
            "Epoch: 093, Loss: 0.6914, Val Acc: 0.7622\n",
            "Epoch: 094, Loss: 0.6949, Val Acc: 0.7491\n",
            "Epoch: 095, Loss: 0.6931, Val Acc: 0.7680\n",
            "Epoch: 096, Loss: 0.6879, Val Acc: 0.7535\n",
            "Epoch: 097, Loss: 0.6967, Val Acc: 0.7564\n",
            "Epoch: 098, Loss: 0.6918, Val Acc: 0.7636\n",
            "Epoch: 099, Loss: 0.6950, Val Acc: 0.7433\n",
            "Epoch: 100, Loss: 0.7055, Val Acc: 0.7622\n",
            "Training time: 54.60 seconds\n",
            "Test Accuracy: 0.7520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # amazon dataset example\n",
        "    memory_stats = calculate_graphsage_memory(\n",
        "        batch_size=128,\n",
        "        hidden_dim=64,\n",
        "        num_layers=2,\n",
        "        num_neighbors=10,\n",
        "        num_features=767,  # AMAZON node features\n",
        "        num_classes=10,     # AMAZON classes\n",
        "        dtype_bytes=4      # float32\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdOctp6044F",
        "outputId": "a44d4a34-387f-4eec-bc57-84fd81041ddf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GraphSAGE Memory Breakdown ===\n",
            "Batch size (b): 128\n",
            "Hidden dim (K): 64\n",
            "Layers (L): 2\n",
            "Neighbors (s): 10\n",
            "Data type: float32\n",
            "\n",
            "Embedding Storage:\n",
            "- Formula: O(b * K * s^(L-1)) = 128 * 64 * 10^(2-1)\n",
            "- Memory: 327,680 bytes (0.31 MB)\n",
            "\n",
            "Weight Matrices:\n",
            "- Formula: O(L * K^2) = 2 * 64^2\n",
            "  (First layer: 767 -> 64)\n",
            "  (Last layer: 64 -> 10)\n",
            "- Memory: 215,296 bytes (0.21 MB)\n",
            "\n",
            "Total Memory:\n",
            "- Total: 542,976 bytes (0.52 MB)\n"
          ]
        }
      ]
    }
  ]
}