{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEbQQpfp0gx980fuZZj7+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/graphSAGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX64VtCLY-Ur",
        "outputId": "a73af6e0-1eb5-409f-9c79-aed31b6e1837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m815.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_cluster-1.6.3+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124 torch_spline_conv-1.2.2+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib import cm\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "from torch_sparse import spmm\n",
        "# import pyg_lib\n",
        "import torch_sparse\n",
        "\n",
        "# PyTorch geometric\n",
        "from torch_geometric.nn import GCNConv,SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric import seed_everything\n",
        "import torch\n",
        "import os.path as osp\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import NeighborLoader\n"
      ],
      "metadata": {
        "id": "z6sZeV9iZbZj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "def clean_gpu_memory():\n",
        "    \"\"\"Cleans GPU memory without fully resetting the CUDA context\"\"\"\n",
        "    import gc\n",
        "    gc.collect()  # Python garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # PyTorch cache\n",
        "        torch.cuda.reset_peak_memory_stats()  # Reset tracking\n",
        "        print(f\"Memory after cleanup: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YSfmdTwZfWu",
        "outputId": "bdc08129-bd67-4dd0-bb9f-e7ebbbbe04ab"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of nodes:          19717\n",
            "Number of edges:          88648\n",
            "Average node degree:      4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "id": "NCqZAWVS7yHD",
        "outputId": "50263f08-3035-40bb-943b-1ec94118f510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 96.37 MB\n",
            "Reserved memory : 158.00 MB\n",
            "Peak allocated memory: 96.37 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()"
      ],
      "metadata": {
        "id": "4ENOBJw0728v",
        "outputId": "4bcc47e6-4475-4ec4-face-8e2267b40806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 56.81 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=data.train_mask,\n",
        "    num_neighbors=[10, 10],  # s = 10 per layer (2 layers)\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "1QWm3OO_Y8WK"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")"
      ],
      "metadata": {
        "id": "EGLpR8TU8Bu1",
        "outputId": "2ac2c327-7f3c-468c-95d0-46b254b38b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 56.81 MB\n",
            "Reserved memory : 120.00 MB\n",
            "Peak allocated memory: 56.81 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class testGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        # First layer: in_channels -> hidden_channels\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        # Intermediate layers: hidden_channels -> hidden_channels\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        # Last layer: hidden_channels -> out_channels (optional, if no linear layers)\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        # Optional MLP head (for further transformation)\n",
        "        self.lin1 = Linear(out_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Last layer (no ReLU/Dropout for classification)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "\n",
        "        # Optional MLP head\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "m-Y5deSUbE66"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels = 64\n",
        "\n",
        "model = testGraphSAGE(\n",
        "    in_channels=dataset.num_features,  # Input feature dimension\n",
        "    hidden_channels=64,               # Hidden layer size\n",
        "    num_layers=2,                     # Number of SAGEConv layers\n",
        "    out_channels=dataset.num_classes,  # Output dimension (number of classes)\n",
        "    dropout=0.5,                      # Dropout rate                         # Jumping Knowledge (optional: \"cat\", \"max\", \"lstm\")\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss()  # Negative Log Likelihood (used with log_softmax)\n"
      ],
      "metadata": {
        "id": "ydOqpr29bGWe"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minibatch training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Full-batch evaluation (for simplicity)\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out[mask].argmax(dim=1)\n",
        "        acc = (pred == data.y[mask]).float().mean().item()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "f0XqokZ3bTJ3"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "allocated_mem = []\n",
        "reserved_mem = []\n",
        "peak_mem = []\n",
        "for epoch in range(1, 101):\n",
        "    #torch.cuda.reset_peak_memory_stats()  # reset peak tracking\n",
        "    loss = train()\n",
        "    val_acc = evaluate(data.val_mask)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "    reserv = torch.cuda.memory_reserved() / 1024**2\n",
        "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "    allocated_mem.append(alloc)\n",
        "    reserved_mem.append(reserv)\n",
        "    peak_mem.append(peak)\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuetwHrbbaQs",
        "outputId": "ddcf200b-080f-4be7-b1bc-01cc66131eaf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1.1076, Val Acc: 0.4160\n",
            "Epoch: 002, Loss: 1.0959, Val Acc: 0.4260\n",
            "Epoch: 003, Loss: 1.0954, Val Acc: 0.6000\n",
            "Epoch: 004, Loss: 1.1024, Val Acc: 0.5920\n",
            "Epoch: 005, Loss: 1.1329, Val Acc: 0.5420\n",
            "Epoch: 006, Loss: 1.0844, Val Acc: 0.5420\n",
            "Epoch: 007, Loss: 1.1066, Val Acc: 0.6040\n",
            "Epoch: 008, Loss: 1.0371, Val Acc: 0.4500\n",
            "Epoch: 009, Loss: 1.0534, Val Acc: 0.4340\n",
            "Epoch: 010, Loss: 1.0380, Val Acc: 0.4820\n",
            "Epoch: 011, Loss: 1.0007, Val Acc: 0.5760\n",
            "Epoch: 012, Loss: 0.9730, Val Acc: 0.6380\n",
            "Epoch: 013, Loss: 0.9670, Val Acc: 0.6200\n",
            "Epoch: 014, Loss: 0.9097, Val Acc: 0.6200\n",
            "Epoch: 015, Loss: 0.8417, Val Acc: 0.6160\n",
            "Epoch: 016, Loss: 0.7736, Val Acc: 0.6160\n",
            "Epoch: 017, Loss: 0.7087, Val Acc: 0.6280\n",
            "Epoch: 018, Loss: 0.6131, Val Acc: 0.6680\n",
            "Epoch: 019, Loss: 0.6066, Val Acc: 0.7000\n",
            "Epoch: 020, Loss: 0.5433, Val Acc: 0.6740\n",
            "Epoch: 021, Loss: 0.5763, Val Acc: 0.6300\n",
            "Epoch: 022, Loss: 0.5136, Val Acc: 0.6300\n",
            "Epoch: 023, Loss: 0.4810, Val Acc: 0.6320\n",
            "Epoch: 024, Loss: 0.4742, Val Acc: 0.6400\n",
            "Epoch: 025, Loss: 0.4183, Val Acc: 0.6760\n",
            "Epoch: 026, Loss: 0.3932, Val Acc: 0.6960\n",
            "Epoch: 027, Loss: 0.3783, Val Acc: 0.7140\n",
            "Epoch: 028, Loss: 0.2726, Val Acc: 0.7280\n",
            "Epoch: 029, Loss: 0.2556, Val Acc: 0.7320\n",
            "Epoch: 030, Loss: 0.2540, Val Acc: 0.7280\n",
            "Epoch: 031, Loss: 0.1846, Val Acc: 0.7300\n",
            "Epoch: 032, Loss: 0.1620, Val Acc: 0.7360\n",
            "Epoch: 033, Loss: 0.0765, Val Acc: 0.7600\n",
            "Epoch: 034, Loss: 0.0777, Val Acc: 0.7640\n",
            "Epoch: 035, Loss: 0.0566, Val Acc: 0.7720\n",
            "Epoch: 036, Loss: 0.0318, Val Acc: 0.7900\n",
            "Epoch: 037, Loss: 0.0164, Val Acc: 0.7860\n",
            "Epoch: 038, Loss: 0.0228, Val Acc: 0.7860\n",
            "Epoch: 039, Loss: 0.0250, Val Acc: 0.7860\n",
            "Epoch: 040, Loss: 0.0216, Val Acc: 0.7920\n",
            "Epoch: 041, Loss: 0.0110, Val Acc: 0.7900\n",
            "Epoch: 042, Loss: 0.0098, Val Acc: 0.7840\n",
            "Epoch: 043, Loss: 0.0176, Val Acc: 0.7800\n",
            "Epoch: 044, Loss: 0.0210, Val Acc: 0.7720\n",
            "Epoch: 045, Loss: 0.0067, Val Acc: 0.7540\n",
            "Epoch: 046, Loss: 0.0092, Val Acc: 0.7580\n",
            "Epoch: 047, Loss: 0.0117, Val Acc: 0.7740\n",
            "Epoch: 048, Loss: 0.0024, Val Acc: 0.7780\n",
            "Epoch: 049, Loss: 0.0028, Val Acc: 0.7840\n",
            "Epoch: 050, Loss: 0.0062, Val Acc: 0.7900\n",
            "Epoch: 051, Loss: 0.0227, Val Acc: 0.7880\n",
            "Epoch: 052, Loss: 0.0119, Val Acc: 0.7820\n",
            "Epoch: 053, Loss: 0.0031, Val Acc: 0.7800\n",
            "Epoch: 054, Loss: 0.0145, Val Acc: 0.7960\n",
            "Epoch: 055, Loss: 0.0089, Val Acc: 0.7860\n",
            "Epoch: 056, Loss: 0.0025, Val Acc: 0.7780\n",
            "Epoch: 057, Loss: 0.0191, Val Acc: 0.7660\n",
            "Epoch: 058, Loss: 0.0055, Val Acc: 0.7360\n",
            "Epoch: 059, Loss: 0.0055, Val Acc: 0.7280\n",
            "Epoch: 060, Loss: 0.0075, Val Acc: 0.7360\n",
            "Epoch: 061, Loss: 0.0326, Val Acc: 0.7960\n",
            "Epoch: 062, Loss: 0.0067, Val Acc: 0.7840\n",
            "Epoch: 063, Loss: 0.0281, Val Acc: 0.7720\n",
            "Epoch: 064, Loss: 0.0209, Val Acc: 0.7660\n",
            "Epoch: 065, Loss: 0.0099, Val Acc: 0.7620\n",
            "Epoch: 066, Loss: 0.0330, Val Acc: 0.7740\n",
            "Epoch: 067, Loss: 0.0073, Val Acc: 0.7920\n",
            "Epoch: 068, Loss: 0.0037, Val Acc: 0.7920\n",
            "Epoch: 069, Loss: 0.0016, Val Acc: 0.7900\n",
            "Epoch: 070, Loss: 0.0081, Val Acc: 0.7580\n",
            "Epoch: 071, Loss: 0.0076, Val Acc: 0.7420\n",
            "Epoch: 072, Loss: 0.0035, Val Acc: 0.7400\n",
            "Epoch: 073, Loss: 0.0119, Val Acc: 0.7460\n",
            "Epoch: 074, Loss: 0.0063, Val Acc: 0.7540\n",
            "Epoch: 075, Loss: 0.0102, Val Acc: 0.7800\n",
            "Epoch: 076, Loss: 0.0079, Val Acc: 0.7980\n",
            "Epoch: 077, Loss: 0.0024, Val Acc: 0.8080\n",
            "Epoch: 078, Loss: 0.0039, Val Acc: 0.8060\n",
            "Epoch: 079, Loss: 0.0054, Val Acc: 0.7900\n",
            "Epoch: 080, Loss: 0.0051, Val Acc: 0.7820\n",
            "Epoch: 081, Loss: 0.0038, Val Acc: 0.7740\n",
            "Epoch: 082, Loss: 0.0049, Val Acc: 0.7700\n",
            "Epoch: 083, Loss: 0.0080, Val Acc: 0.7700\n",
            "Epoch: 084, Loss: 0.0058, Val Acc: 0.7800\n",
            "Epoch: 085, Loss: 0.0062, Val Acc: 0.7920\n",
            "Epoch: 086, Loss: 0.0031, Val Acc: 0.7940\n",
            "Epoch: 087, Loss: 0.0019, Val Acc: 0.7900\n",
            "Epoch: 088, Loss: 0.0035, Val Acc: 0.7760\n",
            "Epoch: 089, Loss: 0.0086, Val Acc: 0.7760\n",
            "Epoch: 090, Loss: 0.0093, Val Acc: 0.7760\n",
            "Epoch: 091, Loss: 0.0031, Val Acc: 0.7740\n",
            "Epoch: 092, Loss: 0.0101, Val Acc: 0.7860\n",
            "Epoch: 093, Loss: 0.0029, Val Acc: 0.7800\n",
            "Epoch: 094, Loss: 0.0030, Val Acc: 0.7500\n",
            "Epoch: 095, Loss: 0.0172, Val Acc: 0.7660\n",
            "Epoch: 096, Loss: 0.0090, Val Acc: 0.7760\n",
            "Epoch: 097, Loss: 0.0175, Val Acc: 0.7620\n",
            "Epoch: 098, Loss: 0.0194, Val Acc: 0.7540\n",
            "Epoch: 099, Loss: 0.0061, Val Acc: 0.7580\n",
            "Epoch: 100, Loss: 0.0009, Val Acc: 0.7600\n",
            "Training time: 1.08 seconds\n",
            "Test Accuracy: 0.7510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7NnuLDbfYp",
        "outputId": "0b19e6ed-2298-451a-a694-b9d179610ae5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(allocated_mem, label='Allocated')\n",
        "plt.plot(reserved_mem, label='Reserved')\n",
        "plt.plot(peak_mem, label='Peak')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Memory (MB)')\n",
        "plt.title('CUDA Memory Usage Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iS8PF8xW5Nx4",
        "outputId": "5242e68a-aeb7-4114-d389-1f7c26656c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_graphsage_memory(\n",
        "    num_layers: int,\n",
        "    hidden_dim: int,\n",
        "    num_classes: int,\n",
        "    batch_size: int,\n",
        "    num_neighbors: list,\n",
        "    dtype_bytes: int = 4,  # float32=4, float16=2\n",
        "    verbose: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate memory usage for GraphSAGE based on:\n",
        "    - Weight matrices: O(LK²)\n",
        "    - Intermediate embeddings: Batch size × Neighbor fanout × Hidden dim\n",
        "\n",
        "    Args:\n",
        "        num_layers: Number of SAGEConv layers (L)\n",
        "        hidden_dim: Hidden dimension size (K)\n",
        "        num_classes: Output dimension size (C)\n",
        "        batch_size: Number of seed nodes (B)\n",
        "        num_neighbors: List of neighbors per layer (e.g., [25, 10] for 2 layers)\n",
        "        dtype_bytes: Bytes per parameter (4 for float32, 2 for float16)\n",
        "        verbose: Print detailed breakdown\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with memory components in MB\n",
        "    \"\"\"\n",
        "    # --- Weight Matrices (O(LK²)) ---\n",
        "    # Input-to-hidden: K×K\n",
        "    # Hidden-to-output: K×C\n",
        "    weight_memory = (num_layers * hidden_dim**2 + hidden_dim * num_classes) * dtype_bytes\n",
        "    weight_memory_mb = weight_memory / (1024 ** 2)\n",
        "\n",
        "    # --- Intermediate Embeddings ---\n",
        "    # Layer 0: seed nodes\n",
        "    current_nodes = batch_size\n",
        "    embedding_memory = 0\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        # Nodes in this layer = current_nodes × neighbors[l]\n",
        "        current_nodes *= num_neighbors[l]\n",
        "        # Memory = nodes × hidden_dim × bytes\n",
        "        layer_memory = current_nodes * hidden_dim * dtype_bytes\n",
        "        embedding_memory += layer_memory\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Layer {l+1}: {current_nodes:,} nodes → {layer_memory/(1024**2):.2f} MB\")\n",
        "\n",
        "    embedding_memory_mb = embedding_memory / (1024 ** 2)\n",
        "\n",
        "    # --- Total ---\n",
        "    total_memory_mb = weight_memory_mb + embedding_memory_mb\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nMemory Breakdown:\")\n",
        "        print(f\"- Weights:    {weight_memory_mb:.4f} MB\")\n",
        "        print(f\"- Embeddings: {embedding_memory_mb:.2f} MB\")\n",
        "        print(f\"Total:       {total_memory_mb:.2f} MB\")\n",
        "\n",
        "    return {\n",
        "        'weights_mb': weight_memory_mb,\n",
        "        'embeddings_mb': embedding_memory_mb,\n",
        "        'total_mb': total_memory_mb,\n",
        "        'peak_nodes': current_nodes\n",
        "    }\n",
        "\n",
        "# Example usage for PubMed dataset\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        'num_layers': 2,\n",
        "        'hidden_dim': 64,\n",
        "        'num_classes': num_classes,\n",
        "        'batch_size': 128,\n",
        "        'num_neighbors': [10, 10],\n",
        "        'dtype_bytes': 4  # float32\n",
        "    }\n",
        "\n",
        "    memory_stats = calculate_graphsage_memory(**config)"
      ],
      "metadata": {
        "id": "Erm71oq96Frg",
        "outputId": "c5031c7f-78e8-4840-f513-fc61a1295dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: 1,280 nodes → 0.31 MB\n",
            "Layer 2: 12,800 nodes → 3.12 MB\n",
            "\n",
            "Memory Breakdown:\n",
            "- Weights:    0.0320 MB\n",
            "- Embeddings: 3.44 MB\n",
            "Total:       3.47 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "def profile_memory_usage(model, data_loader):\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CUDA],  # Track CUDA memory\n",
        "        profile_memory=True,\n",
        "        record_shapes=True\n",
        "    ) as prof:\n",
        "        for batch in data_loader:\n",
        "            with record_function(\"forward_pass\"):\n",
        "                out = model(batch.x, batch.edge_index)\n",
        "            with record_function(\"backward_pass\"):\n",
        "                loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "                loss.backward()\n",
        "\n",
        "    # Print memory summary\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "id": "ZpQ-sKpn61Ra"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory : {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"Reserved memory : {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "print(f\"Peak allocated memory: {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pviuW1zbsaD",
        "outputId": "6c264876-ff12-4390-c383-e6c4e7583403"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory : 56.81 MB\n",
            "Reserved memory : 368.00 MB\n",
            "Peak allocated memory: 302.89 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summ=torch.cuda.memory_summary()"
      ],
      "metadata": {
        "id": "1l7_RQ52zvHh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summ"
      ],
      "metadata": {
        "id": "TGCQfCxv0TqI",
        "outputId": "87b51b7e-7cae-49bc-bbb0-ebddbef5135e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  58174 KiB | 438032 KiB | 266154 MiB | 266097 MiB |\\n|       from large pool |  56937 KiB | 434194 KiB | 256130 MiB | 256074 MiB |\\n|       from small pool |   1237 KiB |   6679 KiB |  10024 MiB |  10023 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  58174 KiB | 438032 KiB | 266154 MiB | 266097 MiB |\\n|       from large pool |  56937 KiB | 434194 KiB | 256130 MiB | 256074 MiB |\\n|       from small pool |   1237 KiB |   6679 KiB |  10024 MiB |  10023 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  57760 KiB | 435056 KiB | 264296 MiB | 264240 MiB |\\n|       from large pool |  56534 KiB | 431241 KiB | 254289 MiB | 254234 MiB |\\n|       from small pool |   1225 KiB |   6658 KiB |  10006 MiB |  10005 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 456704 KiB | 456704 KiB | 456704 KiB |      0 B   |\\n|       from large pool | 448512 KiB | 448512 KiB | 448512 KiB |      0 B   |\\n|       from small pool |   8192 KiB |   8192 KiB |   8192 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  64705 KiB |  64705 KiB |  70953 MiB |  70890 MiB |\\n|       from large pool |  61846 KiB |  61846 KiB |  57247 MiB |  57187 MiB |\\n|       from small pool |   2859 KiB |   3816 KiB |  13706 MiB |  13703 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      48    |     122    |   91827    |   91779    |\\n|       from large pool |       4    |      18    |   12241    |   12237    |\\n|       from small pool |      44    |     110    |   79586    |   79542    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      48    |     122    |   91827    |   91779    |\\n|       from large pool |       4    |      18    |   12241    |   12237    |\\n|       from small pool |      44    |     110    |   79586    |   79542    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      13    |      13    |      13    |       0    |\\n|       from large pool |       9    |       9    |       9    |       0    |\\n|       from small pool |       4    |       4    |       4    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      18    |      23    |   39139    |   39121    |\\n|       from large pool |       4    |       5    |    2224    |    2220    |\\n|       from small pool |      14    |      18    |   36915    |   36901    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "import json\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": \"0.7200\",\n",
        "    \"memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time\n",
        "}\n",
        "\n",
        "with open(\"graphSAGE_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "b6WHTssHb1ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_graphsage_memory_requirements(batch_size, fanout, num_layers, hidden_dim):\n",
        "    bytes_per_float = 4\n",
        "\n",
        "    # Total number of unique nodes sampled for 1 batch (approx)\n",
        "    total_sampled_nodes = batch_size * (fanout ** (num_layers - 1))\n",
        "\n",
        "    # Embedding memory for all sampled nodes\n",
        "    intermediate_embeddings = total_sampled_nodes * hidden_dim * bytes_per_float\n",
        "\n",
        "    # Weight matrices: L layers of KxK\n",
        "    weight_params = num_layers * hidden_dim * hidden_dim * bytes_per_float\n",
        "\n",
        "    total = intermediate_embeddings + weight_params\n",
        "\n",
        "    return {\n",
        "        \"intermediate_embeddings_MB\": intermediate_embeddings / 1024**2,\n",
        "        \"weight_matrices_MB\": weight_params / 1024**2,\n",
        "        \"total_MB\": total / 1024**2\n",
        "    }\n",
        "\n",
        "# Example values:\n",
        "batch_size = 128\n",
        "fanout = 10\n",
        "num_layers = 2\n",
        "hidden_dim = 64\n",
        "\n",
        "mem_usage = calc_graphsage_memory_requirements(batch_size, fanout, num_layers, hidden_dim)\n",
        "\n",
        "print(f\"Intermediate Embeddings: {mem_usage['intermediate_embeddings_MB']:.2f} MB\")\n",
        "print(f\"Weight Matrices        : {mem_usage['weight_matrices_MB']:.2f} MB\")\n",
        "print(f\"Total Estimated        : {mem_usage['total_MB']:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "7fc66EE_X3iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}