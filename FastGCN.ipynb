{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/FastGCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torchvision\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KkPe98bc0zM",
        "outputId": "fdf2eb1b-4ae8-4971-c7b9-4df4aa2fd0dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.11/dist-packages (0.4.0+pt26cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import typing\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "import torch_geometric.utils as utils\n",
        "import time\n",
        "# Try micro F1 score\n",
        "from sklearn.metrics import f1_score as F1\n",
        "from torch_geometric.datasets import Planetoid, Amazon\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
        "import torch_geometric.transforms as T\n",
        "from torch_sparse import SparseTensor\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "TLNpouTSwszY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "W2fiuieGVCQZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gpu_memory():\n",
        "    \"\"\"Cleans GPU memory without fully resetting the CUDA context\"\"\"\n",
        "    import gc\n",
        "    gc.collect()  # Python garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # PyTorch cache\n",
        "        torch.cuda.reset_peak_memory_stats()  # Reset tracking\n",
        "        print(f\"Memory after cleanup: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "aiZrfIrLZKvQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "edges = data.edge_index  # Edge indices [2, num_edges]\n",
        "labels = data.y          # Node labels [num_nodes]\n",
        "feat_data = data.x       # Node features [num_nodes, num_features]\n",
        "num_nodes = data.num_nodes  # This is already an integer value\n",
        "num_classes = dataset.num_classes\n",
        "num_features = dataset.num_features\n",
        "train_nodes = torch.where(data.train_mask)[0]\n",
        "valid_nodes = torch.where(data.val_mask)[0]\n",
        "test_nodes = torch.where(data.test_mask)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XioCcIkCU-XY",
        "outputId": "c7d9d4c7-9f45-47c0-a6ce-a6b616e7dce4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of nodes:          19717\n",
            "Number of edges:          88648\n",
            "Average node degree:      4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj = SparseTensor(\n",
        "    row=data.edge_index[0],\n",
        "    col=data.edge_index[1],\n",
        "    value=torch.ones(data.edge_index.size(1), device=device),\n",
        "    sparse_sizes=(data.num_nodes, data.num_nodes)\n",
        ")\n",
        "\n",
        "# For normalized Laplacian (GPU)\n",
        "deg = adj.sum(dim=1).pow(-0.5)\n",
        "adj_norm = deg.view(-1, 1) * adj * deg.view(1, -1)  # Normalized adjacency"
      ],
      "metadata": {
        "id": "6315T1MgZmuf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def row_normalize_sparse_tensor(adj):\n",
        "    row_sum = adj.sum(dim=1)\n",
        "    row_sum_inv = row_sum.pow(-1)\n",
        "    row_sum_inv[row_sum_inv == float('inf')] = 0\n",
        "\n",
        "    n = adj.size(0)\n",
        "    norm = SparseTensor(\n",
        "        row=torch.arange(n, device=device),\n",
        "        col=torch.arange(n, device=device),\n",
        "        value=row_sum_inv,\n",
        "        sparse_sizes=(n, n)\n",
        "    )\n",
        "    return norm @ adj\n",
        "\n",
        "# Create normalized Laplacian\n",
        "eye = SparseTensor.eye(data.num_nodes, device=device)\n",
        "lap_matrix = row_normalize_sparse_tensor(adj + eye)\n",
        "\n",
        "feat_data = data.x if data.x.is_sparse else data.x\n",
        "labels = data.y"
      ],
      "metadata": {
        "id": "JIlFVD4BV1Cz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, features, adj_list, labels, nodes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(features, adj_list)\n",
        "        pred = out[nodes].argmax(dim=1).cpu()\n",
        "        true = labels[nodes].cpu()\n",
        "\n",
        "        acc = (pred == true).float().mean().item()\n",
        "        f1_micro = f1_score(true, pred, average='micro')\n",
        "\n",
        "    return acc, f1_micro\n",
        "\n",
        "\n",
        "def fastgcn_sampler(batch_nodes, samp_num_list, lap_matrix, depth, device):\n",
        "    \"\"\"\n",
        "    GPU-optimized FastGCN sampler with fixed dimension handling\n",
        "\n",
        "    Args:\n",
        "        batch_nodes: Tensor of starting nodes [batch_size]\n",
        "        samp_num_list: List of sample sizes per layer\n",
        "        lap_matrix: SparseTensor of normalized Laplacian\n",
        "        depth: Number of GCN layers\n",
        "        device: Target device\n",
        "    \"\"\"\n",
        "    previous_nodes = batch_nodes\n",
        "    adjs = []\n",
        "\n",
        "    # Precompute degree and probabilities\n",
        "    deg = lap_matrix.sum(dim=1)\n",
        "    p = deg / deg.sum()\n",
        "\n",
        "    for d in range(depth):\n",
        "        # Sample nodes\n",
        "        s_num = min(int(torch.sum(p > 0)), samp_num_list[d])\n",
        "        after_nodes = torch.multinomial(p, s_num, replacement=False)\n",
        "\n",
        "        # Create subgraph adjacency\n",
        "        adj = lap_matrix[previous_nodes][:, after_nodes]\n",
        "\n",
        "        # Importance weighting - fix dimension handling\n",
        "        p_sampled = p[after_nodes].view(1, -1)  # Shape [1, s_num]\n",
        "        adj = adj / p_sampled  # Broadcasts correctly\n",
        "\n",
        "        # Row normalize\n",
        "        row_sum = adj.sum(dim=1).view(-1, 1)  # Shape [batch_size, 1]\n",
        "        row_sum[row_sum == 0] = 1  # Prevent division by zero\n",
        "        adj = adj / row_sum\n",
        "\n",
        "        adjs.append(adj)\n",
        "        previous_nodes = after_nodes\n",
        "\n",
        "    return adjs[::-1], previous_nodes, batch_nodes\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features [num_nodes, num_features]\n",
        "            adjs: List of sampled adjacency matrices from fastgcn_sampler\n",
        "        \"\"\"\n",
        "        for i, (conv, adj) in enumerate(zip(self.convs[:-1], adjs)):\n",
        "            x = conv(x)\n",
        "            x = adj @ x  # Sparse matrix multiplication\n",
        "            x = F.relu(x)\n",
        "        x = self.convs[-1](x)\n",
        "        return x\n",
        "\n",
        "# Initialize\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 16, num_classes, num_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Sample parameters\n",
        "samp_num_list = [64, 64]  # Sample sizes per layer\n",
        "batch_size = 128\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1,101):\n",
        "    model.train()\n",
        "\n",
        "    # Sample a batch - fixed indexing\n",
        "    idx = torch.randperm(len(train_nodes), device=device)[:batch_size]\n",
        "    batch_nodes = train_nodes[idx]\n",
        "\n",
        "    adjs, sampled_nodes, _ = fastgcn_sampler(\n",
        "        batch_nodes=batch_nodes,\n",
        "        samp_num_list=samp_num_list,\n",
        "        lap_matrix=lap_matrix,\n",
        "        depth=2,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Forward pass - ensure we use sampled nodes\n",
        "    optimizer.zero_grad()\n",
        "    out = model(feat_data[sampled_nodes], adjs)\n",
        "    loss = criterion(out, labels[sampled_nodes])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Create full graph adjacencies for validation\n",
        "            full_adjs = [lap_matrix] * 2\n",
        "            val_out = model(feat_data, full_adjs)\n",
        "        val_acc, val_f1 = evaluate(model, feat_data, [lap_matrix]*2, labels, valid_nodes)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, F1 (micro): {val_f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "test_acc, f1_micro = evaluate(model, feat_data, [lap_matrix]*2, labels, test_nodes)\n",
        "print(f'Test Accuracy: {test_acc:.4f}, F1 (micro): {f1_micro:.4f}')\n",
        "\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQmJ60bROK4",
        "outputId": "338bc13f-702d-44e9-e798-a202fbd2f567"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Loss: 1.0596, Val Acc: 0.3880, F1 (micro): 0.3880\n",
            "Epoch: 020, Loss: 1.0686, Val Acc: 0.3880, F1 (micro): 0.3880\n",
            "Epoch: 030, Loss: 1.0226, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 040, Loss: 1.0594, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 050, Loss: 0.9906, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 060, Loss: 1.0476, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 070, Loss: 1.0873, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 080, Loss: 1.0440, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 090, Loss: 1.1390, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Epoch: 100, Loss: 1.0266, Val Acc: 0.4160, F1 (micro): 0.4160\n",
            "Test Accuracy: 0.4070, F1 (micro): 0.4070\n",
            "Training time: 1.41 seconds\n",
            "Current GPU memory: 61.19 MB\n",
            "Max GPU memory used: 64.50 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_fastgcn_memory_MB(K, L, slayer, b=None):\n",
        "    \"\"\"\n",
        "    Estimate GPU memory usage for FastGCN embedding + transformation weights.\n",
        "\n",
        "    Args:\n",
        "        K (int): Hidden/embedding dimension per layer\n",
        "        L (int): Number of GCN layers\n",
        "        slayer (int): Number of sampled nodes per layer\n",
        "        b (int, optional): Batch size. If None, assume b = slayer\n",
        "\n",
        "    Returns:\n",
        "        memory_MB (float): Estimated memory in megabytes (MB)\n",
        "    \"\"\"\n",
        "    if b is None:\n",
        "        b = slayer  # worst-case assumption as per LADIES paper\n",
        "\n",
        "    # Main memory formula from LADIES paper approximation\n",
        "    total_floats = L * K * slayer + L * K * K\n",
        "    total_bytes = total_floats * 4  # float32 = 4 bytes\n",
        "    memory_MB = total_bytes / (1024 ** 2)\n",
        "\n",
        "    return memory_MB\n",
        "\n",
        "\n",
        "K = 16          # hidden dimension\n",
        "L = 2            # number of layers\n",
        "slayer = 64    # number of sampled nodes per layer\n",
        "b = 128          # batch size (optional)\n",
        "\n",
        "mem_MB = estimate_fastgcn_memory_MB(K, L, slayer, b)\n",
        "print(f\"Estimated FastGCN memory usage: {mem_MB:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT5DimfkVbL",
        "outputId": "681942bc-fa35-48c2-d51f-ae01f9340cc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated FastGCN memory usage: 0.01 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":mem_MB\n",
        "}\n",
        "\n",
        "with open(\"fastGCN_pubmed_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Fq6DJhOr1H7g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORA Dataset**"
      ],
      "metadata": {
        "id": "nT5zmCCFn4UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "edges = data.edge_index  # Edge indices [2, num_edges]\n",
        "labels = data.y          # Node labels [num_nodes]\n",
        "feat_data = data.x       # Node features [num_nodes, num_features]\n",
        "num_nodes = data.num_nodes  # This is already an integer value\n",
        "num_classes = dataset.num_classes\n",
        "num_features = dataset.num_features\n",
        "train_nodes = torch.where(data.train_mask)[0]\n",
        "valid_nodes = torch.where(data.val_mask)[0]\n",
        "test_nodes = torch.where(data.test_mask)[0]\n",
        "\n",
        "\n",
        "adj = SparseTensor(\n",
        "    row=data.edge_index[0],\n",
        "    col=data.edge_index[1],\n",
        "    value=torch.ones(data.edge_index.size(1), device=device),\n",
        "    sparse_sizes=(data.num_nodes, data.num_nodes)\n",
        ")\n",
        "\n",
        "# For normalized Laplacian (GPU)\n",
        "deg = adj.sum(dim=1).pow(-0.5)\n",
        "adj_norm = deg.view(-1, 1) * adj * deg.view(1, -1)  # Normalized adjacency\n",
        "\n",
        "eye = SparseTensor.eye(data.num_nodes, device=device)\n",
        "lap_matrix = row_normalize_sparse_tensor(adj + eye)\n",
        "\n",
        "feat_data = data.x if data.x.is_sparse else data.x\n",
        "labels = data.y\n",
        "\n",
        "# Initialize\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 16, num_classes, num_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Sample parameters\n",
        "samp_num_list = [64, 64]  # Sample sizes per layer\n",
        "batch_size = 128\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1,101):\n",
        "    model.train()\n",
        "\n",
        "    # Sample a batch - fixed indexing\n",
        "    idx = torch.randperm(len(train_nodes), device=device)[:batch_size]\n",
        "    batch_nodes = train_nodes[idx]\n",
        "\n",
        "    adjs, sampled_nodes, _ = fastgcn_sampler(\n",
        "        batch_nodes=batch_nodes,\n",
        "        samp_num_list=samp_num_list,\n",
        "        lap_matrix=lap_matrix,\n",
        "        depth=2,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Forward pass - ensure we use sampled nodes\n",
        "    optimizer.zero_grad()\n",
        "    out = model(feat_data[sampled_nodes], adjs)\n",
        "    loss = criterion(out, labels[sampled_nodes])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Create full graph adjacencies for validation\n",
        "            full_adjs = [lap_matrix] * 2\n",
        "            val_out = model(feat_data, full_adjs)\n",
        "        val_acc, val_f1 = evaluate(model, feat_data, [lap_matrix]*2, labels, valid_nodes)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, F1 (micro): {val_f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "test_acc, f1_micro = evaluate(model, feat_data, [lap_matrix]*2, labels, test_nodes)\n",
        "print(f'Test Accuracy: {test_acc:.4f}, F1 (micro): {f1_micro:.4f}')\n",
        "\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKXU0YJ1oAED",
        "outputId": "7121a2c3-4902-49dc-fa3c-888c71fb526f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 61.19 MB\n",
            "Using device: cuda\n",
            "Number of nodes:          2708\n",
            "Number of edges:          10556\n",
            "Average node degree:      3.90\n",
            "Number of training nodes: 140\n",
            "Training node label rate: 0.052\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 010, Loss: 1.8997, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 020, Loss: 1.9192, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 030, Loss: 1.8926, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 040, Loss: 1.8204, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 050, Loss: 1.7991, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 060, Loss: 1.8465, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 070, Loss: 1.7944, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 080, Loss: 1.8856, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 090, Loss: 1.9360, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Epoch: 100, Loss: 1.8957, Val Acc: 0.3160, F1 (micro): 0.3160\n",
            "Test Accuracy: 0.3190, F1 (micro): 0.3190\n",
            "Training time: 2.12 seconds\n",
            "Current GPU memory: 74.15 MB\n",
            "Max GPU memory used: 77.95 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":mem_MB\n",
        "}\n",
        "\n",
        "with open(\"fastGCN_Cora_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Zk6u0zOFr0Ql"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**citeeser DATASET**"
      ],
      "metadata": {
        "id": "3-jdQKCZr1ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Planetoid(root='data/Planetoid', name='CiteSeer', transform=NormalizeFeatures())\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "edges = data.edge_index  # Edge indices [2, num_edges]\n",
        "labels = data.y          # Node labels [num_nodes]\n",
        "feat_data = data.x       # Node features [num_nodes, num_features]\n",
        "num_nodes = data.num_nodes  # This is already an integer value\n",
        "num_classes = dataset.num_classes\n",
        "num_features = dataset.num_features\n",
        "train_nodes = torch.where(data.train_mask)[0]\n",
        "valid_nodes = torch.where(data.val_mask)[0]\n",
        "test_nodes = torch.where(data.test_mask)[0]\n",
        "\n",
        "\n",
        "adj = SparseTensor(\n",
        "    row=data.edge_index[0],\n",
        "    col=data.edge_index[1],\n",
        "    value=torch.ones(data.edge_index.size(1), device=device),\n",
        "    sparse_sizes=(data.num_nodes, data.num_nodes)\n",
        ")\n",
        "\n",
        "# For normalized Laplacian (GPU)\n",
        "deg = adj.sum(dim=1).pow(-0.5)\n",
        "adj_norm = deg.view(-1, 1) * adj * deg.view(1, -1)  # Normalized adjacency\n",
        "\n",
        "eye = SparseTensor.eye(data.num_nodes, device=device)\n",
        "lap_matrix = row_normalize_sparse_tensor(adj + eye)\n",
        "\n",
        "feat_data = data.x if data.x.is_sparse else data.x\n",
        "labels = data.y\n",
        "\n",
        "# Initialize\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 16, num_classes, num_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Sample parameters\n",
        "samp_num_list = [64, 64]  # Sample sizes per layer\n",
        "batch_size = 128\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1,101):\n",
        "    model.train()\n",
        "\n",
        "    # Sample a batch - fixed indexing\n",
        "    idx = torch.randperm(len(train_nodes), device=device)[:batch_size]\n",
        "    batch_nodes = train_nodes[idx]\n",
        "\n",
        "    adjs, sampled_nodes, _ = fastgcn_sampler(\n",
        "        batch_nodes=batch_nodes,\n",
        "        samp_num_list=samp_num_list,\n",
        "        lap_matrix=lap_matrix,\n",
        "        depth=2,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Forward pass - ensure we use sampled nodes\n",
        "    optimizer.zero_grad()\n",
        "    out = model(feat_data[sampled_nodes], adjs)\n",
        "    loss = criterion(out, labels[sampled_nodes])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Create full graph adjacencies for validation\n",
        "            full_adjs = [lap_matrix] * 2\n",
        "            val_out = model(feat_data, full_adjs)\n",
        "        val_acc, val_f1 = evaluate(model, feat_data, [lap_matrix]*2, labels, valid_nodes)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, F1 (micro): {val_f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "test_acc, f1_micro = evaluate(model, feat_data, [lap_matrix]*2, labels, test_nodes)\n",
        "print(f'Test Accuracy: {test_acc:.4f}, F1 (micro): {f1_micro:.4f}')\n",
        "\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "9SVImlM8r7ux",
        "outputId": "b975079e-5c8a-4787-f6da-0a50d7c309dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 74.15 MB\n",
            "Using device: cuda\n",
            "Number of nodes:          3327\n",
            "Number of edges:          9104\n",
            "Average node degree:      2.74\n",
            "Number of training nodes: 120\n",
            "Training node label rate: 0.036\n",
            "Has isolated nodes:       True\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 010, Loss: 1.7690, Val Acc: 0.1880, F1 (micro): 0.1880\n",
            "Epoch: 020, Loss: 1.8027, Val Acc: 0.1880, F1 (micro): 0.1880\n",
            "Epoch: 030, Loss: 1.7502, Val Acc: 0.1880, F1 (micro): 0.1880\n",
            "Epoch: 040, Loss: 1.7831, Val Acc: 0.1740, F1 (micro): 0.1740\n",
            "Epoch: 050, Loss: 1.7597, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Epoch: 060, Loss: 1.7965, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Epoch: 070, Loss: 1.7261, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Epoch: 080, Loss: 1.8047, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Epoch: 090, Loss: 1.7373, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Epoch: 100, Loss: 1.7267, Val Acc: 0.2120, F1 (micro): 0.2120\n",
            "Test Accuracy: 0.2310, F1 (micro): 0.2310\n",
            "Training time: 1.54 seconds\n",
            "Current GPU memory: 65.12 MB\n",
            "Max GPU memory used: 121.90 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":mem_MB\n",
        "}\n",
        "\n",
        "with open(\"fastGCN_Citeser_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Mbvu7DXJsR7T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**amazon**"
      ],
      "metadata": {
        "id": "0whGheJSsUkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def dataset_load():\n",
        "  print(f\"Using device: {device}\")\n",
        "  dataset = Amazon(\n",
        "        root='data/Amazon',\n",
        "        name='Computers',\n",
        "        transform=T.Compose([\n",
        "        NormalizeFeatures(),          # feature‑wise ℓ₂ normalisation\n",
        "        RandomNodeSplit(              # ⇦ add a split transform\n",
        "                split='train_rest',       # 10% val, 10% test by default\n",
        "                num_val=0.1,\n",
        "                num_test=0.1,\n",
        "                num_splits=1,\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  data = dataset[0].to(device)  # Get the first graph object.\n",
        "  return num_features, data, num_classes, device,dataset\n",
        "\n",
        "num_features, data, num_classes, device, dataset = dataset_load()\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "edges = data.edge_index  # Edge indices [2, num_edges]\n",
        "labels = data.y          # Node labels [num_nodes]\n",
        "feat_data = data.x       # Node features [num_nodes, num_features]\n",
        "num_nodes = data.num_nodes  # This is already an integer value\n",
        "num_classes = dataset.num_classes\n",
        "num_features = dataset.num_features\n",
        "train_nodes = torch.where(data.train_mask)[0]\n",
        "valid_nodes = torch.where(data.val_mask)[0]\n",
        "test_nodes = torch.where(data.test_mask)[0]\n",
        "\n",
        "\n",
        "adj = SparseTensor(\n",
        "    row=data.edge_index[0],\n",
        "    col=data.edge_index[1],\n",
        "    value=torch.ones(data.edge_index.size(1), device=device),\n",
        "    sparse_sizes=(data.num_nodes, data.num_nodes)\n",
        ")\n",
        "\n",
        "# For normalized Laplacian (GPU)\n",
        "deg = adj.sum(dim=1).pow(-0.5)\n",
        "adj_norm = deg.view(-1, 1) * adj * deg.view(1, -1)  # Normalized adjacency\n",
        "\n",
        "eye = SparseTensor.eye(data.num_nodes, device=device)\n",
        "lap_matrix = row_normalize_sparse_tensor(adj + eye)\n",
        "\n",
        "feat_data = data.x if data.x.is_sparse else data.x\n",
        "labels = data.y\n",
        "\n",
        "# Initialize\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 16, num_classes, num_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Sample parameters\n",
        "samp_num_list = [64, 64]  # Sample sizes per layer\n",
        "batch_size = 128\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1,101):\n",
        "    model.train()\n",
        "\n",
        "    # Sample a batch - fixed indexing\n",
        "    idx = torch.randperm(len(train_nodes), device=device)[:batch_size]\n",
        "    batch_nodes = train_nodes[idx]\n",
        "\n",
        "    adjs, sampled_nodes, _ = fastgcn_sampler(\n",
        "        batch_nodes=batch_nodes,\n",
        "        samp_num_list=samp_num_list,\n",
        "        lap_matrix=lap_matrix,\n",
        "        depth=2,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Forward pass - ensure we use sampled nodes\n",
        "    optimizer.zero_grad()\n",
        "    out = model(feat_data[sampled_nodes], adjs)\n",
        "    loss = criterion(out, labels[sampled_nodes])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Create full graph adjacencies for validation\n",
        "            full_adjs = [lap_matrix] * 2\n",
        "            val_out = model(feat_data, full_adjs)\n",
        "        val_acc, val_f1 = evaluate(model, feat_data, [lap_matrix]*2, labels, valid_nodes)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, F1 (micro): {val_f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "test_acc, f1_micro = evaluate(model, feat_data, [lap_matrix]*2, labels, test_nodes)\n",
        "print(f'Test Accuracy: {test_acc:.4f}, F1 (micro): {f1_micro:.4f}')\n",
        "\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "_b6vDrF4sYSi",
        "outputId": "58bda8c2-3573-4a53-ba74-4fc903a544e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 65.03 MB\n",
            "Using device: cuda\n",
            "Number of nodes:          13752\n",
            "Number of edges:          491722\n",
            "Average node degree:      35.76\n",
            "Number of training nodes: 11002\n",
            "Training node label rate: 0.800\n",
            "Has isolated nodes:       True\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 010, Loss: 2.2551, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 020, Loss: 2.1644, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 030, Loss: 2.1347, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 040, Loss: 2.0510, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 050, Loss: 1.9469, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 060, Loss: 2.0157, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 070, Loss: 1.8924, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 080, Loss: 2.0311, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 090, Loss: 2.0033, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Epoch: 100, Loss: 1.9547, Val Acc: 0.3840, F1 (micro): 0.3840\n",
            "Test Accuracy: 0.3549, F1 (micro): 0.3549\n",
            "Training time: 1.69 seconds\n",
            "Current GPU memory: 80.42 MB\n",
            "Max GPU memory used: 143.74 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"graphSAGE\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"mem_MB\":mem_MB\n",
        "}\n",
        "\n",
        "with open(\"fastGCN_amazon_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "3idwr1rbs4ck"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}