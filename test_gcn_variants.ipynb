{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTYZ+YISDbxucgvr2UUGGZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/test_gcn_variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x5loG4i3uJp",
        "outputId": "e2dc2a8e-5fed-421f-8538-666686a8cde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgoD1Pkr_fXV",
        "outputId": "7e477f02-73d1-465d-ddd5-a05fd6118e3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DayBwOJd_mKn",
        "outputId": "5929fd69-a909-4201-c3c9-68518b9d5824"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KkCxJS-YSz_",
        "outputId": "2862a411-fe3b-427a-a182-5c8cd97a32ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp4kz6bH_pnL",
        "outputId": "94ce17c7-55ac-43b3-aad6-7a07e2523633"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.11/dist-packages (0.4.0+pt26cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib import cm\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "# import pyg_lib\n",
        "import torch_sparse\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# PyTorch geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid, Amazon\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric import seed_everything\n",
        "from torch_geometric.nn.models import GraphSAGE\n",
        "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
        "import torch_geometric.transforms as T\n",
        "import json\n"
      ],
      "metadata": {
        "id": "F1xbmd_1AJxm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STANDARD GCN pubmed**"
      ],
      "metadata": {
        "id": "XqGIH2c8CdHw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djZb3AuzB5hz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset[0].to(device)  # Get the first graph object.\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoIPzKlZCDAm",
        "outputId": "6db98067-8920-45bc-cb20-0653284fa39b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_68HoG0CFvQ",
        "outputId": "3ed9a466-54d5-4465-ef54-ce883cca90ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes:          19717\n",
            "Number of edges:          88648\n",
            "Average node degree:      4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gpu_memory():\n",
        "    \"\"\"Cleans GPU memory without fully resetting the CUDA context\"\"\"\n",
        "    import gc\n",
        "    gc.collect()  # Python garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # PyTorch cache\n",
        "        torch.cuda.reset_peak_memory_stats()  # Reset tracking\n",
        "        print(f\"Memory after cleanup: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "oHQAaCSmc3wI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "WO1wwKbMCGbE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 64, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoV5H-7JCLrB",
        "outputId": "5e3692ce-dc6a-48a3-ef66-8ede9180bdec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(500, 64)\n",
            "  (conv2): GCNConv(64, 3)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, mask):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()  # Clear gradients.\n",
        "  out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "  loss = criterion(out[mask], data.y[mask])  # Compute the loss solely based on the training nodes.\n",
        "  loss.backward()  # Derive gradients.\n",
        "  optimizer.step()  # Update parameters based on gradients.\n",
        "  return loss\n",
        "\n",
        "def test(data, mask):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    true = data.y[mask].cpu()\n",
        "    pred = pred[mask].cpu()\n",
        "    acc = (pred == true).sum().item() / mask.sum().item()\n",
        "    micro_f1 = f1_score(true, pred, average='micro')\n",
        "    return acc, micro_f1\n"
      ],
      "metadata": {
        "id": "DU2MmFAXCN2u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train(data, data.train_mask)\n",
        "    train_acc, train_f1 = test(data, data.train_mask)\n",
        "    val_acc, val_f1 = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "170T7bBCCR8l",
        "outputId": "ca7c1661-6a5d-4770-898a-44e259532583"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6167, Train F1: 0.6167, Val Acc: 0.5360, Val F1: 0.5360\n",
            "Epoch: 002, Train Acc: 0.8500, Train F1: 0.8500, Val Acc: 0.7060, Val F1: 0.7060\n",
            "Epoch: 003, Train Acc: 0.8333, Train F1: 0.8333, Val Acc: 0.6680, Val F1: 0.6680\n",
            "Epoch: 004, Train Acc: 0.8500, Train F1: 0.8500, Val Acc: 0.6680, Val F1: 0.6680\n",
            "Epoch: 005, Train Acc: 0.8667, Train F1: 0.8667, Val Acc: 0.6720, Val F1: 0.6720\n",
            "Epoch: 006, Train Acc: 0.9000, Train F1: 0.9000, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 007, Train Acc: 0.9167, Train F1: 0.9167, Val Acc: 0.7140, Val F1: 0.7140\n",
            "Epoch: 008, Train Acc: 0.9333, Train F1: 0.9333, Val Acc: 0.7160, Val F1: 0.7160\n",
            "Epoch: 009, Train Acc: 0.9333, Train F1: 0.9333, Val Acc: 0.7300, Val F1: 0.7300\n",
            "Epoch: 010, Train Acc: 0.9167, Train F1: 0.9167, Val Acc: 0.7320, Val F1: 0.7320\n",
            "Epoch: 011, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7380, Val F1: 0.7380\n",
            "Epoch: 012, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7380, Val F1: 0.7380\n",
            "Epoch: 013, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7340, Val F1: 0.7340\n",
            "Epoch: 014, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7320, Val F1: 0.7320\n",
            "Epoch: 015, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7380, Val F1: 0.7380\n",
            "Epoch: 016, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7360, Val F1: 0.7360\n",
            "Epoch: 017, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7440, Val F1: 0.7440\n",
            "Epoch: 018, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7440, Val F1: 0.7440\n",
            "Epoch: 019, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7460, Val F1: 0.7460\n",
            "Epoch: 020, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7500, Val F1: 0.7500\n",
            "Epoch: 021, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7560, Val F1: 0.7560\n",
            "Epoch: 022, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 023, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 024, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 025, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 026, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7560, Val F1: 0.7560\n",
            "Epoch: 027, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 028, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 029, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 030, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 031, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 032, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 033, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.7640, Val F1: 0.7640\n",
            "Epoch: 034, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 035, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 036, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 037, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 038, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 039, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 040, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 041, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 042, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 043, Train Acc: 0.9833, Train F1: 0.9833, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 044, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 045, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 046, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 047, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 048, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 049, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 050, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 051, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 052, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7840, Val F1: 0.7840\n",
            "Epoch: 053, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7840, Val F1: 0.7840\n",
            "Epoch: 054, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 055, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 056, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 057, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 058, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 059, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 060, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 061, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 062, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 063, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 064, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 065, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7680, Val F1: 0.7680\n",
            "Epoch: 066, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 067, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 068, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 069, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 070, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 071, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 072, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7680, Val F1: 0.7680\n",
            "Epoch: 073, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7640, Val F1: 0.7640\n",
            "Epoch: 074, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 075, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 076, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 077, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 078, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 079, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 080, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 081, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 082, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 083, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7600, Val F1: 0.7600\n",
            "Epoch: 084, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 085, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7680, Val F1: 0.7680\n",
            "Epoch: 086, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 087, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 088, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7680, Val F1: 0.7680\n",
            "Epoch: 089, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 090, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 091, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 092, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7680, Val F1: 0.7680\n",
            "Epoch: 093, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 094, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 095, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 096, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 097, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 098, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 099, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7620, Val F1: 0.7620\n",
            "Epoch: 100, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7640, Val F1: 0.7640\n",
            "Current GPU memory: 56.31 MB\n",
            "Max GPU memory used: 120.92 MB\n",
            "Time taken: 1.77 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc,f1_micro = test(data, data.test_mask)\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEytRA11CZTv",
        "outputId": "648222bf-2055-4d35-e264-ab92cde1c07e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.765"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()"
      ],
      "metadata": {
        "id": "3fixWNKucuNM",
        "outputId": "211dcdd7-69fa-4c2a-b044-5fdf0e90f28a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 56.31 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gcn_memory_usage(num_layers: int, num_nodes: int, feat_dim: int, use_bytes: bool = True):\n",
        "\n",
        "    # Each embedding matrix is size |V| × K floats, there are L such layers\n",
        "    num_emb_floats = num_layers * num_nodes * feat_dim\n",
        "    # Each weight matrix is K × K floats per layer\n",
        "    num_weight_floats = num_layers * feat_dim * feat_dim\n",
        "\n",
        "    if use_bytes:\n",
        "        bytes_per_float = 4  # assuming float32\n",
        "        embeddings_bytes = num_emb_floats * bytes_per_float\n",
        "        weights_bytes = num_weight_floats * bytes_per_float\n",
        "        total_bytes = embeddings_bytes + weights_bytes\n",
        "        return embeddings_bytes, weights_bytes, total_bytes\n",
        "    else:\n",
        "        return num_emb_floats, num_weight_floats, num_emb_floats + num_weight_floats\n",
        "\n",
        "\n",
        "e, w, t = gcn_memory_usage(2, data.num_nodes, data.num_features)\n",
        "print(f\"Embeddings: {e/1e6:.1f} MB, Weights: {w*4/1e6:.1f} MB, Total: {t/1e6:.1f} MB\")\n",
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"GCN full-batch\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"embedding_storage\":e,\n",
        "    \"Weight_Matrices\":w,\n",
        "    \"Total_Memory\":t\n",
        "}\n",
        "\n",
        "with open(\"GCN_full_batch_pubmed_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "YpVvpb23YxmE",
        "outputId": "a32977e1-408c-4e88-86cd-aebca5ad08f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: 78.9 MB, Weights: 8.0 MB, Total: 80.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cora dataset**"
      ],
      "metadata": {
        "id": "rjE_S1PJ9Gae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset[0].to(device)  # Get the first graph object.\n",
        "data\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 64, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train(data, data.train_mask)\n",
        "    train_acc, train_f1 = test(data, data.train_mask)\n",
        "    val_acc, val_f1 = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "test_acc,f1_micro = test(data, data.test_mask)\n",
        "test_acc\n",
        "\n",
        "e, w, t = gcn_memory_usage(2, data.num_nodes, data.num_features)\n",
        "print(f\"Embeddings: {e/1e6:.1f} MB, Weights: {w*4/1e6:.1f} MB, Total: {t/1e6:.1f} MB\")\n",
        "\n",
        "\n",
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"GCN full-batch\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"embedding_storage\":e,\n",
        "    \"Weight_Matrices\":w,\n",
        "    \"Total_Memory\":t\n",
        "}\n",
        "\n",
        "with open(\"GCN_full_batch_cora_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "18hVrUko9KGA",
        "outputId": "95381a43-6e1b-4fbf-829f-01aef0b2a773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 56.31 MB\n",
            "Using device: cuda\n",
            "Number of nodes:          2708\n",
            "Number of edges:          10556\n",
            "Average node degree:      3.90\n",
            "Number of training nodes: 140\n",
            "Training node label rate: 0.052\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 001, Train Acc: 0.6857, Train F1: 0.6857, Val Acc: 0.4140, Val F1: 0.4140\n",
            "Epoch: 002, Train Acc: 0.8500, Train F1: 0.8500, Val Acc: 0.5680, Val F1: 0.5680\n",
            "Epoch: 003, Train Acc: 0.9214, Train F1: 0.9214, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 004, Train Acc: 0.9643, Train F1: 0.9643, Val Acc: 0.7040, Val F1: 0.7040\n",
            "Epoch: 005, Train Acc: 0.9786, Train F1: 0.9786, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 006, Train Acc: 0.9786, Train F1: 0.9786, Val Acc: 0.7480, Val F1: 0.7480\n",
            "Epoch: 007, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7500, Val F1: 0.7500\n",
            "Epoch: 008, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7420, Val F1: 0.7420\n",
            "Epoch: 009, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7440, Val F1: 0.7440\n",
            "Epoch: 010, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7380, Val F1: 0.7380\n",
            "Epoch: 011, Train Acc: 0.9786, Train F1: 0.9786, Val Acc: 0.7340, Val F1: 0.7340\n",
            "Epoch: 012, Train Acc: 0.9714, Train F1: 0.9714, Val Acc: 0.7320, Val F1: 0.7320\n",
            "Epoch: 013, Train Acc: 0.9714, Train F1: 0.9714, Val Acc: 0.7300, Val F1: 0.7300\n",
            "Epoch: 014, Train Acc: 0.9714, Train F1: 0.9714, Val Acc: 0.7260, Val F1: 0.7260\n",
            "Epoch: 015, Train Acc: 0.9786, Train F1: 0.9786, Val Acc: 0.7380, Val F1: 0.7380\n",
            "Epoch: 016, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7420, Val F1: 0.7420\n",
            "Epoch: 017, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7440, Val F1: 0.7440\n",
            "Epoch: 018, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7440, Val F1: 0.7440\n",
            "Epoch: 019, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7480, Val F1: 0.7480\n",
            "Epoch: 020, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7540, Val F1: 0.7540\n",
            "Epoch: 021, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 022, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7580, Val F1: 0.7580\n",
            "Epoch: 023, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7640, Val F1: 0.7640\n",
            "Epoch: 024, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7660, Val F1: 0.7660\n",
            "Epoch: 025, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 026, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 027, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 028, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 029, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 030, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 031, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 032, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 033, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 034, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 035, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 036, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 037, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 038, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 039, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 040, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 041, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 042, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 043, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 044, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 045, Train Acc: 0.9857, Train F1: 0.9857, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 046, Train Acc: 0.9929, Train F1: 0.9929, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 047, Train Acc: 0.9929, Train F1: 0.9929, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 048, Train Acc: 0.9929, Train F1: 0.9929, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 049, Train Acc: 0.9929, Train F1: 0.9929, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 050, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 051, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 052, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 053, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 054, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 055, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 056, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 057, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 058, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7700, Val F1: 0.7700\n",
            "Epoch: 059, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 060, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 061, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 062, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 063, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 064, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 065, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 066, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 067, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 068, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 069, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 070, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7720, Val F1: 0.7720\n",
            "Epoch: 071, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Epoch: 072, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 073, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 074, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 075, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 076, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7780, Val F1: 0.7780\n",
            "Epoch: 077, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 078, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 079, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7840, Val F1: 0.7840\n",
            "Epoch: 080, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7840, Val F1: 0.7840\n",
            "Epoch: 081, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7820, Val F1: 0.7820\n",
            "Epoch: 082, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 083, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 084, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 085, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 086, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 087, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 088, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 089, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 090, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 091, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 092, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 093, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 094, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 095, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7800, Val F1: 0.7800\n",
            "Epoch: 096, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 097, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 098, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 099, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7760, Val F1: 0.7760\n",
            "Epoch: 100, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.7740, Val F1: 0.7740\n",
            "Current GPU memory: 72.22 MB\n",
            "Max GPU memory used: 82.01 MB\n",
            "Time taken: 1.10 seconds\n",
            "Embeddings: 31.0 MB, Weights: 65.7 MB, Total: 47.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Citeseer dataset**"
      ],
      "metadata": {
        "id": "KCY-fcpYFVUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "dataset = Planetoid(root='data/Planetoid', name='CiteSeer', transform=NormalizeFeatures())\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset[0].to(device)  # Get the first graph object.\n",
        "data\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 64, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train(data, data.train_mask)\n",
        "    train_acc, train_f1 = test(data, data.train_mask)\n",
        "    val_acc, val_f1 = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "test_acc,f1_micro = test(data, data.test_mask)\n",
        "test_acc\n",
        "\n",
        "e, w, t = gcn_memory_usage(2, data.num_nodes, data.num_features)\n",
        "print(f\"Embeddings: {e/1e6:.1f} MB, Weights: {w*4/1e6:.1f} MB, Total: {t/1e6:.1f} MB\")\n",
        "\n",
        "\n",
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"GCN full-batch\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"embedding_storage\":e,\n",
        "    \"Weight_Matrices\":w,\n",
        "    \"Total_Memory\":t\n",
        "}\n",
        "\n",
        "with open(\"GCN_full_batch_citeseer_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "Wbi7f8EBFi9s",
        "outputId": "580540fa-ab4d-42ec-de4b-0be082f99c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 72.22 MB\n",
            "Using device: cuda\n",
            "Number of nodes:          3327\n",
            "Number of edges:          9104\n",
            "Average node degree:      2.74\n",
            "Number of training nodes: 120\n",
            "Training node label rate: 0.036\n",
            "Has isolated nodes:       True\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 001, Train Acc: 0.1917, Train F1: 0.1917, Val Acc: 0.0680, Val F1: 0.0680\n",
            "Epoch: 002, Train Acc: 0.4333, Train F1: 0.4333, Val Acc: 0.1060, Val F1: 0.1060\n",
            "Epoch: 003, Train Acc: 0.8250, Train F1: 0.8250, Val Acc: 0.3680, Val F1: 0.3680\n",
            "Epoch: 004, Train Acc: 0.8917, Train F1: 0.8917, Val Acc: 0.5060, Val F1: 0.5060\n",
            "Epoch: 005, Train Acc: 0.9500, Train F1: 0.9500, Val Acc: 0.6100, Val F1: 0.6100\n",
            "Epoch: 006, Train Acc: 0.9583, Train F1: 0.9583, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 007, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.6440, Val F1: 0.6440\n",
            "Epoch: 008, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.6420, Val F1: 0.6420\n",
            "Epoch: 009, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.6440, Val F1: 0.6440\n",
            "Epoch: 010, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.6400, Val F1: 0.6400\n",
            "Epoch: 011, Train Acc: 0.9583, Train F1: 0.9583, Val Acc: 0.6520, Val F1: 0.6520\n",
            "Epoch: 012, Train Acc: 0.9667, Train F1: 0.9667, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 013, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6700, Val F1: 0.6700\n",
            "Epoch: 014, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6700, Val F1: 0.6700\n",
            "Epoch: 015, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6720, Val F1: 0.6720\n",
            "Epoch: 016, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6800, Val F1: 0.6800\n",
            "Epoch: 017, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6820, Val F1: 0.6820\n",
            "Epoch: 018, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6900, Val F1: 0.6900\n",
            "Epoch: 019, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6920, Val F1: 0.6920\n",
            "Epoch: 020, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6960, Val F1: 0.6960\n",
            "Epoch: 021, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 022, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 023, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 024, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 025, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 026, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 027, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7020, Val F1: 0.7020\n",
            "Epoch: 028, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7040, Val F1: 0.7040\n",
            "Epoch: 029, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7100, Val F1: 0.7100\n",
            "Epoch: 030, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7080, Val F1: 0.7080\n",
            "Epoch: 031, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7060, Val F1: 0.7060\n",
            "Epoch: 032, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7040, Val F1: 0.7040\n",
            "Epoch: 033, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7040, Val F1: 0.7040\n",
            "Epoch: 034, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.7000, Val F1: 0.7000\n",
            "Epoch: 035, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6960, Val F1: 0.6960\n",
            "Epoch: 036, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6940, Val F1: 0.6940\n",
            "Epoch: 037, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6880, Val F1: 0.6880\n",
            "Epoch: 038, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6840, Val F1: 0.6840\n",
            "Epoch: 039, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6840, Val F1: 0.6840\n",
            "Epoch: 040, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6840, Val F1: 0.6840\n",
            "Epoch: 041, Train Acc: 0.9750, Train F1: 0.9750, Val Acc: 0.6840, Val F1: 0.6840\n",
            "Epoch: 042, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6820, Val F1: 0.6820\n",
            "Epoch: 043, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6760, Val F1: 0.6760\n",
            "Epoch: 044, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6740, Val F1: 0.6740\n",
            "Epoch: 045, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6700, Val F1: 0.6700\n",
            "Epoch: 046, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 047, Train Acc: 0.9917, Train F1: 0.9917, Val Acc: 0.6660, Val F1: 0.6660\n",
            "Epoch: 048, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 049, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6660, Val F1: 0.6660\n",
            "Epoch: 050, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6660, Val F1: 0.6660\n",
            "Epoch: 051, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6680, Val F1: 0.6680\n",
            "Epoch: 052, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6680, Val F1: 0.6680\n",
            "Epoch: 053, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6660, Val F1: 0.6660\n",
            "Epoch: 054, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 055, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 056, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 057, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 058, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 059, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 060, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 061, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 062, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 063, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 064, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 065, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 066, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 067, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 068, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 069, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6620, Val F1: 0.6620\n",
            "Epoch: 070, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 071, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6640, Val F1: 0.6640\n",
            "Epoch: 072, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 073, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 074, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6620, Val F1: 0.6620\n",
            "Epoch: 075, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6620, Val F1: 0.6620\n",
            "Epoch: 076, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6620, Val F1: 0.6620\n",
            "Epoch: 077, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6600, Val F1: 0.6600\n",
            "Epoch: 078, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6620, Val F1: 0.6620\n",
            "Epoch: 079, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6580, Val F1: 0.6580\n",
            "Epoch: 080, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 081, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 082, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 083, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 084, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 085, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6460, Val F1: 0.6460\n",
            "Epoch: 086, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 087, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6500, Val F1: 0.6500\n",
            "Epoch: 088, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 089, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6500, Val F1: 0.6500\n",
            "Epoch: 090, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 091, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 092, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 093, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6480, Val F1: 0.6480\n",
            "Epoch: 094, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6520, Val F1: 0.6520\n",
            "Epoch: 095, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 096, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 097, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 098, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6540, Val F1: 0.6540\n",
            "Epoch: 099, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Epoch: 100, Train Acc: 1.0000, Train F1: 1.0000, Val Acc: 0.6560, Val F1: 0.6560\n",
            "Current GPU memory: 106.61 MB\n",
            "Max GPU memory used: 130.95 MB\n",
            "Time taken: 1.27 seconds\n",
            "Embeddings: 98.6 MB, Weights: 438.8 MB, Total: 208.3 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amazon dataset**"
      ],
      "metadata": {
        "id": "O86zR-LAL7O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_gpu_memory()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "dataset = Amazon(\n",
        "        root='data/Amazon',\n",
        "        name='Computers',\n",
        "        transform=T.Compose([\n",
        "        NormalizeFeatures(),          # feature‑wise ℓ₂ normalisation\n",
        "        RandomNodeSplit(              # ⇦ add a split transform\n",
        "                split='train_rest',       # 10% val, 10% test by default\n",
        "                num_val=0.1,\n",
        "                num_test=0.1,\n",
        "                num_splits=1,\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset[0].to(device)  # Get the first graph object.\n",
        "data\n",
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(num_features, 64, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train(data, data.train_mask)\n",
        "    train_acc, train_f1 = test(data, data.train_mask)\n",
        "    val_acc, val_f1 = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Max GPU memory used: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "test_acc,f1_micro = test(data, data.test_mask)\n",
        "test_acc\n",
        "\n",
        "e, w, t = gcn_memory_usage(2, data.num_nodes, data.num_features)\n",
        "print(f\"Embeddings: {e/1e6:.1f} MB, Weights: {w*4/1e6:.1f} MB, Total: {t/1e6:.1f} MB\")\n",
        "\n",
        "\n",
        "peak_memory_mb=f\"{torch.cuda.max_memory_allocated()/1024**2:.2f}\"\n",
        "total_train_time=f\"{end_time - start_time:.2f}\"\n",
        "\n",
        "metrics = {\n",
        "    \"model\": \"GCN full-batch\",\n",
        "    \"accuracy\": test_acc,\n",
        "    \"f1_micro\":f1_micro,\n",
        "    \"peak_memory_MB\": peak_memory_mb,\n",
        "    \"train_time_sec\": total_train_time,\n",
        "    \"embedding_storage\":e,\n",
        "    \"Weight_Matrices\":w,\n",
        "    \"Total_Memory\":t\n",
        "}\n",
        "\n",
        "with open(\"GCN_full_batch_Amazon_results.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)"
      ],
      "metadata": {
        "id": "crw4u9MFL-N5",
        "outputId": "7ae22747-015e-4e65-cd58-3ce0928ad94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory after cleanup: 106.61 MB\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes:          13752\n",
            "Number of edges:          491722\n",
            "Average node degree:      35.76\n",
            "Number of training nodes: 11002\n",
            "Training node label rate: 0.800\n",
            "Has isolated nodes:       True\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n",
            "Epoch: 001, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 002, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 003, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 004, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 005, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 006, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 007, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 008, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 009, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 010, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 011, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 012, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 013, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 014, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 015, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 016, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 017, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 018, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 019, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 020, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 021, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 022, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 023, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 024, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 025, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 026, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 027, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 028, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 029, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 030, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 031, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 032, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 033, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 034, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 035, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 036, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 037, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 038, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 039, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 040, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 041, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 042, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 043, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 044, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 045, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 046, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 047, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 048, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 049, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 050, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3847, Val F1: 0.3847\n",
            "Epoch: 051, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 052, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 053, Train Acc: 0.3761, Train F1: 0.3761, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 054, Train Acc: 0.3762, Train F1: 0.3762, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 055, Train Acc: 0.3764, Train F1: 0.3764, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 056, Train Acc: 0.3766, Train F1: 0.3766, Val Acc: 0.3855, Val F1: 0.3855\n",
            "Epoch: 057, Train Acc: 0.3793, Train F1: 0.3793, Val Acc: 0.3884, Val F1: 0.3884\n",
            "Epoch: 058, Train Acc: 0.3817, Train F1: 0.3817, Val Acc: 0.3927, Val F1: 0.3927\n",
            "Epoch: 059, Train Acc: 0.3842, Train F1: 0.3842, Val Acc: 0.3935, Val F1: 0.3935\n",
            "Epoch: 060, Train Acc: 0.3865, Train F1: 0.3865, Val Acc: 0.3942, Val F1: 0.3942\n",
            "Epoch: 061, Train Acc: 0.3878, Train F1: 0.3878, Val Acc: 0.3964, Val F1: 0.3964\n",
            "Epoch: 062, Train Acc: 0.3904, Train F1: 0.3904, Val Acc: 0.3971, Val F1: 0.3971\n",
            "Epoch: 063, Train Acc: 0.3925, Train F1: 0.3925, Val Acc: 0.3993, Val F1: 0.3993\n",
            "Epoch: 064, Train Acc: 0.3950, Train F1: 0.3950, Val Acc: 0.4007, Val F1: 0.4007\n",
            "Epoch: 065, Train Acc: 0.3993, Train F1: 0.3993, Val Acc: 0.4073, Val F1: 0.4073\n",
            "Epoch: 066, Train Acc: 0.4037, Train F1: 0.4037, Val Acc: 0.4160, Val F1: 0.4160\n",
            "Epoch: 067, Train Acc: 0.4085, Train F1: 0.4085, Val Acc: 0.4225, Val F1: 0.4225\n",
            "Epoch: 068, Train Acc: 0.4125, Train F1: 0.4125, Val Acc: 0.4262, Val F1: 0.4262\n",
            "Epoch: 069, Train Acc: 0.4167, Train F1: 0.4167, Val Acc: 0.4313, Val F1: 0.4313\n",
            "Epoch: 070, Train Acc: 0.4220, Train F1: 0.4220, Val Acc: 0.4364, Val F1: 0.4364\n",
            "Epoch: 071, Train Acc: 0.4282, Train F1: 0.4282, Val Acc: 0.4415, Val F1: 0.4415\n",
            "Epoch: 072, Train Acc: 0.4342, Train F1: 0.4342, Val Acc: 0.4480, Val F1: 0.4480\n",
            "Epoch: 073, Train Acc: 0.4388, Train F1: 0.4388, Val Acc: 0.4545, Val F1: 0.4545\n",
            "Epoch: 074, Train Acc: 0.4459, Train F1: 0.4459, Val Acc: 0.4589, Val F1: 0.4589\n",
            "Epoch: 075, Train Acc: 0.4523, Train F1: 0.4523, Val Acc: 0.4669, Val F1: 0.4669\n",
            "Epoch: 076, Train Acc: 0.4628, Train F1: 0.4628, Val Acc: 0.4720, Val F1: 0.4720\n",
            "Epoch: 077, Train Acc: 0.4736, Train F1: 0.4736, Val Acc: 0.4836, Val F1: 0.4836\n",
            "Epoch: 078, Train Acc: 0.4846, Train F1: 0.4846, Val Acc: 0.4938, Val F1: 0.4938\n",
            "Epoch: 079, Train Acc: 0.4965, Train F1: 0.4965, Val Acc: 0.5069, Val F1: 0.5069\n",
            "Epoch: 080, Train Acc: 0.5066, Train F1: 0.5066, Val Acc: 0.5200, Val F1: 0.5200\n",
            "Epoch: 081, Train Acc: 0.5183, Train F1: 0.5183, Val Acc: 0.5331, Val F1: 0.5331\n",
            "Epoch: 082, Train Acc: 0.5308, Train F1: 0.5308, Val Acc: 0.5498, Val F1: 0.5498\n",
            "Epoch: 083, Train Acc: 0.5421, Train F1: 0.5421, Val Acc: 0.5564, Val F1: 0.5564\n",
            "Epoch: 084, Train Acc: 0.5523, Train F1: 0.5523, Val Acc: 0.5658, Val F1: 0.5658\n",
            "Epoch: 085, Train Acc: 0.5622, Train F1: 0.5622, Val Acc: 0.5716, Val F1: 0.5716\n",
            "Epoch: 086, Train Acc: 0.5675, Train F1: 0.5675, Val Acc: 0.5775, Val F1: 0.5775\n",
            "Epoch: 087, Train Acc: 0.5717, Train F1: 0.5717, Val Acc: 0.5847, Val F1: 0.5847\n",
            "Epoch: 088, Train Acc: 0.5763, Train F1: 0.5763, Val Acc: 0.5898, Val F1: 0.5898\n",
            "Epoch: 089, Train Acc: 0.5825, Train F1: 0.5825, Val Acc: 0.5956, Val F1: 0.5956\n",
            "Epoch: 090, Train Acc: 0.5898, Train F1: 0.5898, Val Acc: 0.5978, Val F1: 0.5978\n",
            "Epoch: 091, Train Acc: 0.5985, Train F1: 0.5985, Val Acc: 0.6036, Val F1: 0.6036\n",
            "Epoch: 092, Train Acc: 0.6067, Train F1: 0.6067, Val Acc: 0.6109, Val F1: 0.6109\n",
            "Epoch: 093, Train Acc: 0.6123, Train F1: 0.6123, Val Acc: 0.6182, Val F1: 0.6182\n",
            "Epoch: 094, Train Acc: 0.6165, Train F1: 0.6165, Val Acc: 0.6233, Val F1: 0.6233\n",
            "Epoch: 095, Train Acc: 0.6214, Train F1: 0.6214, Val Acc: 0.6233, Val F1: 0.6233\n",
            "Epoch: 096, Train Acc: 0.6237, Train F1: 0.6237, Val Acc: 0.6240, Val F1: 0.6240\n",
            "Epoch: 097, Train Acc: 0.6269, Train F1: 0.6269, Val Acc: 0.6269, Val F1: 0.6269\n",
            "Epoch: 098, Train Acc: 0.6306, Train F1: 0.6306, Val Acc: 0.6298, Val F1: 0.6298\n",
            "Epoch: 099, Train Acc: 0.6344, Train F1: 0.6344, Val Acc: 0.6371, Val F1: 0.6371\n",
            "Epoch: 100, Train Acc: 0.6385, Train F1: 0.6385, Val Acc: 0.6422, Val F1: 0.6422\n",
            "Current GPU memory: 104.78 MB\n",
            "Max GPU memory used: 419.93 MB\n",
            "Time taken: 2.70 seconds\n",
            "Embeddings: 84.4 MB, Weights: 18.8 MB, Total: 89.1 MB\n"
          ]
        }
      ]
    }
  ]
}