{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMm/ex4p//7a1u5gPBxYVs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghommidhWassim/GNN-variants/blob/main/test_gcn_variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x5loG4i3uJp",
        "outputId": "161641c1-1e72-462e-c8ee-4262c930dc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgoD1Pkr_fXV",
        "outputId": "522db1ff-45e2-4896-ae22-66a83064bea4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DayBwOJd_mKn",
        "outputId": "0552a200-65a1-4829-e6bc-01c32a07763a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KkCxJS-YSz_",
        "outputId": "571f6a0e-d919-463d-ab1a-82e9088654d9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp4kz6bH_pnL",
        "outputId": "960cae42-27c1-4d3d-c5a7-05f669a5c87d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.11/dist-packages (0.4.0+pt26cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib import cm\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "\n",
        "# import pyg_lib\n",
        "import torch_sparse\n",
        "\n",
        "# PyTorch geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric import seed_everything"
      ],
      "metadata": {
        "id": "F1xbmd_1AJxm"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(1234567)\n",
        "seed_everything(42)\n",
        "plt.style.use('dark_background')\n",
        "num_epochs = 101"
      ],
      "metadata": {
        "id": "djZb3AuzB5hz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset[0].to(device)  # Get the first graph object.\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoIPzKlZCDAm",
        "outputId": "34289f61-4d4c-466c-f867-8996a5afb1ee"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes:          {data.num_nodes}')\n",
        "print(f'Number of edges:          {data.num_edges}')\n",
        "print(f'Average node degree:      {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes:       {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:           {data.has_self_loops()}')\n",
        "print(f'Is undirected:            {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_68HoG0CFvQ",
        "outputId": "874ecc68-e98e-40d3-e6e7-27328edd8bcc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes:          19717\n",
            "Number of edges:          88648\n",
            "Average node degree:      4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes:       False\n",
            "Has self-loops:           False\n",
            "Is undirected:            True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "    self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    return x"
      ],
      "metadata": {
        "id": "WO1wwKbMCGbE"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STANDARD GCN**"
      ],
      "metadata": {
        "id": "XqGIH2c8CdHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(hidden_channels=16).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoV5H-7JCLrB",
        "outputId": "92f92ce4-ef19-4ad9-9429-5081f6e17c49"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(500, 16)\n",
            "  (conv2): GCNConv(16, 3)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, mask):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()  # Clear gradients.\n",
        "  out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "  loss = criterion(out[mask], data.y[mask])  # Compute the loss solely based on the training nodes.\n",
        "  loss.backward()  # Derive gradients.\n",
        "  optimizer.step()  # Update parameters based on gradients.\n",
        "  return loss\n",
        "\n",
        "def test(data, mask):\n",
        "  model.eval()\n",
        "  out = model(data.x, data.edge_index)\n",
        "  pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "  correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "  acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "  return acc"
      ],
      "metadata": {
        "id": "DU2MmFAXCN2u"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(hidden_channels=16).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "for epoch in range(1, num_epochs):\n",
        "  loss = train(data, data.train_mask)\n",
        "  if epoch % 10 == 0:\n",
        "    train_acc = test(data, data.train_mask)\n",
        "    val_acc = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "170T7bBCCR8l",
        "outputId": "fb61f794-bf3f-46fe-f863-6a847f979b13"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Train: 0.9000\n",
            "Epoch: 020, Train: 0.9000\n",
            "Epoch: 030, Train: 0.9333\n",
            "Epoch: 040, Train: 0.9333\n",
            "Epoch: 050, Train: 0.9500\n",
            "Epoch: 060, Train: 0.9667\n",
            "Epoch: 070, Train: 0.9667\n",
            "Epoch: 080, Train: 0.9833\n",
            "Epoch: 090, Train: 1.0000\n",
            "Epoch: 100, Train: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test(data, data.test_mask)\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEytRA11CZTv",
        "outputId": "2189a8a9-dab8-44e4-9641-5bf246179f5b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.771"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLUSTER GCN**"
      ],
      "metadata": {
        "id": "pMfsq-w5Cjmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.cpu()\n",
        "cluster_data = ClusterData(data, num_parts=128)\n",
        "train_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "total_num_nodes = 0\n",
        "#for step, sub_data in enumerate(train_loader):\n",
        " # print(f'Batch: {step + 1} has {sub_data.num_nodes} nodes')\n",
        "  #print(sub_data)\n",
        "  #print()\n",
        "  #total_num_nodes += sub_data.num_nodes\n",
        "\n",
        "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6OxssB-CnN9",
        "outputId": "846a8e97-bb54-43d3-fe1a-25a5858b6b60"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing METIS partitioning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterated over 0 of 19717 nodes!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(loader):\n",
        "  model.train()\n",
        "  for sub_data in train_loader:  # Iterate over each mini-batch.\n",
        "    sub_data = sub_data.to(device)\n",
        "    out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
        "    loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    optimizer.zero_grad()  # Clear gradients."
      ],
      "metadata": {
        "id": "1mca-m_ECtZ7"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GCN(hidden_channels=16).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "data=data.to(device)\n",
        "for epoch in range(1, num_epochs):\n",
        "  loss = train_batch(train_loader)\n",
        "  if epoch % 10 == 0:\n",
        "    train_acc = test(data, data.train_mask)\n",
        "    val_acc = test(data, data.val_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "5EpNNohtCz5t",
        "outputId": "784d65f9-1fdd-44a4-df65-19eec6d841bd"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Train: 0.9167\n",
            "Epoch: 020, Train: 0.9667\n",
            "Epoch: 030, Train: 0.9833\n",
            "Epoch: 040, Train: 1.0000\n",
            "Epoch: 050, Train: 0.9833\n",
            "Epoch: 060, Train: 1.0000\n",
            "Epoch: 070, Train: 1.0000\n",
            "Epoch: 080, Train: 1.0000\n",
            "Epoch: 090, Train: 1.0000\n",
            "Epoch: 100, Train: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test(data, data.test_mask)\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjbB-MLbC7zk",
        "outputId": "8bf47487-596c-407b-94de-c2794b60ae79"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.767"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_profiling(model, data, loader=train_loader)  # Or None for full-batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc23QTedE_mS",
        "outputId": "7da515b1-5d87-4261-f994-92b15579f2c2"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 2 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 3 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 4 Profile:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls   Total FLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us       5.414ms       221.84%       5.414ms       1.805ms           0 b           0 b           0 b           0 b             3            --  \n",
            "                                       loss_calculation         0.00%       0.000us         0.00%       0.000us       0.000us       1.271ms        52.09%       1.271ms     423.734us           0 b           0 b           0 b           0 b             3            --  \n",
            "                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us       1.066ms        43.68%       1.066ms     355.384us           0 b           0 b           0 b           0 b             3            --  \n",
            "                                                epoch_3         0.00%       0.000us         0.00%       0.000us       0.000us     697.553us        28.58%     697.553us     697.553us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                               aten::mm         2.70%     764.928us         4.02%       1.138ms      47.417us     676.875us        27.73%     676.875us      28.203us           0 b           0 b     994.50 Kb     994.50 Kb            24  194501760.000  \n",
            "                                                epoch_2         0.00%       0.000us         0.00%       0.000us       0.000us     572.115us        23.44%     572.115us     572.115us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                                epoch_4         0.00%       0.000us         0.00%       0.000us       0.000us     531.731us        21.79%     531.731us     531.731us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                     aten::index_select         1.36%     384.874us         3.36%     952.667us      45.365us     384.249us        15.74%     384.249us      18.298us           0 b           0 b      17.80 Mb           0 b            21            --  \n",
            "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     322.680us        13.22%     322.680us      35.853us           0 b           0 b           0 b           0 b             9            --  \n",
            "                        volta_sgemm_128x32_sliced1x4_nt         0.00%       0.000us         0.00%       0.000us       0.000us     279.895us        11.47%     279.895us      46.649us           0 b           0 b           0 b           0 b             6            --  \n",
            "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     231.642us         9.49%     231.642us      38.607us           0 b           0 b           0 b           0 b             6            --  \n",
            "                                            aten::addmm         1.36%     385.203us         1.97%     558.211us      93.035us     187.675us         7.69%     187.675us      31.279us           0 b           0 b     299.50 Kb      -5.71 Mb             6  64577152.000  \n",
            "                                     aten::scatter_add_         0.91%     257.484us         1.33%     377.943us      31.495us     183.739us         7.53%     183.739us      15.312us           0 b           0 b           0 b           0 b            12            --  \n",
            "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us     183.739us         7.53%     183.739us      15.312us           0 b           0 b           0 b           0 b            12            --  \n",
            "                                              aten::div         0.56%     157.337us         0.95%     269.634us      29.959us     147.451us         6.04%     147.451us      16.383us           0 b           0 b       8.14 Mb       8.14 Mb             9            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 28.337ms\n",
            "Self CUDA time total: 2.441ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls   Total FLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          aten::resize_         0.59%     168.552us         0.59%     168.552us       3.511us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      17.81 Mb      17.81 Mb            48            --  \n",
            "                                            aten::empty         1.87%     529.025us         1.87%     529.025us       6.531us       0.000us         0.00%       0.000us       0.000us       7.28 Kb       7.28 Kb      14.28 Mb      14.28 Mb            81            --  \n",
            "                                              aten::div         0.56%     157.337us         0.95%     269.634us      29.959us     147.451us         6.04%     147.451us      16.383us           0 b           0 b       8.14 Mb       8.14 Mb             9            --  \n",
            "                                               aten::mm         2.70%     764.928us         4.02%       1.138ms      47.417us     676.875us        27.73%     676.875us      28.203us           0 b           0 b     994.50 Kb     994.50 Kb            24  194501760.000  \n",
            "                                           aten::gather         0.24%      67.054us         0.35%     100.063us      33.354us      22.689us         0.93%      22.689us       7.563us           0 b           0 b     320.50 Kb     320.50 Kb             3            --  \n",
            "                                    aten::empty_strided         0.87%     246.024us         0.87%     246.024us       6.308us       0.000us         0.00%       0.000us       0.000us     151.34 Kb     151.34 Kb     309.00 Kb     309.00 Kb            39            --  \n",
            "                                              aten::add         0.60%     168.969us         0.86%     242.783us      40.464us      19.776us         0.81%      19.776us       3.296us           0 b           0 b     299.50 Kb     299.50 Kb             6     76228.000  \n",
            "                                        aten::clamp_min         0.24%      68.852us         0.36%     102.415us      34.138us       9.662us         0.40%       9.662us       3.221us           0 b           0 b     251.50 Kb     251.50 Kb             3            --  \n",
            "                               aten::threshold_backward         0.15%      43.543us         0.24%      67.139us      22.380us      10.399us         0.43%      10.399us       3.466us           0 b           0 b     251.50 Kb     251.50 Kb             3            --  \n",
            "                                            aten::clamp         0.34%      96.082us         0.52%     146.495us      24.416us      19.294us         0.79%      19.294us       3.216us           0 b           0 b      33.00 Kb      33.00 Kb             6            --  \n",
            "                                     aten::_log_softmax         0.22%      63.341us         0.32%      91.836us      30.612us      10.304us         0.42%      10.304us       3.435us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                 aten::nll_loss_forward         0.21%      60.650us         0.30%      85.875us      28.625us      15.583us         0.64%      15.583us       5.194us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                aten::nll_loss_backward         0.27%      77.698us         0.80%     225.378us      75.126us      13.216us         0.54%      20.512us       6.837us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                       aten::_log_softmax_backward_data         0.19%      53.609us         0.28%      80.546us      26.849us       9.855us         0.40%       9.855us       3.285us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                              aten::sum         0.60%     171.166us         0.80%     226.897us      37.816us      98.556us         4.04%      98.556us      16.426us           0 b           0 b       3.00 Kb       3.00 Kb             6            --  \n",
            "                                            aten::index         1.44%     406.788us         4.20%       1.189ms      79.293us      44.897us         1.84%     134.559us       8.971us      42.81 Kb      42.81 Kb       4.50 Kb       1.50 Kb            15            --  \n",
            "                                        aten::remainder         0.22%      62.238us         0.31%      87.158us      29.053us      10.848us         0.44%      10.848us       3.616us           0 b           0 b       1.50 Kb       1.50 Kb             3            --  \n",
            "                                              aten::mul         0.17%      47.954us         0.25%      70.131us      23.377us       9.152us         0.37%       9.152us       3.051us           0 b           0 b       1.50 Kb       1.50 Kb             3       180.000  \n",
            "                                          ProfilerStep*         3.13%     886.818us        84.04%      23.816ms       7.939ms       0.000us         0.00%       1.573ms     524.211us      24.31 Kb           0 b     -16.00 Kb           0 b             3            --  \n",
            "                                          aten::random_         0.25%      70.434us         0.25%      70.434us       4.696us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            15            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 28.337ms\n",
            "Self CUDA time total: 2.441ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GraphSAGE**"
      ],
      "metadata": {
        "id": "djoXaFrhOmPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "\n",
        "# Create neighbor loader for mini-batch training\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[10, 10],  # 2 layers with 10 neighbors each\n",
        "    batch_size=1024,\n",
        "    input_nodes=data.train_mask,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Initialize GraphSAGE model (similar hidden size to GCN)\n",
        "model = GraphSAGE(\n",
        "    in_channels=dataset.num_features,  # 500 for PubMed\n",
        "    hidden_channels=16,               # Same as GCN\n",
        "    num_layers=2,                     # 2-layer model\n",
        "    out_channels=dataset.num_classes, # 3 for PubMed\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Same LR as GCN\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = total_correct = total_examples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss) * batch.train_mask.sum()\n",
        "        total_correct += int((out[batch.train_mask].argmax(dim=-1) == batch.y[batch.train_mask]).sum())\n",
        "        total_examples += batch.train_mask.sum()\n",
        "\n",
        "    return total_loss / total_examples, total_correct / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=-1)\n",
        "    acc = accuracy_score(data.y[mask].cpu(), pred[mask].cpu())\n",
        "    return acc\n",
        "\n",
        "# Training loop (same number of epochs as GCN)\n",
        "for epoch in range(1, 101):\n",
        "    loss, train_acc = train()\n",
        "    if epoch % 10 == 0:\n",
        "        val_acc = test(data.val_mask)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}')\n",
        "\n",
        "# Final testing\n",
        "test_acc = test(data.test_mask)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "torch.cuda.memory._dump_snapshot(\"profile.pkl\")\n",
        "torch.cuda.memory._record_memory_history(enabled=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq111ndiPQMs",
        "outputId": "13609c86-b50b-4af4-db54-f1c5c23e3adf"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Loss: 0.9164, Train: 0.9667, Val: 0.7160\n",
            "Epoch: 020, Loss: 0.5277, Train: 0.9833, Val: 0.7400\n",
            "Epoch: 030, Loss: 0.2005, Train: 0.9833, Val: 0.7560\n",
            "Epoch: 040, Loss: 0.0689, Train: 1.0000, Val: 0.7600\n",
            "Epoch: 050, Loss: 0.0272, Train: 1.0000, Val: 0.7640\n",
            "Epoch: 060, Loss: 0.0146, Train: 1.0000, Val: 0.7640\n",
            "Epoch: 070, Loss: 0.0089, Train: 1.0000, Val: 0.7680\n",
            "Epoch: 080, Loss: 0.0073, Train: 1.0000, Val: 0.7660\n",
            "Epoch: 090, Loss: 0.0059, Train: 1.0000, Val: 0.7680\n",
            "Epoch: 100, Loss: 0.0047, Train: 1.0000, Val: 0.7720\n",
            "Test Accuracy: 0.7460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import torch\n",
        "\n",
        "def train_step(batch, model, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    with record_function(\"forward_pass\"):\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "    with record_function(\"loss_calculation\"):\n",
        "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "    with record_function(\"backward_pass\"):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def run_profiling(model, data, loader=None, epochs=5):\n",
        "    # Warm-up (important for CUDA)\n",
        "    for _ in range(2):\n",
        "        if loader:\n",
        "            for batch in loader:\n",
        "                train_step(batch, model, optimizer, criterion)\n",
        "        else:\n",
        "            train_step(data, model, optimizer, criterion)\n",
        "\n",
        "    # Profiling proper\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        schedule=torch.profiler.schedule(\n",
        "            wait=1,  # Skip first epoch\n",
        "            warmup=1,  # Warmup profiler\n",
        "            active=3  # Profile next 3 epochs\n",
        "        ),\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True,\n",
        "        with_flops=True  # Measure FLOPs if available\n",
        "    ) as prof:\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            with record_function(f\"epoch_{epoch}\"):\n",
        "                if loader:\n",
        "                    for batch in loader:\n",
        "                        train_step(batch, model, optimizer, criterion)\n",
        "                else:\n",
        "                    train_step(data, model, optimizer, criterion)\n",
        "\n",
        "            prof.step()\n",
        "\n",
        "            # Print summary every epoch\n",
        "            print(f\"\\nEpoch {epoch} Profile:\")\n",
        "            print(prof.key_averages().table(\n",
        "                sort_by=\"self_cuda_time_total\" if torch.cuda.is_available() else \"self_cpu_time_total\",\n",
        "                row_limit=15\n",
        "            ))\n",
        "            print(prof.key_averages().table(\n",
        "            sort_by=\"self_cuda_memory_usage\",\n",
        "            row_limit=20\n",
        "            ))\n",
        "\n",
        "# Usage\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Profiling GraphSAGE...\")\n",
        "run_profiling(model, data, loader=train_loader)  # Or None for full-batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsCKONps3XXo",
        "outputId": "0b5d0cdc-af6b-4c72-c541-95206cdad6dc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling GraphSAGE...\n",
            "\n",
            "Epoch 0 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 2 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 3 Profile:\n",
            "\n",
            "\n",
            "\n",
            "Epoch 4 Profile:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls   Total FLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us       8.606ms       354.32%       8.606ms       2.869ms           0 b           0 b           0 b           0 b             3            --  \n",
            "                                       loss_calculation         0.00%       0.000us         0.00%       0.000us       0.000us       1.889ms        77.77%       1.889ms     629.639us           0 b           0 b           0 b           0 b             3            --  \n",
            "                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us       1.804ms        74.29%       1.804ms     601.426us           0 b           0 b           0 b           0 b             3            --  \n",
            "                                                epoch_2         0.00%       0.000us         0.00%       0.000us       0.000us     800.750us        32.97%     800.750us     800.750us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                                epoch_4         0.00%       0.000us         0.00%       0.000us       0.000us     764.719us        31.49%     764.719us     764.719us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                                epoch_3         0.00%       0.000us         0.00%       0.000us       0.000us     690.512us        28.43%     690.512us     690.512us           0 b           0 b           0 b           0 b             1            --  \n",
            "                                               aten::mm         2.58%       1.065ms         3.85%       1.591ms      66.272us     676.848us        27.87%     676.848us      28.202us           0 b           0 b     976.50 Kb     976.50 Kb            24  190477920.000  \n",
            "                                     aten::index_select         1.36%     559.495us         3.16%       1.304ms      62.110us     379.354us        15.62%     379.354us      18.064us           0 b           0 b      17.59 Mb           0 b            21            --  \n",
            "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     318.395us        13.11%     318.395us      35.377us           0 b           0 b           0 b           0 b             9            --  \n",
            "                        volta_sgemm_128x32_sliced1x4_nt         0.00%       0.000us         0.00%       0.000us       0.000us     280.761us        11.56%     280.761us      46.794us           0 b           0 b           0 b           0 b             6            --  \n",
            "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     230.330us         9.48%     230.330us      38.388us           0 b           0 b           0 b           0 b             6            --  \n",
            "                                            aten::addmm         1.14%     469.387us         1.73%     715.846us     119.308us     185.981us         7.66%     185.981us      30.997us           0 b           0 b     292.50 Kb      -5.71 Mb             6  63241184.000  \n",
            "                                     aten::scatter_add_         0.99%     407.587us         1.45%     596.848us      49.737us     182.298us         7.51%     182.298us      15.192us           0 b           0 b           0 b           0 b            12            --  \n",
            "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us     182.298us         7.51%     182.298us      15.192us           0 b           0 b           0 b           0 b            12            --  \n",
            "                                              aten::div         0.63%     260.977us         0.97%     400.472us      44.497us     145.343us         5.98%     145.343us      16.149us           0 b           0 b       7.98 Mb       7.98 Mb             9            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 41.262ms\n",
            "Self CUDA time total: 2.429ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls   Total FLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          aten::resize_         0.64%     262.931us         0.64%     262.931us       5.478us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      17.59 Mb      17.59 Mb            48            --  \n",
            "                                            aten::empty         1.95%     803.139us         1.95%     803.139us       9.915us       0.000us         0.00%       0.000us       0.000us       7.28 Kb       7.28 Kb      14.11 Mb      14.11 Mb            81            --  \n",
            "                                              aten::div         0.63%     260.977us         0.97%     400.472us      44.497us     145.343us         5.98%     145.343us      16.149us           0 b           0 b       7.98 Mb       7.98 Mb             9            --  \n",
            "                                               aten::mm         2.58%       1.065ms         3.85%       1.591ms      66.272us     676.848us        27.87%     676.848us      28.202us           0 b           0 b     976.50 Kb     976.50 Kb            24  190477920.000  \n",
            "                                           aten::gather         0.26%     106.819us         0.40%     164.344us      54.781us      22.880us         0.94%      22.880us       7.627us           0 b           0 b     317.00 Kb     317.00 Kb             3            --  \n",
            "                                    aten::empty_strided         0.94%     389.192us         0.94%     389.192us       9.979us       0.000us         0.00%       0.000us       0.000us     149.31 Kb     149.31 Kb     308.00 Kb     308.00 Kb            39            --  \n",
            "                                              aten::add         0.52%     216.507us         0.73%     299.276us      49.879us      19.423us         0.80%      19.423us       3.237us           0 b           0 b     292.50 Kb     292.50 Kb             6     74651.000  \n",
            "                                        aten::clamp_min         0.22%      90.084us         0.38%     155.594us      51.865us       9.792us         0.40%       9.792us       3.264us           0 b           0 b     246.00 Kb     246.00 Kb             3            --  \n",
            "                               aten::threshold_backward         0.17%      69.111us         0.31%     126.791us      42.264us      10.336us         0.43%      10.336us       3.445us           0 b           0 b     246.00 Kb     246.00 Kb             3            --  \n",
            "                                            aten::clamp         0.40%     166.745us         0.63%     261.631us      43.605us      19.232us         0.79%      19.232us       3.205us           0 b           0 b      33.00 Kb      33.00 Kb             6            --  \n",
            "                                     aten::_log_softmax         0.21%      85.178us         0.30%     122.884us      40.961us      10.273us         0.42%      10.273us       3.424us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                 aten::nll_loss_forward         0.20%      80.511us         0.27%     112.963us      37.654us      15.584us         0.64%      15.584us       5.195us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                aten::nll_loss_backward         0.19%      79.485us         0.54%     221.653us      73.884us      13.184us         0.54%      20.480us       6.827us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                       aten::_log_softmax_backward_data         0.15%      60.234us         0.23%      96.272us      32.091us       9.824us         0.40%       9.824us       3.275us           0 b           0 b       3.00 Kb       3.00 Kb             3            --  \n",
            "                                              aten::sum         0.46%     191.015us         0.66%     270.446us      45.074us      98.877us         4.07%      98.877us      16.479us           0 b           0 b       3.00 Kb       3.00 Kb             6            --  \n",
            "                                            aten::index         1.46%     602.561us         4.35%       1.796ms     119.739us      44.574us         1.84%     135.261us       9.017us      42.35 Kb      42.35 Kb       4.50 Kb       1.50 Kb            15            --  \n",
            "                                        aten::remainder         0.26%     108.758us         0.35%     142.651us      47.550us      10.975us         0.45%      10.975us       3.658us           0 b           0 b       1.50 Kb       1.50 Kb             3            --  \n",
            "                                              aten::mul         0.18%      73.819us         0.25%     103.979us      34.660us       9.152us         0.38%       9.152us       3.051us           0 b           0 b       1.50 Kb       1.50 Kb             3       180.000  \n",
            "                                          ProfilerStep*         3.68%       1.520ms        84.20%      34.741ms      11.580ms       0.000us         0.00%       1.561ms     520.245us      23.80 Kb           0 b       1.50 Kb           0 b             3            --  \n",
            "                                          aten::random_         0.27%     110.291us         0.27%     110.291us       7.353us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            15            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 41.262ms\n",
            "Self CUDA time total: 2.429ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(\n",
        "    sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\",\n",
        "    row_limit=10\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "XTG3_iuG5XkB",
        "outputId": "3b7f94a6-2e41-477e-9b21-63f793366d71"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prof' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-3938004311>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(prof.key_averages().table(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msort_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda_time_total\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu_time_total\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrow_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prof' is not defined"
          ]
        }
      ]
    }
  ]
}